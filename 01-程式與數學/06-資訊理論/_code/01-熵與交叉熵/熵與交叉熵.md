## 熵，交叉熵，KL 散度之間的關係

    cross_entropy(p,q) = entropy(p) + kl_divergence(p,q)

請看 ChatGPT 的解釋與 crossEntropy1.py

* https://chatgpt.com/c/67171e7a-4d6c-8012-9f5d-11bf7be050f8
    * https://chatgpt.com/share/67171ee0-3080-8012-be3d-6696ca14eebe

## ccc:

這段程式碼展示了如何計算**熵（entropy）**、**交叉熵（cross entropy）**和**KL 散度（KL divergence）**。我們來逐一解釋這些概念及其意義。

### 1. 熵（Entropy）

熵是資訊理論中的一個核心概念，它衡量一個隨機變量的不確定性或混亂度。具體來說，對於一個概率分佈 $p$ ，它的熵定義為：

$$
H(p) = - \sum_{i} p(i) \log_2 p(i)
$$

也就是說，熵代表了從這個分佈中抽取一個事件所需的平均資訊量。當所有事件發生的機率相等時，熵最大；當某些事件的機率很高，其他事件的機率很低時，熵較小。

程式中的 `entropy(p)` 函數對應了上述公式，它對分佈 $p$ 的每個事件進行計算，並返回總的熵值。

#### 範例
- 對於分佈 $p = [1/4, 1/4, 1/4, 1/4]$ ，因為每個事件的機率相等，這是最大熵的情況。
- 對於 $r = [1/100, 1/100, 1/100, 97/100]$ ，因為大部分的機率集中在某一個事件上，熵會較低。

### 2. 交叉熵（Cross Entropy）

交叉熵衡量的是用一個分佈 $q$ 來對另一個分佈 $p$ 進行編碼時所需的平均位元數。具體來說，它的定義是：

$$
H(p, q) = - \sum_{i} p(i) \log_2 q(i)
$$

當 $p = q$ 時，交叉熵等於分佈 $p$ 的熵。當 $p$ 和 $q$ 不同時，交叉熵會比單純的熵更大，這是因為使用錯誤的分佈 $q$ 來編碼分佈 $p$ 需要更多的資訊量。

`cross_entropy(p, q)` 函數對應這個公式，計算的是使用 $q$ 來描述 $p$ 的交叉熵。

#### 範例
- `cross_entropy(p, p)` 會等於 `entropy(p)`。
- 當 $p$ 和 $q$ 差異較大時（如 `p = [1/4,1/4,1/4,1/4]` 和 `q = [1/8,1/4,1/4,3/8]`），交叉熵會比熵更大。

### 3. KL 散度（KL Divergence）

KL 散度（Kullback-Leibler Divergence）是衡量兩個分佈之間差異的量度，它表示用分佈 $q$ 來近似 $p$ 所損失的資訊量。其定義為：

$$
D_{KL}(p \parallel q) = \sum_{i} p(i) \log_2 \frac{p(i)}{q(i)}
$$

KL 散度衡量的是在使用分佈 $q$ 近似 $p$ 時所需要的額外資訊位元數。KL 散度越大，表示兩個分佈之間的差異越大。當 $p = q$ 時，KL 散度為 0，表示無損失。

`kl_divergence(p, q)` 函數實現了這個計算公式。

#### 範例
- `kl_divergence(p, q)` 會計算出分佈 $p$ 和 $q$ 之間的差異。如果這個值較大，表示 $p$ 和 $q$ 差異大，所需的額外資訊也多。

### 結論

最後，我們看到 `entropy(p) + kl_divergence(p, q)` 等於 `cross_entropy(p, q)`。這說明交叉熵實際上是熵加上 $p$ 和 $q$ 之間的KL散度，即使用 $q$ 來編碼 $p$ 的額外資訊位元數。這個關係在資訊理論中是基本且重要的：

$$
H(p, q) = H(p) + D_{KL}(p \parallel q)
$$