#### **6.3 激活函數的數學原理**

激活函數（Activation Function）是神經網絡中的一個關鍵組件，主要作用是對神經元的輸出進行非線性變換。這一過程允許神經網絡學習並擬合複雜的模式，從而使得神經網絡能夠進行非線性映射，解決線性模型無法處理的問題。激活函數在神經網絡的每一層中都扮演著至關重要的角色，尤其在深層網絡中，它們能夠增強模型的表達能力。

本節將介紹激活函數的數學原理、常見的激活函數及其特性，並分析其在神經網絡中的作用。

---

#### **6.3.1 激活函數的數學定義**

激活函數通常是對神經元的輸入信號（加權總和）進行變換，從而產生神經元的輸出。對於第  $i$  個神經元，其輸入為加權和  $z_i$ ，即：


```math
z_i = \sum_{j} w_{ij} x_j + b_i

```

其中：
-  $w_{ij}$  是從第  $j$  層到第  $i$  層的權重。
-  $x_j$  是來自前一層的輸入信號。
-  $b_i$  是偏置項。

然後，激活函數  $f$  對該加權和  $z_i$  進行變換，產生輸出  $a_i$ ：


```math
a_i = f(z_i)

```

激活函數的選擇會影響網絡的表達能力和收斂速度。常見的激活函數包括 Sigmoid 函數、ReLU 函數、Tanh 函數等。

---

#### **6.3.2 常見激活函數**

1. **Sigmoid 函數**

   Sigmoid 函數是一個常用的激活函數，其數學形式為：


```math
   f(z) = \frac{1}{1 + e^{-z}}

```

   其中：
   -  $z$  是神經元的加權和。

   Sigmoid 函數的輸出範圍是  $(0, 1)$ ，因此它通常用於需要概率輸出的任務中，如二分類問題。然而，Sigmoid 函數的主要缺點是當  $z$  取極大或極小值時，梯度會變得非常小（梯度消失問題），這會影響深層網絡的訓練。

2. **Tanh 函數（雙曲正切函數）**

   Tanh 函數是 Sigmoid 函數的擴展，其數學形式為：


```math
   f(z) = \tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}

```

   Tanh 函數的輸出範圍是  $(-1, 1)$ ，它的優點是輸出值包含零中心，使得訓練過程中有更好的收斂性。然而，Tanh 函數同樣面臨梯度消失的問題，尤其是在深層網絡中。

3. **ReLU 函數（Rectified Linear Unit）**

   ReLU 函數是目前最常用的激活函數之一，尤其在深度學習中表現出色。其數學形式為：


```math
   f(z) = \max(0, z)

```

   ReLU 函數的輸出為  $z$  本身，當  $z > 0$  時；當  $z \leq 0$  時，輸出為 0。這使得 ReLU 函數的計算非常高效，並且能夠減少梯度消失的問題。

   然而，ReLU 函數也存在一個問題，即「死神經元」問題（Dead Neurons）：當  $z \leq 0$  時，ReLU 的梯度為零，這可能導致神經元在訓練過程中永遠無法激活。

4. **Leaky ReLU 函數**

   為了解決 ReLU 的死神經元問題，Leaky ReLU 函數被提出。其數學形式為：


```math
   f(z) = \begin{cases} 
   z & \text{如果 } z > 0 \\
   \alpha z & \text{如果 } z \leq 0
   \end{cases}

```

   其中， $\alpha$  是一個小的常數，通常設為 0.01。Leaky ReLU 函數對於  $z \leq 0$  時，會保留一個非常小的梯度，這樣可以減少死神經元的問題。

5. **Softmax 函數**

   Softmax 函數通常用於多分類問題的輸出層，它將一組實數轉換為一組概率值，使得每個輸出的值在  $(0, 1)$  範圍內，且所有輸出的和為 1。其數學公式為：


```math
   f(z_i) = \frac{e^{z_i}}{\sum_{j} e^{z_j}}

```

   其中：
   -  $z_i$  是第  $i$  類的加權和。
   -  $\sum_{j} e^{z_j}$  是所有類別的加權和的指數和。

   Softmax 函數常用於神經網絡的最後一層，特別是在多分類的情況下，能夠將原始的得分轉換為概率分佈。

---

#### **6.3.3 激活函數的選擇原則**

選擇激活函數通常基於以下幾個原則：

1. **非線性變換**：激活函數應該具有非線性特性，這樣才能讓神經網絡學習複雜的模式和關係。Sigmoid、Tanh 和 ReLU 等函數都滿足這一要求。

2. **梯度消失問題**：像 Sigmoid 和 Tanh 函數，當輸入值非常大或非常小時，梯度會變得接近零，導致模型訓練變慢或無法收斂。因此，ReLU 函數由於其簡單的結構，通常能夠減少這一問題。

3. **計算效率**：ReLU 和其變體（如 Leaky ReLU）具有高效的計算特性，因為它們的計算只需要簡單的比較和加法運算，這對於大規模神經網絡訓練非常重要。

4. **輸出範圍**：選擇激活函數時，輸出的範圍也是一個重要因素。例如，Softmax 函數可以將輸出轉換為概率分佈，而 ReLU 函數將會產生非負數值，這對一些應用（如語音合成）非常重要。

---

#### **6.3.4 小結**

本節介紹了激活函數的數學原理和幾種常見的激活函數，包括 Sigmoid 函數、Tanh 函數、ReLU 函數、Leaky ReLU 函數以及 Softmax 函數。激活函數使得神經網絡能夠進行非線性映射，從而增強其表達能力。在實際應用中，根據問題的需求選擇合適的激活函數，對提高模型的性能至關重要。