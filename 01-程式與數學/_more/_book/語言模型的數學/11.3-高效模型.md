### **11.3 高效 Transformer 的數學改進**

傳統 Transformer 模型在處理大規模數據（例如長序列）時，由於其計算複雜度高（主要集中在自注意力機制的  $O(n^2)$  時間和空間複雜度），限制了其在資源受限場景下的應用。高效 Transformer 通過數學改進降低計算需求，以下是關鍵技術與其數學基礎。

---

### **1. 自注意力機制的高效化**

#### **1.1 線性注意力**

傳統自注意力的計算為：

```math
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V

```
其中  $Q$ 、 $K$ 、 $V$  分別是查詢、鍵和值的矩陣，複雜度為  $O(n^2 \cdot d_k)$ 。

##### **改進：核技巧（Kernel Trick）**
將點積注意力分解為兩個線性運算：

```math
\text{Attention}(Q, K, V) = \phi(Q) \cdot \left(\phi(K)^\top V \right)

```
其中  $\phi(x)$  是可選的非線性核函數，例如：

```math
\phi(x) = \text{ReLU}(x) + 1

```
這樣計算複雜度降為  $O(n \cdot d_k)$ 。

---

#### **1.2 稀疏注意力**

傳統注意力考慮了序列中所有位置的互動，稀疏注意力只考慮部分關鍵位置的注意力權重，降低計算負擔。

1. **稀疏模式：**
   定義稀疏注意力矩陣  $S \in \{0, 1\}^{n \times n}$ ，其中  $S_{ij} = 1$  表示位置  $i$  和  $j$  之間有注意力計算。

2. **稀疏計算：**
   將注意力計算限制在  $S$  定義的範圍內：

```math
   \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}} \odot S \right)V

```
   其中  $\odot$  表示逐元素乘積。

3. **常見稀疏模式：**
   - **局部稀疏：** 每個位置只與固定窗口範圍內的位置互動。
   - **全局稀疏：** 在關鍵位置（如特殊標記）進行全局計算。

稀疏注意力的複雜度通常為  $O(n \log n)$ 。

---

### **2. 模型壓縮與權重共享**

#### **2.1 投影壓縮（Projection Compression）**
對查詢、鍵和值的線性變換矩陣進行壓縮，減少計算量。

1. **低秩分解：**
   將矩陣  $W \in \mathbb{R}^{d \times d}$  分解為兩個低秩矩陣  $W_1 \in \mathbb{R}^{d \times r}$  和  $W_2 \in \mathbb{R}^{r \times d}$ ：

```math
   W = W_1 W_2

```
   複雜度由  $O(d^2)$  降為  $O(dr)$ ，其中  $r \ll d$ 。

2. **因式分解機制：**
   將多頭注意力的矩陣進行共享或壓縮。

---

#### **2.2 權重共享（Weight Sharing）**
減少 Transformer 中層與層之間的參數數量，降低模型大小。

1. **共享設計：**
   假設第  $l$  層和第  $l+1$  層的權重  $W^l, W^{l+1}$  是相同的：

```math
   W^l = W^{l+1}

```

2. **應用場景：**
   - ALBERT 模型使用跨層參數共享來顯著減少參數數量。

---

### **3. 序列壓縮與分段處理**

#### **3.1 緩衝窗口處理（Sliding Window Attention）**
對長序列分段處理，將序列分為  $m$  個子序列，每段長度為  $k$ 。自注意力僅在子序列內計算，邊界使用額外的緩衝區（窗口）。

計算複雜度降為：

```math
O(m \cdot k^2)

```

---

#### **3.2 层次化建模（Hierarchical Modeling）**
將序列分層次處理，高層次以摘要表示序列信息：

1. **低層處理：**
   局部自注意力以獲取短距離關係。

2. **高層處理：**
   基於低層的摘要表示進行全局自注意力計算。

---

### **4. 位置編碼的高效設計**

#### **4.1 可學習的位置編碼**
用矩陣  $P$  表示可學習的位置編碼，而非固定正弦編碼，簡化計算並增加靈活性：

```math
X_{\text{input}} = X + P

```

#### **4.2 相對位置編碼**
改進傳統絕對位置編碼，僅考慮相對位置關係：

```math
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top + R}{\sqrt{d_k}}\right)V

```
其中  $R$  是相對位置偏置矩陣。

---

### **5. 高效 Transformer 的實例**

1. **Linformer：**
   通過低秩分解降低注意力矩陣複雜度至  $O(n)$ 。

2. **Longformer：**
   使用局部窗口與稀疏注意力處理長序列。

3. **Performer：**
   引入核技巧將計算降為線性時間。

4. **Reformer：**
   使用哈希分桶技術進行高效序列分組。

---

### **總結**

高效 Transformer 的數學改進旨在解決大規模序列建模的計算瓶頸，通過稀疏化、自注意力的線性化、權重壓縮和層次化建模等方法，顯著降低計算成本，並保持性能的可靠性。這些技術為實現資源有限場景下的高效語言模型部署提供了強大的支持。