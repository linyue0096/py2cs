## **《LLM 背後的數學》**

### **第一部分：基礎數學**
#### **第 1 章 線性代數**
1.1 向量與矩陣基礎  
1.2 張量運算與應用  
1.3 矩陣分解：SVD、PCA 與應用  
1.4 線性代數在 LLM 的角色  

#### **第 2 章 微積分**
2.1 微分與積分基礎  
2.2 梯度與鏈式法則  
2.3 梯度下降法與優化技術  
2.4 自動微分的數學基礎  

#### **第 3 章 概率與統計**
3.1 概率基礎：條件概率與貝葉斯定理  
3.2 隨機變數與分布：高斯分布與多元分布  
3.3 最大似然估計（MLE）與最大後驗估計（MAP）  
3.4 貝葉斯推論在 LLM 中的應用  

#### **第 4 章 信息論**
4.1 熵與交叉熵的數學解釋  
4.2 KL 散度與分布匹配  
4.3 信息量在語言建模中的應用  

---

### **第二部分：核心理論**
#### **第 5 章 自然語言處理數學基礎**
5.1 語言表示與詞嵌入（Word Embeddings）  
5.2 矩陣分解與詞向量生成  
5.3 語言分布假設與語言模型  

#### **第 6 章 深度學習數學**
6.1 神經網絡的數學建模  
6.2 損失函數設計與學習目標  
6.3 激活函數的數學原理  
6.4 反向傳播與權重更新  

#### **第 7 章 Transformer 的數學**
7.1 自注意力機制的數學原理  
7.2 多頭注意力與矩陣運算  
7.3 位置編碼的數學基礎  
7.4 殘差連接與正則化  

#### **第 8 章 生成式語言模型**
8.1 預測式語言模型（例如 GPT）  
8.2 掩碼語言模型（例如 BERT）  
8.3 模型預測中的概率分布計算  
8.4 Beam Search 與 Top-k 生成方法  

---

### **第三部分：進階與應用**
#### **第 9 章 優化算法**
9.1 梯度下降變體（SGD、Adam、RMSprop）  
9.2 損失函數曲面的幾何性質  
9.3 收斂性與數值穩定性  

#### **第 10 章 語言模型的預訓練與微調**
10.1 預訓練的目標與損失函數  
10.2 微調中的數學挑戰  
10.3 Prompt Engineering 的數學分析  

#### **第 11 章 模型壓縮與部署**
11.1 知識蒸餾的數學原理  
11.2 量化與剪枝技術的數學基礎  
11.3 高效 Transformer 的數學改進  

#### **第 12 章 語言模型的解釋性與公平性**
12.1 注意力權重的數學解釋  
12.2 模型偏差與公平性評估  
12.3 LLM 可解釋性的數學工具  

---

### **附錄**
- 附錄 A 數學符號表與常用公式  
- 附錄 B PyTorch 與 TensorFlow 的數學實現基礎  
- 附錄 C 語言模型的數學研究前沿  
