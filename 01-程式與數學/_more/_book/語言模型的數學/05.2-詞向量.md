#### **5.2 矩陣分解與詞向量生成**

矩陣分解是自然語言處理（NLP）中的一個重要技術，尤其在生成詞向量（Word Embeddings）方面具有重要應用。矩陣分解的基本思想是將一個大矩陣分解為多個小矩陣，使得這些小矩陣的乘積能夠近似原矩陣。這一過程不僅有助於數據降維，還能夠揭示數據中的潛在結構。在詞嵌入的生成過程中，矩陣分解被用來從語言的共現矩陣中提取語義結構，進而生成能夠表示語義的詞向量。

本節將深入探討矩陣分解在詞向量生成中的應用，並介紹幾個常見的基於矩陣分解的方法，包括奇異值分解（SVD）、GloVe 和其他一些變體。

---

#### **5.2.1 矩陣分解的基本原理**

矩陣分解的核心思想是將一個矩陣  $A$  分解為兩個或多個矩陣的乘積，這樣的分解可以揭示矩陣中的隱含結構。常見的矩陣分解方法包括：

1. **奇異值分解（SVD, Singular Value Decomposition）**：
   奇異值分解是一種常用的矩陣分解方法，將矩陣  $A$  分解為三個矩陣的乘積：


```math
   A = U \Sigma V^T

```

   其中：
   -  $U$  和  $V$  是正交矩陣，包含了原矩陣的左奇異向量和右奇異向量。
   -  $\Sigma$  是對角矩陣，包含了矩陣  $A$  的奇異值。

   在詞嵌入中，我們常用 SVD 來對詞語共現矩陣進行分解，通過保留最大的奇異值來減少矩陣的維度，從而得到低維度的詞向量表示。

2. **非負矩陣分解（NMF, Non-negative Matrix Factorization）**：
   非負矩陣分解是一種限制矩陣元素必須為非負數的分解方法，這對於語言模型中需要保證向量元素為正的情況非常有用。在生成詞向量時，NMF 通過對詞語共現矩陣進行分解，將原矩陣分解為兩個非負矩陣，使得這兩個矩陣的乘積能夠近似原矩陣。

3. **奇異值分解在詞向量生成中的應用**：
   SVD 是矩陣分解中最常見的一種方法，並且被廣泛應用於語言模型中的詞嵌入生成。假設我們有一個語言的共現矩陣  $X$ ，其中每個元素  $X_{ij}$  表示詞  $i$  和詞  $j$  的共現頻率。我們希望通過分解矩陣  $X$ ，生成每個詞的嵌入向量  $v_i$ ，這樣可以將高維的詞語表示轉換為低維的嵌入向量。

   進行 SVD 分解後，我們可以得到：


```math
   X \approx U \Sigma V^T

```

   其中， $U$  和  $V$  分別對應於詞的左奇異向量和右奇異向量，這些向量可以作為詞的嵌入表示。

   通常，我們只保留前  $k$  個奇異值，這樣可以將矩陣的維度減少到  $k$  維，並且在此過程中會保留最重要的語義信息。這樣，通過矩陣分解，我們能夠得到詞的低維度嵌入，這些嵌入向量能夠捕捉到語詞間的語義關聯。

---

#### **5.2.2 GloVe 模型與矩陣分解**

GloVe（Global Vectors for Word Representation）是一種基於矩陣分解的詞嵌入方法，它利用詞語的全局共現統計來生成詞向量。GloVe 的主要思想是，詞語的語義可以從它們與其他詞的共現模式中獲得，而這些共現模式可以表示為一個共現矩陣。

GloVe 的訓練過程包括以下幾個步驟：

1. **構建共現矩陣**：
   假設我們有一個語料庫，首先需要計算詞語之間的共現頻率，並將這些頻率表示為一個共現矩陣  $X$ ，其中  $X_{ij}$  代表詞  $i$  和詞  $j$  的共現頻率。

2. **定義損失函數**：
   GloVe 的目標是將共現矩陣  $X$  分解為兩個矩陣的乘積，使得這兩個矩陣的乘積能夠逼近原始的共現矩陣。為此，GloVe 定義了一個損失函數  $J$ ：


```math
   J = \sum_{i,j=1}^{|V|} f(X_{ij}) \left( v_i^T v_j + b_i + b_j - \log X_{ij} \right)^2

```

   其中：
   -  $v_i$  和  $v_j$  是詞  $i$  和  $j$  的嵌入向量。
   -  $b_i$  和  $b_j$  是偏差項，用來調整詞的嵌入。
   -  $f(X_{ij})$  是一個權重函數，通常設定為  $f(X_{ij}) = (X_{ij} / X_{\max})^\alpha$ ，這樣可以減少高頻詞對模型訓練的影響。

3. **優化損失函數**：
   通過最小化損失函數  $J$ ，GloVe 會學習到詞的低維嵌入向量  $v_i$ ，這些向量能夠捕捉到詞之間的語義關聯。

   GloVe 的特點是，它利用了全局的共現統計信息，而不是像 Word2Vec 一樣依賴局部的上下文窗口。這使得 GloVe 能夠更有效地捕捉語詞之間的長距離關聯。

---

#### **5.2.3 矩陣分解在 LLM 中的應用**

矩陣分解在大型語言模型（LLM）中起著關鍵作用，特別是在學習語言表示和生成高質量的詞向量方面。在 LLM 的訓練過程中，模型通過對大量語料庫的共現矩陣進行分解，從而學習到能夠捕捉語言語義結構的詞向量。

例如，在使用 Transformer 架構的模型（如 GPT 和 BERT）中，矩陣分解的思想被用於對詞嵌入進行初始化和優化。模型的每一層都利用這些詞向量來進行計算，並在反向傳播中不斷調整詞嵌入，使得最終的詞向量能夠準確地反映出語言的語法和語義結構。

---

#### **5.2.4 小結**

- 矩陣分解是生成詞向量的有效數學工具，可以幫助從語言的共現信息中提取語義結構。
- 常見的矩陣分解方法包括奇異值分解（SVD）、非負矩陣分解（NMF）和 GloVe，它們在語言建模中被廣泛應用。
- 矩陣分解方法能夠有效捕捉語詞之間的語義關聯，並為 LLM 的訓練提供強大的數學基礎。

