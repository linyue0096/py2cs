## **2.2 梯度與反傳遞**

---

### **2.2.1 梯度的基本概念**
- **梯度的定義**：  
  - 梯度是多變數函數在每個變數方向上的偏導數組成的向量，描述函數在某點的最大上升方向。  

```math
    \nabla f(x_1, x_2, \dots, x_n) = \left( \frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, \dots, \frac{\partial f}{\partial x_n} \right)

```

- **幾何意義**：  
  梯度的方向是函數值增長最快的方向，梯度的大小表示增長的速率。  

- **梯度在機器學習中的作用**：  
  - 梯度指導模型的參數更新，幫助最小化損失函數。  

---

### **2.2.2 鏈式法則的基本原理**
- **鏈式法則的數學表達**：  
  若  $z$  是  $y$  的函數，且  $y$  是  $x$  的函數，則：

```math
  \frac{dz}{dx} = \frac{dz}{dy} \cdot \frac{dy}{dx}

```

- **多變量鏈式法則**：  
  如果  $z = f(x_1, x_2)$ ，且  $x_1$  和  $x_2$  又分別是  $u$  和  $v$  的函數：

```math
  \frac{\partial z}{\partial u} = \frac{\partial z}{\partial x_1} \cdot \frac{\partial x_1}{\partial u} + \frac{\partial z}{\partial x_2} \cdot \frac{\partial x_2}{\partial u}

```

---

### **2.2.3 梯度與鏈式法則在 LLM 中的應用**
- **梯度的反向傳播**：  
  - **損失函數對參數的梯度計算**：  
    反向傳播算法依賴鏈式法則，從輸出層向輸入層逐層傳遞梯度：  

```math
    \frac{\partial \mathcal{L}}{\partial \mathbf{W}_l} = \frac{\partial \mathcal{L}}{\partial \mathbf{h}_l} \cdot \frac{\partial \mathbf{h}_l}{\partial \mathbf{W}_l}

```

- **神經網絡中的鏈式法則**：  
  - **輸入到輸出的複合函數**：  

```math
    \mathbf{h}_l = f_l(\mathbf{h}_{l-1}, \mathbf{W}_l), \quad \mathcal{L} = g(\mathbf{h}_L)

```
    使用鏈式法則計算：

```math
    \frac{\partial \mathcal{L}}{\partial \mathbf{W}_l} = \frac{\partial \mathcal{L}}{\partial \mathbf{h}_L} \cdot \frac{\partial \mathbf{h}_L}{\partial \mathbf{h}_{l}} \cdot \frac{\partial \mathbf{h}_l}{\partial \mathbf{W}_l}

```

- **Transformer 中的梯度傳播**：  
  - 注意力機制中的梯度計算：  

```math
    \frac{\partial \mathcal{L}}{\partial \mathbf{Q}} = \frac{\partial \mathcal{L}}{\partial \mathbf{Z}} \cdot \frac{\partial \mathbf{Z}}{\partial \mathbf{Q}}

```
    -  $\mathbf{Q}$ 、 $\mathbf{K}$ 、 $\mathbf{V}$ ：查詢、鍵和值向量。  
    -  $\mathbf{Z}$ ：注意力輸出。

---

### **2.2.4 梯度下降與鏈式法則的結合**
- **梯度下降法的更新公式**：  
  - 利用梯度計算參數更新：  

```math
    \theta \leftarrow \theta - \eta \nabla_\theta \mathcal{L}

```
    -  $\eta$ ：學習率。  

- **鏈式法則在深度網絡的應用**：  
  - 深度網絡包含多層非線性映射，梯度的計算需要鏈式法則連接每一層的偏導數。  

---

### **2.2.5 Python 實作：梯度與鏈式法則**
- **自動微分與梯度計算**：  
  ```python
  import torch

  # 定義變數與函數
  x = torch.tensor(3.0, requires_grad=True)
  y = x**2 + 2*x + 1

  # 自動計算梯度
  y.backward()
  print(f"dy/dx = {x.grad}")  # 結果為 2*x + 2
  ```

- **反向傳播的鏈式法則實現**：  
  ```python
  import torch

  # 定義多層函數
  x = torch.tensor(2.0, requires_grad=True)
  h = x**2 + 3
  y = 5 * h + 1

  # 自動計算梯度
  y.backward()
  print(f"dy/dx = {x.grad}")  # 利用鏈式法則計算
  ```

---

### **2.2.6 梯度與鏈式法則的數學視角**
- **梯度與損失函數曲線的幾何意義**：  
  梯度指向損失函數下降最快的方向，幾何上對應曲線的最陡下降路徑。

- **鏈式法則的多層映射解釋**：  
  透過每一層的偏導數逐層傳遞，鏈式法則保證了梯度計算的完整性。

---

### **總結**
- 梯度是損失函數優化的核心，描述了函數在參數空間的變化方向與速率。  
- 鏈式法則作為梯度計算的基礎，支持了深度學習中多層非線性模型的訓練。  
- 理解梯度與鏈式法則不僅有助於掌握模型的數學原理，也為高效實現優化算法提供了基礎。

如果需要補充更深入的推導過程或應用實例，請隨時告訴我！