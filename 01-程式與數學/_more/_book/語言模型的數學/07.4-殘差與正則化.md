#### **7.4 殘差連接與正則化**

---

在深度學習中，當模型變得越來越深時，訓練過程中的梯度消失或梯度爆炸問題可能會使得訓練變得困難。為了解決這些問題，並提高深層神經網絡的表現，**殘差連接（Residual Connection）** 和 **正則化（Regularization）** 技術被引入到許多模型中，尤其是 Transformer 中。這些技術不僅提高了模型的學習能力，還有助於控制過擬合，從而提升模型的泛化能力。

在這一節，我們將探討 **殘差連接** 和 **正則化** 的數學基礎，並介紹它們如何在 Transformer 模型中發揮作用。

---

#### **7.4.1 殘差連接的數學基礎**

**殘差連接** 是一種將網絡層的輸入與其輸出相加的技術。在深度神經網絡中，殘差連接幫助信息在層與層之間傳遞，從而緩解了深層網絡中的梯度消失問題。具體來說，假設有一個層的輸入為  $x$ ，經過一個非線性變換後，該層的輸出為  $F(x)$ ，則殘差連接的數學公式為：


```math
y = F(x) + x

```

這裡， $F(x)$  表示該層的操作（例如，線性變換、激活函數等）， $x$  表示該層的輸入，並且輸出  $y$  是輸入與操作結果的加和。

**殘差連接** 的核心思想是，網絡學到的不是映射  $F(x)$  本身，而是將輸入  $x$  加上一個偏差量  $F(x)$ ，這有助於學習到更簡單的修改，而不需要每一層都從零開始學習。這樣的設計可以使得梯度在反向傳播過程中更容易流動，從而避免梯度消失或梯度爆炸問題。

---

#### **7.4.2 殘差連接在 Transformer 中的應用**

在 Transformer 中，殘差連接被廣泛應用於每個子層（包括自注意力機制和前向全連接層）。每個子層的結構如下：

1. **自注意力層**：每個自注意力層的輸入會經過一系列的操作（如注意力計算、加權求和等），然後與原始輸入進行相加，生成最終輸出。
2. **前向全連接層**：類似地，前向全連接層的輸入會經過非線性變換（如全連接層、激活函數等），然後與原始輸入相加，生成輸出。

具體來說，假設某個子層的輸入為  $x$ ，經過操作後的輸出為  $z$ ，那麼經過殘差連接後的最終輸出  $y$  為：


```math
y = z + x

```

這樣的結構使得每個子層的學習變得更加高效，並且有助於提升模型的深度，因為它緩解了深層網絡中常見的問題，如梯度消失或梯度爆炸。

---

#### **7.4.3 正則化的數學基礎**

正則化是指在訓練過程中添加額外的約束，以防止模型過於複雜而過擬合訓練數據。正則化的主要目的是提高模型的泛化能力，使其能夠在未見過的數據上表現良好。

常見的正則化技術包括：

- **L1 正則化（Lasso）**：
  
  L1 正則化通過對模型的權重進行懲罰來減少不重要的特徵。其數學形式為：


```math
  \mathcal{L}_{L1} = \lambda \sum_i |w_i|

```

  其中  $w_i$  是模型中的權重， $\lambda$  是正則化強度，控制了懲罰項的權重。

- **L2 正則化（Ridge）**：
  
  L2 正則化則對模型權重的平方進行懲罰。其數學形式為：


```math
  \mathcal{L}_{L2} = \lambda \sum_i w_i^2

```

  這樣，較大的權重會被懲罰，使得模型在學習過程中偏向於較小的權重，從而降低過擬合風險。

- **Dropout 正則化**：

  Dropout 是一種隨機丟棄神經元的正則化技術。在每次訓練過程中，隨機將神經網絡中的部分神經元設為 0，這樣可以防止網絡過於依賴某些神經元，並促使其學習更加穩健的特徵。

  Dropout 的數學表示為：給定一層的輸入  $x$ ，其經過 Dropout 操作後的輸出  $y$  為：


```math
  y = x \cdot d

```

  其中  $d$  是一個二進位掩碼向量，其中的每個元素  $d_i$  是隨機生成的，值為 0 或 1，根據給定的丟棄概率。

---

#### **7.4.4 正則化在 Transformer 中的應用**

在 Transformer 模型中，正則化技術被廣泛應用以提高模型的泛化能力。常見的正則化技術包括：

1. **Layer Normalization（層正規化）**：這是一種在每層輸出處進行正規化的技術。它的數學表示為，給定一個向量  $x = [x_1, x_2, \dots, x_n]$ ，其經過層正規化後的輸出為：


```math
   \hat{x_i} = \frac{x_i - \mu}{\sigma} \cdot \gamma + \beta

```

   其中， $\mu$  和  $\sigma$  分別是  $x$  的均值和標準差， $\gamma$  和  $\beta$  是學習的縮放和偏移參數。

2. **Dropout**：在 Transformer 中，Dropout 被應用於多個部分，包括自注意力機制和前向全連接層中，以減少過擬合的風險。

3. **L2 正則化**：在模型的學習過程中，L2 正則化被應用於權重，這有助於控制模型的複雜度，避免過度擬合訓練數據。

---

#### **7.4.5 小結**

殘差連接和正則化技術是 Transformer 模型中兩個關鍵的設計元素，它們在解決深層神經網絡的訓練問題以及提高模型的泛化能力方面發揮了重要作用。殘差連接使得模型能夠更有效地學習深層特徵，而正則化技術則有助於防止過擬合，提高模型在未知數據上的表現。這些技術共同作用，使得 Transformer 成為當前最成功的模型之一。