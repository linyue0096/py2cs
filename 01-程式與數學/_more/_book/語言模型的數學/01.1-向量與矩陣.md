### **1.1 向量與矩陣基礎**

#### **1.1.1 向量的定義與幾何解釋**  
- 向量的數學表示  

```math
  \mathbf{v} = [v_1, v_2, \dots, v_n]

```
- 向量的幾何意義：  
  - 向量表示多維空間中的點或方向。  
  - 範例：詞嵌入中的詞向量如何用向量表示語義。  
- 向量的基本運算：加法、減法、標量乘法。  

#### **1.1.2 向量的範數與距離**
- 範數的概念與類型：  
  -  $L^1$  範數（曼哈頓距離）  

```math
    \|\mathbf{v}\|_1 = \sum_{i=1}^n |v_i|

```
  -  $L^2$  範數（歐幾里得距離）  

```math
    \|\mathbf{v}\|_2 = \sqrt{\sum_{i=1}^n v_i^2}

```
- 向量之間的距離：  
  - 歐幾里得距離  
  - 餘弦相似度：  

```math
    \text{CosSim}(\mathbf{u}, \mathbf{v}) = \frac{\mathbf{u} \cdot \mathbf{v}}{\|\mathbf{u}\|_2 \|\mathbf{v}\|_2}

```
    - 應用：語言模型中詞向量的相似性計算。

#### **1.1.3 矩陣的定義與基本運算**
- 矩陣的數學定義：  

```math
  \mathbf{A} = 
  \begin{bmatrix}
  a_{11} & a_{12} & \dots & a_{1n} \\
  a_{21} & a_{22} & \dots & a_{2n} \\
  \vdots & \vdots & \ddots & \vdots \\
  a_{m1} & a_{m2} & \dots & a_{mn}
  \end{bmatrix}

```
- 矩陣的基本運算：  
  - 加法、減法、標量乘法。  
  - 矩陣乘法與性質：  

```math
    (\mathbf{A} \cdot \mathbf{B})_{ij} = \sum_{k} a_{ik} b_{kj}

```
    - 與神經網絡權重更新的聯繫。  
  - 矩陣的轉置與逆矩陣。  

#### **1.1.4 矩陣與向量的關係**
- 矩陣與向量的乘法：  
  - 將矩陣作為線性變換的工具。  
  - 範例：詞嵌入矩陣的作用。  
- 線性映射的幾何解釋：旋轉、縮放與投影。  

#### **1.1.5 單位矩陣與對角矩陣**
- 單位矩陣（Identity Matrix）：  

```math
  \mathbf{I} = 
  \begin{bmatrix}
  1 & 0 & \dots & 0 \\
  0 & 1 & \dots & 0 \\
  \vdots & \vdots & \ddots & \vdots \\
  0 & 0 & \dots & 1
  \end{bmatrix}

```
  - 特性：任意矩陣乘以單位矩陣等於自身。  
- 對角矩陣（Diagonal Matrix）：  
  - 定義與應用。  
  - 範例：損失權重矩陣設置。  

#### **1.1.6 高維空間的視角**
- 高維向量空間的幾何理解。  
- 內積與投影：理解注意力機制中的數學基礎。  

#### **1.1.7 Python 與線性代數實作**
- 使用 NumPy 實作基本向量與矩陣操作：  
  ```python
  import numpy as np
  
  # 向量與範數
  v = np.array([1, 2, 3])
  norm = np.linalg.norm(v)
  
  # 矩陣運算
  A = np.array([[1, 2], [3, 4]])
  B = np.array([[5, 6], [7, 8]])
  C = np.dot(A, B)  # 矩陣乘法
  ```  
- 使用 PyTorch 實現張量操作與矩陣乘法：  
  ```python
  import torch
  
  # 張量初始化
  A = torch.tensor([[1, 2], [3, 4]])
  B = torch.tensor([[5, 6], [7, 8]])
  
  # 矩陣乘法
  C = torch.matmul(A, B)
  ```

---

以上內容重點在於幫助讀者打好線性代數的基礎，同時通過範例引入其在 LLM 的應用場景，為後續章節鋪墊。如果需要更深入的細化或增加範例，可以再進一步補充！