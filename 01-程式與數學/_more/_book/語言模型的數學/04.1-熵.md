
#### **4.1 熵與交叉熵的數學解釋**

信息論是數據科學、機器學習和自然語言處理領域中的核心概念之一。它主要關心如何測量和處理信息的傳遞。熵（Entropy）和交叉熵（Cross-entropy）是信息論中最基本且最重要的概念之一，尤其在訓練語言模型（LLM）時具有廣泛應用。

本章將深入探討熵和交叉熵的數學背景，以及它們如何應用於語言模型中。

---

#### **4.1.1 熵（Entropy）的定義與數學背景**

熵是由克勞德·香農（Claude Shannon）於 1948 年提出的，並被用來量化隨機變數不確定性的大小。熵越大，系統中不確定性越高。反之，熵越小，則表示系統越確定。

對於一個離散隨機變數  $X$ ，其概率分布為  $P(x)$ ，熵  $H(X)$  定義為：


```math
H(X) = - \sum_{x \in X} P(x) \log P(x)

```

其中：
-  $P(x)$  是隨機變數  $X$  取值為  $x$  的概率。
-  $\log$  是以 2 為底的對數，這使得熵的單位是比特（bit）。

熵度量了從隨機變數中觀察到一個特定事件所需的最小位數。在語言模型中，熵衡量的是某一詞序列的預測不確定性。如果一個語言模型能夠高效地預測出下一個詞，則該模型的熵會較低。

例如，假設我們有一個簡單的概率分布：


```math
P(X) = \{0.5, 0.25, 0.25\}

```

則熵為：


```math
H(X) = - (0.5 \log 0.5 + 0.25 \log 0.25 + 0.25 \log 0.25)

```

```math
H(X) = - (0.5 \times -1 + 0.25 \times -2 + 0.25 \times -2) = 1.5 \text{ bits}

```

#### **4.1.2 熵的意義與語言模型中的應用**

熵在語言模型中的重要性在於它反映了模型對於下一個詞的預測不確定性。假設我們的語言模型在預測一段文本中的下一個詞時，每個詞的預測概率可以看作是一個隨機變數。語言模型的目標是減少熵，也就是讓模型的預測越來越精確，從而提高語言生成的效率和準確度。

在訓練語言模型時，我們希望模型學到的參數能夠最大限度地減少每次預測的熵，這樣才能準確地生成自然語言。

#### **4.1.3 交叉熵（Cross-entropy）**

交叉熵是衡量兩個概率分布之間差異的一種指標，特別是用來評估一個概率分布  $Q$  與真實分布  $P$  之間的差異。交叉熵的數學公式為：


```math
H(P, Q) = - \sum_{x \in X} P(x) \log Q(x)

```

其中：
-  $P(x)$  是真實分布，即數據的實際分布。
-  $Q(x)$  是模型預測的概率分布。

交叉熵是對模型預測的損失度量，當預測分布  $Q$  與真實分布  $P$  越接近時，交叉熵的值越小。如果模型的預測越準確，交叉熵越低。

#### **4.1.4 交叉熵與熵的區別**

- 熵衡量的是隨機變數  $X$  的內在不確定性，通常指的是在沒有任何先驗知識的情況下，系統的固有“信息量”。
- 交叉熵則衡量的是兩個概率分布（真實分布和預測分布）之間的差異。在語言模型中，交叉熵常用來衡量模型的預測結果與真實數據分布之間的誤差。

#### **4.1.5 交叉熵在語言模型中的應用**

在訓練語言模型時，交叉熵通常作為損失函數來進行優化。對於一個序列模型（例如 RNN 或 Transformer），我們希望最小化模型的預測分布  $Q$  與實際文本的真實分布  $P$  之間的交叉熵。具體來說，對於給定的語句，模型的預測分布  $Q$  會在每個時間步生成一個詞的概率分佈。

假設語句為  $w_1, w_2, \dots, w_T$ ，那麼語言模型的交叉熵可以寫為：


```math
H(P, Q) = - \sum_{t=1}^T \log P(w_t | w_1, w_2, \dots, w_{t-1})

```

這樣，語言模型的目標就是最小化這個交叉熵，即提高對語言序列的預測準確度。

#### **4.1.6 交叉熵與對數損失函數**

交叉熵損失函數通常在深度學習中用來訓練分類模型。在語言模型中，交叉熵損失用來衡量模型預測的詞分布與實際詞分布的偏差，並且在每次訓練步驟中，這一損失函數會被用來計算梯度，從而更新模型參數。

交叉熵損失函數的形式為：


```math
L = - \sum_{i=1}^{N} y_i \log \hat{y}_i

```

其中：
-  $y_i$  是實際標籤（目標詞）。
-  $\hat{y}_i$  是模型預測的概率分佈。

#### **4.1.7 Python 實作：交叉熵計算**

下面是計算交叉熵的 Python 代碼，適用於語言模型訓練中的損失計算。

```python
import numpy as np
from sklearn.metrics import log_loss

# 假設實際分布為P，模型預測為Q
y_true = [1, 0, 0, 1]  # 實際標籤 (例如: [詞A, 詞B, 詞C, 詞D])
y_pred = [0.7, 0.2, 0.1, 0.9]  # 模型預測的概率 (模型對每個詞的預測概率)

# 計算交叉熵損失
cross_entropy_loss = log_loss(y_true, y_pred)
print(f"Cross-entropy Loss: {cross_entropy_loss}")
```

這段代碼展示了如何計算交叉熵損失，該損失衡量模型的預測與實際標籤之間的差異。

#### **4.1.8 小結**

- **熵**是衡量隨機變數不確定性的指標，在語言模型中，熵反映了模型對下一個詞預測的難度。
- **交叉熵**衡量的是模型預測分布與真實分布之間的差異，是語言模型訓練中常用的損失函數。
- 在訓練語言模型時，我們通常目標是最小化交叉熵損失，從而提高模型的預測準確性和生成能力。

熵和交叉熵是語言模型訓練和性能評估中不可或缺的數學工具，它們幫助我們理解和優化模型的預測能力。