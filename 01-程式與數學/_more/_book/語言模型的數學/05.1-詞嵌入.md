#### **第 5 章 自然語言處理數學基礎**

自然語言處理（NLP）是讓計算機理解、解釋和生成自然語言的一門技術。這一領域涉及的數學基礎包括向量空間模型、概率論、最優化方法等，對於語言模型（LLM）的訓練和應用至關重要。本章將介紹一些自然語言處理中的核心數學概念，並深入探討它們在語言表示、詞嵌入及更高層次的語言理解中的應用。

---

#### **5.1 語言表示與詞嵌入（Word Embeddings）**

詞嵌入（Word Embedding）是自然語言處理中的一個關鍵概念，它是將語言中的詞轉換為向量的數學技術。這些向量捕捉了詞之間的語義關係，使得計算機能夠處理文本中的語言結構與語義。

傳統的語言表示方法（如詞袋模型）將每個詞視為一個單獨的符號，而詞嵌入則將每個詞映射到一個多維的向量空間中，這些向量能夠有效地捕捉語詞間的相似性和關聯性。以下是詞嵌入方法的數學基礎及其應用。

---

#### **5.1.1 詞嵌入的數學背景**

詞嵌入的核心思想是將每個詞表示為一個實數向量，這些向量具有以下特點：

- 向量之間的距離反映了詞語之間的語義相似度。
- 這些向量可以通過數據驅動的學習過程（如神經網絡訓練）來獲得，並且能夠捕捉詞語的上下文信息。

在數學上，詞嵌入通常使用一個映射函數  $f: V \to \mathbb{R}^d$  來表示，其中：
-  $V$  是詞彙表，包含所有可能的詞。
-  $d$  是嵌入的維度，通常是選擇一個較小的數值，從而使得模型的計算效率更高。

詞嵌入的學習目標是通過優化一個損失函數，使得語言模型能夠最小化詞向量表示和上下文之間的差異。

---

#### **5.1.2 典型的詞嵌入方法**

目前，最常用的詞嵌入方法包括：

1. **Word2Vec**:
   Word2Vec 是由 Mikolov 等人於 2013 年提出的一種基於神經網絡的詞嵌入方法。Word2Vec 有兩種主要的訓練架構：
   - **CBOW（Continuous Bag of Words）**：根據上下文詞預測中心詞。
   - **Skip-gram**：根據中心詞預測上下文詞。

   這些模型的核心思想是學習一個詞向量，使得在給定上下文的情況下，能夠最大化中心詞出現的概率。數學上，Word2Vec 的目標是最小化負對數似然（Negative Log-Likelihood, NLL）損失。

   假設  $c$  是上下文， $w$  是中心詞，則 Skip-gram 模型的目標是最大化下式的概率：


```math
   \prod_{c \in \text{context}} P(w|c) = \prod_{c \in \text{context}} \frac{e^{v_w \cdot v_c}}{\sum_{w' \in V} e^{v_{w'} \cdot v_c}}

```

   其中：
   -  $v_w$  和  $v_c$  分別是詞  $w$  和  $c$  的向量表示。
   -  $V$  是詞彙表， $\cdot$  是內積操作。

2. **GloVe (Global Vectors for Word Representation)**:
   GloVe 是由斯坦福大學提出的一種基於全局統計的詞嵌入方法。與 Word2Vec 不同，GloVe 通過矩陣分解的方式學習詞嵌入。GloVe 的核心思想是將詞與詞之間的共現矩陣  $X$  分解為兩個矩陣  $W$  和  $H$ ，使得：


```math
   X_{ij} \approx \frac{(W_i \cdot H_j^T)}{Z_i}

```

   其中：
   -  $X_{ij}$  是詞  $i$  和詞  $j$  之間的共現頻次。
   -  $W_i$  和  $H_j$  分別是詞  $i$  和詞  $j$  的嵌入向量。
   -  $Z_i$  是一個正規化項，用來調整不同詞語的出現頻率。

3. **FastText**:
   FastText 是 Facebook 提出的一種基於 Word2Vec 改進的詞嵌入方法，FastText 在訓練詞嵌入時不僅考慮單個詞，還考慮詞的子字元。這使得 FastText 對處理詞形變化（如詞綴）具有優勢，特別是在低資源語言中。

   數學上，FastText 的模型與 Word2Vec 類似，只不過 FastText 的詞向量是由詞的子字元組成的。這意味著，每個詞的嵌入向量是由所有包含該詞的子字元向量的總和構成。

---

#### **5.1.3 詞嵌入在語言模型中的應用**

詞嵌入在語言模型中起著關鍵作用。語言模型的目標是學習詞與詞之間的語義關係，詞嵌入則是實現這一目標的數學工具。詞嵌入能夠將語言中的詞轉換為向量，這些向量可以捕捉語義相似性、語法結構等。

在訓練語言模型時，詞嵌入作為輸入層提供了文本的數字表示，這些嵌入向量可以直接輸入到神經網絡中。隨著網絡的訓練，詞向量將學習到更多語言特徵，並且可以應用於各種 NLP 任務，如：

- **文本分類**：詞嵌入能夠將文本中的關鍵詞轉換為向量，進行情感分析或主題識別。
- **命名實體識別**（NER）：詞嵌入幫助模型識別文本中的特定實體（如人名、地名等）。
- **機器翻譯**：詞嵌入使得模型能夠在不同語言之間進行高效的詞對齊與翻譯。
- **語音識別與生成**：詞嵌入能夠有效捕捉語音到文本的映射關係。

---

#### **5.1.4 動態詞嵌入與上下文感知**

傳統的詞嵌入（如 Word2Vec 和 GloVe）將每個詞固定為一個靜態向量，但在許多 NLP 任務中，詞的語義是會根據上下文發生變化的。為了解決這一問題，現代的上下文感知詞嵌入方法（如 BERT、GPT、ELMo）應運而生。

上下文感知詞嵌入會根據句子中詞的上下文動態地生成詞向量，這使得同一個詞在不同上下文中的表示可以不同，從而更準確地捕捉語義。例如，BERT 使用雙向 Transformer 編碼器來生成基於上下文的詞嵌入。

這些方法在處理語言的多義性和細粒度語義時具有顯著的優勢，使得語言模型能夠理解更複雜的語言結構和語義關聯。

---

#### **5.1.5 小結**

- 詞嵌入是將詞語映射為實數向量的數學方法，能夠捕捉語義和語法結構。
- 常見的詞嵌入方法包括 Word2Vec、GloVe 和 FastText，它們分別基於神經網絡和矩陣分解技