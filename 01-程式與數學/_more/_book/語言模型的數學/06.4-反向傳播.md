#### **6.4 反向傳播與權重更新**

反向傳播（Backpropagation）是神經網絡訓練中最重要的算法之一，它使得神經網絡能夠通過梯度下降法自動地調整網絡權重，以最小化損失函數。這一過程依賴於微積分中的鏈式法則，通過計算損失函數對網絡每一層權重的偏導數，進行梯度下降更新權重。

本節將詳細介紹反向傳播的數學原理，包括其計算過程、權重更新的公式及常見的優化技術。

---

#### **6.4.1 反向傳播的數學原理**

反向傳播的基本思想是利用損失函數對神經網絡參數（權重和偏置）進行微分，計算每個參數對損失函數的貢獻，然後通過梯度下降法來更新這些參數。具體步驟如下：

1. **正向傳播（Forward Propagation）**
   
   首先，在每一層中計算神經元的加權總和和激活函數的輸出：


```math
   z^{(l)} = W^{(l)} a^{(l-1)} + b^{(l)}

```

```math
   a^{(l)} = f(z^{(l)})

```
   其中：
   -  $W^{(l)}$  是第  $l$  層的權重矩陣。
   -  $a^{(l-1)}$  是第  $l-1$  層的激活值。
   -  $b^{(l)}$  是第  $l$  層的偏置項。
   -  $f$  是激活函數。

   最後，計算網絡的預測輸出  $\hat{y}$ ：


```math
   \hat{y} = a^{(L)}

```
   其中  $L$  是神經網絡的總層數。

2. **損失函數計算（Loss Calculation）**
   
   計算模型的損失函數，通常使用均方誤差（MSE）或交叉熵損失函數。例如，對於二分類問題，損失函數可以定義為交叉熵：


```math
   L = - \left( y \log(\hat{y}) + (1 - y) \log(1 - \hat{y}) \right)

```
   其中：
   -  $y$  是真實標籤， $\hat{y}$  是模型的預測輸出。

3. **反向傳播（Backpropagation）**
   
   在正向傳播完成後，反向傳播算法會從輸出層開始，逐層計算損失函數對每一層權重的梯度。這個過程依賴於鏈式法則（Chain Rule）來計算每一層的梯度。

   反向傳播的核心是計算每一層的誤差（error）並利用這些誤差來調整權重：


```math
   \delta^{(l)} = \frac{\partial L}{\partial a^{(l)}} \cdot \frac{\partial a^{(l)}}{\partial z^{(l)}} = \frac{\partial L}{\partial z^{(l)}}

```

   其中：
   -  $\delta^{(l)}$  是第  $l$  層的誤差項。
   -  $\frac{\partial L}{\partial a^{(l)}}$  是損失對第  $l$  層激活值的偏導數。
   -  $\frac{\partial a^{(l)}}{\partial z^{(l)}}$  是激活函數對加權和的導數。

   接下來，對於每一層的權重和偏置，根據誤差來計算梯度：


```math
   \frac{\partial L}{\partial W^{(l)}} = \delta^{(l)} \cdot (a^{(l-1)})^T

```

```math
   \frac{\partial L}{\partial b^{(l)}} = \delta^{(l)}

```

   這些偏導數將用於更新權重和偏置。

4. **權重更新（Weight Update）**

   根據計算出的梯度，使用梯度下降法來更新網絡中的權重和偏置。更新公式如下：


```math
   W^{(l)} \leftarrow W^{(l)} - \eta \frac{\partial L}{\partial W^{(l)}}

```

```math
   b^{(l)} \leftarrow b^{(l)} - \eta \frac{\partial L}{\partial b^{(l)}}

```

   其中：
   -  $\eta$  是學習率，它控制每次更新的步長。

   在每次反向傳播過程中，權重會根據計算出的梯度進行調整，最終使得損失函數達到最小。

---

#### **6.4.2 梯度下降法與變體**

在實際應用中，反向傳播通常與梯度下降法（Gradient Descent）一起使用。梯度下降法通過迭代更新權重，使損失函數逐步降低。根據不同的更新策略，梯度下降法有不同的變體：

1. **隨機梯度下降（SGD）**

   隨機梯度下降每次使用一個樣本來計算梯度並更新權重。其更新公式為：


```math
   \theta \leftarrow \theta - \eta \nabla_{\theta} L(\theta)

```

   儘管SGD計算速度較快，但它的更新可能會很不穩定，容易造成震盪。

2. **小批量梯度下降（Mini-batch SGD）**

   小批量梯度下降每次使用一小批樣本來計算梯度並更新權重，這樣既能提高計算效率，又能保持一定的穩定性。

3. **動量法（Momentum）**

   動量法通過引入過去梯度的加權平均來幫助更新，使得梯度更新更平滑，避免了震盪。其更新公式為：


```math
   v^{(t)} = \beta v^{(t-1)} + (1 - \beta) \nabla_{\theta} L(\theta)

```

```math
   \theta^{(t)} = \theta^{(t-1)} - \eta v^{(t)}

```

   其中， $\beta$  是動量係數。

4. **Adam（Adaptive Moment Estimation）**

   Adam 是一種自適應學習率的優化算法，它結合了動量法和自適應學習率的思想，能夠根據每個參數的梯度一階和二階矩估計來調整學習率。Adam 更新公式為：


```math
   m^{(t)} = \beta_1 m^{(t-1)} + (1 - \beta_1) \nabla_{\theta} L(\theta)

```

```math
   v^{(t)} = \beta_2 v^{(t-1)} + (1 - \beta_2) (\nabla_{\theta} L(\theta))^2

```

```math
   \hat{m}^{(t)} = \frac{m^{(t)}}{1 - \beta_1^t}, \quad \hat{v}^{(t)} = \frac{v^{(t)}}{1 - \beta_2^t}

```

```math
   \theta^{(t)} = \theta^{(t-1)} - \eta \frac{\hat{m}^{(t)}}{\sqrt{\hat{v}^{(t)}} + \epsilon}

```

   其中  $\epsilon$  是為了防止除以零而加入的小常數。

---

#### **6.4.3 小結**

本節介紹了反向傳播算法的數學原理，包括正向傳播、反向傳播、梯度計算及權重更新的過程。反向傳播是神經網絡訓練的核心，通過鏈式法則計算每一層的梯度並利用梯度下降法更新權重。進一步了解各種梯度下降法及其變體（如動量法、Adam）對於提升訓練效果至關重要。反向傳播算法的優化使得深度學習模型能夠在複雜任務中取得優異的表現。