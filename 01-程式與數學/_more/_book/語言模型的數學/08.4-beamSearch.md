#### **8.4 Beam Search 與 Top-k 生成方法**

---

在語言生成模型中，解碼策略是決定模型生成文本質量和多樣性的重要因素之一。貪心解碼（Greedy Decoding）雖然速度快，但可能會因為局部最優解的選擇而導致生成文本的質量下降。為了克服這一問題，常見的解碼策略包括 **束搜索（Beam Search）** 和 **Top-k 生成方法**，這兩種方法能夠在生成過程中增加更多的多樣性和全局優化的可能性。

在本節中，我們將詳細介紹這兩種解碼方法，並探討它們在語言模型中的應用。

---

#### **8.4.1 Beam Search 解碼方法**

**束搜索（Beam Search）** 是一種在生成過程中同時探索多條候選路徑的解碼方法。與貪心解碼（只選擇當前步驟最有可能的詞語）不同，束搜索會保留多條最有可能的候選序列，並在每個生成步驟後擴展這些序列，從而能夠在一定程度上避免陷入局部最優解，生成更加合理的文本。

**Beam Search 的基本過程**：

1. **初始化**：
   - 從初始詞（通常是 `<s>` 或其他起始符號）開始，將其放入候選序列中。
   - 設定一個束大小  $B$ ，即每步保留的候選序列數量。

2. **逐步生成**：
   - 在每一步中，從每個當前候選序列的末尾詞語出發，計算所有可能的下個詞的概率。
   - 對每個候選序列，計算所有可能擴展詞語的總概率（通常是對數概率），並選擇  $B$  個概率最大的序列作為新的候選。

3. **終止條件**：
   - 當生成的序列達到最大長度或所有候選序列都包含終止符（例如 `</s>`）時，停止生成。
   - 最終選擇概率最高的序列作為最終生成結果。

**Beam Search 的數學原理**：

束搜索每步選擇的候選序列基於其累積概率（或對數概率）。假設在第  $t$  步選擇的序列為  $s_t = (x_1, x_2, \dots, x_t)$ ，對應的概率為：


```math
P(s_t) = P(x_1) \cdot P(x_2 | x_1) \cdot P(x_3 | x_1, x_2) \cdot \dots \cdot P(x_t | x_1, x_2, \dots, x_{t-1})

```

在每一步，束搜索會選擇對數概率最大的  $B$  個序列進行擴展。計算過程通常基於對數概率進行，這樣可以避免數值下溢問題，並確保運算的穩定性。

**束搜索的優點與缺點**：

- **優點**：
  - 能夠避免貪心解碼中的局部最優解，探索更多的解碼路徑。
  - 在保持計算效率的同時，能夠生成質量較高的文本。

- **缺點**：
  - 計算量較大，尤其是當束大小  $B$  增大時，運算量呈指數增長。
  - 如果束大小設置過小，仍然可能無法充分探索整個搜索空間，造成生成質量下降。

---

#### **8.4.2 Top-k 生成方法**

**Top-k 生成方法** 是另一種常用的生成策略，其核心思想是每一步從模型的預測中選擇概率最大的  $k$  個詞語進行採樣，而不是只選擇最有可能的詞語。這樣可以有效增加生成文本的多樣性，避免模型生成過於單一或重複的內容。

**Top-k 生成的基本過程**：

1. **初始化**：
   - 從初始詞（例如 `<s>`）開始生成。

2. **逐步生成**：
   - 在每個步驟中，從當前的輸出概率分佈中選擇概率最大的  $k$  個詞語，並將其作為候選詞語。
   - 隨機從這  $k$  個候選詞中選擇一個詞語作為當前步驟的輸出。
   
3. **終止條件**：
   - 當生成的序列達到最大長度或生成終止符（例如 `</s>`）時，停止生成。

**Top-k 生成的數學原理**：

假設在第  $t$  步，模型預測出所有詞語的概率分佈  $P(x_t | x_1, x_2, \dots, x_{t-1})$ 。Top-k 策略會從中選擇出概率前  $k$  個最大值，這些最大值的詞語將被保留下來，其他的詞語則被丟棄。


```math
P(x_t \in \text{Top-k} | x_1, x_2, \dots, x_{t-1}) = \frac{\exp(\mathbf{w}_{x_t} \cdot \mathbf{h}_{t-1})}{\sum_{x_t \in \text{Top-k}} \exp(\mathbf{w}_{x_t} \cdot \mathbf{h}_{t-1})}

```

然後，從這  $k$  個選擇的詞語中隨機抽取一個詞，並將其作為下一個步驟的輸出。

**Top-k 生成的優點與缺點**：

- **優點**：
  - 能夠提高生成文本的多樣性，尤其是相較於貪心解碼和束搜索。
  - 不需要額外的計算資源來維持多條候選序列（如束搜索中的候選序列數量）。

- **缺點**：
  - 可能會生成語法結構較差或不合邏輯的文本，尤其是在  $k$  設置較大時。
  - 生成過程具有一定的隨機性，可能導致生成文本的可預測性較差。

---

#### **8.4.3 Top-p（Nucleus）采樣**

**Top-p 采樣**（又稱為 Nucleus Sampling）是對 Top-k 生成方法的一種改進。與 Top-k 只考慮選擇概率前  $k$  個詞語不同，Top-p 根據詞語的累積概率來決定選擇哪些詞語。具體來說，Top-p 會選擇最小的一組詞語，使得它們的累積概率達到預設的閾值  $p$ （通常是 0.9 或 0.95）。

**Top-p 生成的基本過程**：

1. 計算所有詞語的概率。
2. 排序詞語的概率並累積概率，直到累積概率大於或等於  $p$ 。
3. 從這些累積概率達到閾值的詞語中隨機選擇一個作為當前的輸出。

Top-p 生成可以在一定程度上增加生成文本的多樣性，同時避免 Top-k 生成中可能出現的過多無關詞語。

---

#### **8.4.4 小結**

Beam Search 和 Top-k 生成方法是現代語言生成模型中常用的兩種解碼策略。Beam Search 通過探索多條候選路徑來提高生成結果的質量，但計算開銷較大；Top-k 生成則通過限制每一步的候選詞數量來提高多樣性，但可能會犧牲一定的生成質量。Top-p（Nucleus）采樣是一種更靈活的方法，它根據累積概率進行詞語選擇，能夠平衡生成文本的多樣性和合理性。理解這些解碼方法有助於我們根據不同的需求選擇合適的生成策略，從而生成符合期望的文本內容。