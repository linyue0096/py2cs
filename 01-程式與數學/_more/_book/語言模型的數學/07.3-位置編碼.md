#### **7.3 位置編碼的數學基礎**

---

在自然語言處理中，序列數據的順序信息是至關重要的，尤其是在處理文本、語音等具有明確順序的數據時。傳統的神經網絡，如 RNN 和 LSTM，能夠通過其循環結構來捕捉序列中的順序信息。然而，Transformer 模型並不具備內建的順序感知能力，這意味著它無法直接了解輸入數據中元素的相對或絕對位置。因此，Transformer 引入了**位置編碼（Positional Encoding）**來提供位置信息。

本節將深入探討位置編碼的數學原理，並解析其如何在 Transformer 模型中實現順序感知。

---

#### **7.3.1 位置編碼的基本概念**

位置編碼的核心思想是將每個輸入元素的位置轉換為一個向量，並將這些位置向量添加到模型的輸入中。這樣，儘管 Transformer 只關注單詞（或子詞）的內容，通過位置編碼，它仍能夠了解每個單詞在序列中的位置。具體來說，位置編碼會被加到輸入的詞向量上，從而形成帶有位置感知的輸入向量。

數學上，位置編碼向量是基於位置索引  $p$  和嵌入維度  $d$  計算的。假設輸入序列的長度為  $n$ ，每個位置  $p$  的位置編碼  $PE(p)$  是一個與輸入向量相同維度的向量。

---

#### **7.3.2 位置編碼的數學公式**

在 Transformer 中，位置編碼是基於正弦和餘弦函數的組合來生成的。這樣的設計是為了使位置編碼在不同的維度上有不同的變化頻率，從而提供豐富的位置信息。具體公式如下：

對於序列中的每個位置  $p$  和嵌入維度中的每個維度  $2i$  和  $2i+1$ ，位置編碼的計算公式為：


```math
PE(p, 2i) = \sin\left(\frac{p}{10000^{\frac{2i}{d}}}\right)

```

```math
PE(p, 2i+1) = \cos\left(\frac{p}{10000^{\frac{2i+1}{d}}}\right)

```

其中：
-  $p$  是位置索引，表示該位置在序列中的位置， $p \in \{0, 1, 2, \dots, n-1\}$ 。
-  $i$  是位置編碼向量的維度索引， $i \in \{0, 1, \dots, \frac{d}{2}-1\}$ 。
-  $d$  是位置編碼的維度，也是輸入詞嵌入向量的維度。

上述公式的關鍵是：
- 使用正弦函數  $\sin$  和餘弦函數  $\cos$  來生成位置編碼。
- 每個維度  $2i$  使用正弦函數，維度  $2i+1$  使用餘弦函數。
- 位置的編碼依賴於一個分母  $10000^{\frac{2i}{d}}$ ，這個分母確保了不同維度的頻率不同，使得低維度的編碼對應於較大的範圍（較慢的變化），而高維度的編碼則對應於較小的範圍（較快的變化）。

這樣生成的每個位置編碼向量都包含了序列中元素的位置信息，並且不同位置的編碼在每個維度上都有不同的頻率，從而提供了對序列順序的充分表徵。

---

#### **7.3.3 位置編碼的加法運算**

在實際的 Transformer 模型中，位置編碼是與詞嵌入向量進行加法運算的。假設詞嵌入矩陣為  $E$ ，位置編碼矩陣為  $PE$ ，那麼對於序列中的第  $p$  個位置，其最終的輸入向量  $x_p$  會是詞嵌入向量和位置編碼向量的逐元素相加：


```math
x_p = E(p) + PE(p)

```

這樣，位置編碼向量就被加到了每個詞向量中，使得每個詞向量同時攜帶了語言信息和位置信息，從而使得 Transformer 模型能夠捕捉到序列的順序結構。

---

#### **7.3.4 位置編碼的優勢**

1. **無需循環結構**：
   位置編碼使得 Transformer 模型能夠感知序列順序，而不需要像 RNN 或 LSTM 那樣依賴循環結構來捕捉順序信息。這也使得 Transformer 在計算上更具並行性，並且能夠更高效地處理長序列。

2. **固定的數學結構**：
   使用正弦和餘弦函數作為位置編碼的數學形式，能夠保證編碼的規律性和穩定性。這種結構使得模型能夠捕捉到不同位置間的相對關係，並能夠在訓練中穩定地學習到位置編碼對模型性能的貢獻。

3. **位置編碼的可視化**：
   由於位置編碼是基於正弦和餘弦函數的，對應的圖像在不同的維度上會呈現出周期性的變化。這使得位置編碼在直觀上容易理解，並且能夠很好地捕捉到不同位置之間的相對信息。

---

#### **7.3.5 小結**

位置編碼是 Transformer 模型中解決順序問題的關鍵技術。通過利用正弦和餘弦函數，位置編碼為每個序列元素提供了唯一的位置信息，使得模型能夠感知序列中的順序。這種設計不僅提高了模型的計算效率，還使得 Transformer 能夠在捕捉序列關係時保持較好的表達能力和可擴展性。