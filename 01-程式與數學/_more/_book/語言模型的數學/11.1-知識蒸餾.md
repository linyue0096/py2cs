### **11.1 知識蒸餾的數學原理**

知識蒸餾（Knowledge Distillation）是一種通過訓練較小的學生模型（Student Model）來模仿較大教師模型（Teacher Model）的行為的模型壓縮方法。它主要用於減少模型的參數量，同時儘可能保留原模型的性能。以下是其數學原理的詳細解析。

---

#### **教師模型與學生模型的定義**

1. **教師模型（Teacher Model）：**  
   - 訓練好的大模型，通常具有較高的準確率，但參數量大，運行成本高。
   - 給定輸入  $x$ ，教師模型產生輸出概率分佈  $q = \text{Softmax}(z_T / T)$ ，其中  $z_T$  是教師模型的 logits， $T$  是溫度參數。

2. **學生模型（Student Model）：**  
   - 較小的模型，目標是學習教師模型的輸出分佈。
   - 給定相同的輸入  $x$ ，學生模型產生概率分佈  $p = \text{Softmax}(z_S / T)$ ，其中  $z_S$  是學生模型的 logits。

---

#### **目標：匹配概率分佈**

知識蒸餾的目標是使學生模型的輸出分佈  $p$  與教師模型的輸出分佈  $q$  盡可能接近。這可以通過最小化兩個分佈之間的距離來實現，通常採用 **KL 散度（Kullback-Leibler Divergence）** 作為損失函數的一部分。

---

#### **損失函數設計**

知識蒸餾的損失函數通常由兩部分組成：

1. **硬目標損失（Hard Label Loss）：**  
   使用真實標籤  $y$  訓練學生模型，例如交叉熵損失：

```math
   \mathcal{L}_{\text{hard}} = - \sum_{i} y_i \log p_i

```
   這部分強調學生模型的分類能力。

2. **軟目標損失（Soft Label Loss）：**  
   使用教師模型的軟目標輸出  $q$  作為目標，通過 KL 散度度量學生模型與教師模型的相似性：

```math
   \mathcal{L}_{\text{soft}} = T^2 \cdot \text{KL}(q || p) = T^2 \sum_{i} q_i \log \frac{q_i}{p_i}

```
   其中，溫度  $T$  用於平滑 logits，使概率分佈的細微差異更加明顯。

3. **總損失：**  
   知識蒸餾損失是上述兩部分的加權和：

```math
   \mathcal{L} = \alpha \mathcal{L}_{\text{hard}} + (1 - \alpha) \mathcal{L}_{\text{soft}}

```
   其中  $\alpha$  是權重係數，用於平衡硬目標和軟目標的影響。

---

#### **溫度參數  $T$  的作用**

1. **溫度平滑效應：**  
   溫度  $T$  提高 logits 的平滑程度，公式如下：

```math
   q_i = \frac{\exp(z_{T,i} / T)}{\sum_j \exp(z_{T,j} / T)}

```
   當  $T > 1$  時，概率分佈更加均勻，小概率事件的重要性增加。

2. **優化穩定性：**  
   高溫度的平滑效果使模型更容易捕捉教師模型輸出中的隱含關係，從而幫助學生模型更穩定地學習。

---

#### **知識蒸餾的數學本質**

1. **信息傳遞：**  
   知識蒸餾本質上是教師模型向學生模型傳遞關於數據分佈的額外信息，特別是類與類之間的相對關係。

2. **正則化效果：**  
   軟目標損失充當一種正則化手段，有助於減少學生模型過度擬合真實標籤的風險。

3. **約束最優化問題：**  
   從數學角度，知識蒸餾是求解一個約束條件下的最優化問題，其中損失函數平衡了硬目標和軟目標的需求。

---

#### **應用與實踐建議**

- **適當選擇溫度  $T$ ：**  
   $T$  通常在 1 到 10 之間選擇，需根據具體數據集和模型調參。

- **權重平衡  $\alpha$ ：**  
  權重  $\alpha$  應根據硬目標和軟目標的重要性分配，一般  $\alpha \approx 0.5$ 。

- **模型兼容性：**  
  教師模型和學生模型的結構不必相同，但輸出的維度需要一致。

---

知識蒸餾的數學原理為語言模型壓縮提供了強大的工具。通過合理設計損失函數和平衡參數，學生模型可以在保持性能的同時，大幅降低計算資源需求。