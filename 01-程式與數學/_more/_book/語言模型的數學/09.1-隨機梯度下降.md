

#### **9.1 梯度下降變體（SGD、Adam、RMSprop）**

梯度下降（Gradient Descent, GD）是最常用的優化方法，通過計算損失函數對模型參數的梯度，並根據這些梯度更新參數。然而，標準的梯度下降方法可能會存在收斂速度慢、容易陷入局部最優解等問題。因此，許多梯度下降的變體被提出來，旨在提高收斂速度和優化效果。

##### **9.1.1 隨機梯度下降（SGD）**

隨機梯度下降（Stochastic Gradient Descent, SGD）是梯度下降的一個變體，它並不是每次都使用整個訓練集來計算梯度，而是隨機選擇一個訓練樣本來計算梯度並更新參數。這樣做能夠提高計算效率，尤其在處理大規模數據集時。

**SGD 的數學原理**：

對於一個簡單的損失函數  $L(\theta)$ ，其中  $\theta$  代表模型參數，標準梯度下降的更新公式為：


```math
\theta_{t+1} = \theta_t - \eta \nabla_\theta L(\theta_t)

```

其中：
-  $\eta$  是學習率（learning rate），控制每次更新的步長。
-  $\nabla_\theta L(\theta_t)$  是損失函數對參數  $\theta$  的梯度。

在隨機梯度下降中，每次更新使用的是一個訓練樣本  $x_i$ ，而不是整個訓練集。這使得每次更新的方向不會太精確，但能夠更快地進行迭代。

**優點與缺點**：
- **優點**：
  - 計算效率高，適用於大規模數據集。
  - 由於隨機性，容易跳出局部最優解。
- **缺點**：
  - 收斂過程中波動較大，可能需要較長時間才能穩定收斂。
  - 需要仔細調整學習率。

---

##### **9.1.2 RMSprop（Root Mean Square Propagation）**

RMSprop 是一種基於自適應學習率的方法，它在 SGD 的基礎上進行改進。RMSprop 將每個參數的學習率進行調整，使得對於頻繁更新的參數，學習率會自動減小，而對於更新較少的參數，學習率則較大。

**RMSprop 的數學原理**：

RMSprop 基於對梯度的平方的指數衰減平均來調整每個參數的學習率。其更新公式如下：


```math
g_t = \nabla_\theta L(\theta_t)

```

```math
v_t = \beta v_{t-1} + (1 - \beta) g_t^2

```

```math
\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{v_t + \epsilon}} g_t

```

其中：
-  $v_t$  是梯度的平方的移動平均， $\beta$  是衰減率（通常接近 1，例如  $\beta = 0.9$ ）。
-  $\epsilon$  是一個小常數，用於避免除以零的情況。
-  $\eta$  是學習率，通常會固定。

**優點與缺點**：
- **優點**：
  - 有助於快速收斂，特別是在非平穩的損失函數上。
  - 自適應學習率可以避免過大或過小的學習步長問題。
- **缺點**：
  - 學習率仍然是超參數，雖然方法對學習率較不敏感，但選擇一個適當的學習率仍然至關重要。

---

##### **9.1.3 Adam（Adaptive Moment Estimation）**

Adam 是目前最流行的優化算法之一，結合了動量方法（Momentum）和 RMSprop 的優點。它在每個參數的更新中不僅考慮梯度的均值（像 Momentum 一樣），還考慮了梯度平方的均值（像 RMSprop 一樣），並對這兩個均值進行自適應調整。

**Adam 的數學原理**：

Adam 算法通過以下兩個公式計算動量和梯度的均方根（RMS）：


```math
m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t

```

```math
v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2

```

這裡：
-  $m_t$  是梯度的一階矩的估計（動量）， $\beta_1$  是控制一階矩更新的超參數。
-  $v_t$  是梯度的二階矩的估計（RMS）， $\beta_2$  是控制二階矩更新的超參數。

然後，Adam 更新規則為：


```math
\hat{m_t} = \frac{m_t}{1 - \beta_1^t}

```

```math
\hat{v_t} = \frac{v_t}{1 - \beta_2^t}

```

```math
\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{\hat{v_t}} + \epsilon} \hat{m_t}

```

其中：
-  $\hat{m_t}$  和  $\hat{v_t}$  是對  $m_t$  和  $v_t$  的偏差修正。
-  $\eta$  是學習率，通常默認為 0.001。

**優點與缺點**：
- **優點**：
  - 結合了動量和自適應學習率，收斂速度快。
  - 具有較好的魯棒性，即使對學習率較為敏感的問題也能有較好的表現。
  - 在處理大規模數據集和高維問題時表現優越。
- **缺點**：
  - 雖然收斂速度較快，但最終可能收斂到一個次優解，而非全局最優解。
  - 需要調整超參數，尤其是  $\beta_1$ ,  $\beta_2$  和  $\epsilon$ 。

---

#### **小結**

本節介紹了三種常見的梯度下降變體：隨機梯度下降（SGD）、RMSprop 和 Adam。這些算法各有優缺點，並適用於不同的訓練場景。對於大多數深度學習任務，Adam 通常是優先選擇的優化算法，因為它能夠有效地處理稀疏數據和大規模參數空間，並且對超參數不太敏感。然而，在某些情況下，RMSprop 和 SGD 也可能會提供有競爭力的性能。選擇合適的優化算法對於模型的訓練和收斂至關重要。