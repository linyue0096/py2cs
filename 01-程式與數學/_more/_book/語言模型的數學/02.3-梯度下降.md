以下是 **2.3 梯度下降法與優化技術** 的詳細內容提綱，重點講解梯度下降法的數學基礎、變體以及在深度學習中的應用。

---

## **2.3 梯度下降法與優化技術**

---

### **2.3.1 梯度下降法的基礎概念**
- **梯度下降法的定義**：  
  - 梯度下降法是一種迭代優化算法，通過沿著損失函數的負梯度方向更新參數，逐步逼近最小值。  
    更新公式：

```math
    \theta \leftarrow \theta - \eta \nabla_\theta \mathcal{L}

```
    -  $\theta$ ：模型參數。  
    -  $\eta$ ：學習率，控制每次更新的步伐大小。  
    -  $\mathcal{L}$ ：損失函數。  

- **幾何意義**：  
  梯度下降法在參數空間中沿著損失函數下降最快的方向移動，尋找最小值。

---

### **2.3.2 梯度下降法的變體**
- **批量梯度下降（Batch Gradient Descent）**：  
  - 使用整個訓練數據集計算梯度。  
  - 優點：收斂穩定。  
  - 缺點：計算量大，對大數據集不友好。  

- **隨機梯度下降（Stochastic Gradient Descent, SGD）**：  
  - 每次隨機選擇一個樣本計算梯度並更新參數：  

```math
    \theta \leftarrow \theta - \eta \nabla_\theta \mathcal{L}(x_i, y_i)

```
  - 優點：計算速度快，適用於大規模數據。  
  - 缺點：收斂過程波動較大。

- **小批量梯度下降（Mini-Batch Gradient Descent）**：  
  - 每次隨機選取一個小批量樣本計算梯度：  

```math
    \theta \leftarrow \theta - \eta \frac{1}{B} \sum_{i=1}^B \nabla_\theta \mathcal{L}(x_i, y_i)

```
    -  $B$ ：批量大小。  
  - 綜合了批量梯度下降和隨機梯度下降的優點。

---

### **2.3.3 常用優化技術**
- **動量法（Momentum）**：  
  - 為梯度下降添加動量項，避免收斂過程的劇烈振盪：  

```math
    v_t = \beta v_{t-1} + (1 - \beta) \nabla_\theta \mathcal{L}

```

```math
    \theta \leftarrow \theta - \eta v_t

```
    -  $\beta$ ：控制動量的權重。  
  - 特性：在狹長曲面（如鞍點）上表現良好。

- **AdaGrad**：  
  - 根據參數的歷史梯度大小調整學習率：  

```math
    \theta \leftarrow \theta - \frac{\eta}{\sqrt{G_t + \epsilon}} \nabla_\theta \mathcal{L}

```
    -  $G_t = \sum_{i=1}^t g_i^2$ ：累積梯度平方。  
  - 優點：適合處理稀疏數據。  
  - 缺點：學習率可能過早收斂。

- **RMSProp**：  
  - 改進 AdaGrad，通過指數加權移動平均（Exponential Moving Average, EMA）平滑累積梯度平方：  

```math
    G_t = \beta G_{t-1} + (1 - \beta) g_t^2

```

```math
    \theta \leftarrow \theta - \frac{\eta}{\sqrt{G_t + \epsilon}} \nabla_\theta \mathcal{L}

```
    -  $\beta$ ：控制平滑程度。

- **Adam（Adaptive Moment Estimation）**：  
  - 結合動量法與 RMSProp，綜合考慮一階和二階矩估計：  

```math
    m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t

```

```math
    v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2

```

```math
    \theta \leftarrow \theta - \frac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t

```
    -  $\hat{m}_t$  和  $\hat{v}_t$ ：偏差修正後的一階和二階矩估計。

---

### **2.3.4 梯度下降與優化技術的收斂性**
- **學習率的選擇**：  
  - 學習率過大：可能跳過最小值或導致發散。  
  - 學習率過小：收斂速度慢，可能陷入局部最小值。  

- **損失函數的形狀**：  
  - 鞍點（Saddle Point）：梯度為零但不是極小值，可能阻礙收斂。  
  - 狹長曲面（Plateau）：需要動量法加速收斂。

- **正則化技術**：  
  - 在損失函數中加入正則化項，控制模型的複雜度，避免過擬合。

---

### **2.3.5 Python 實作：優化技術的應用**
- **SGD 的簡單實現**：  
  ```python
  import torch
  import torch.optim as optim

  # 假設模型參數和損失函數
  model = torch.nn.Linear(2, 1)
  loss_fn = torch.nn.MSELoss()
  optimizer = optim.SGD(model.parameters(), lr=0.01)

  # 模擬訓練
  for epoch in range(10):
      inputs = torch.rand(2)
      targets = torch.rand(1)

      # 前向傳播
      outputs = model(inputs)
      loss = loss_fn(outputs, targets)

      # 反向傳播和參數更新
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()
  ```

- **使用 Adam 優化器**：  
  ```python
  optimizer = optim.Adam(model.parameters(), lr=0.001)

  for epoch in range(10):
      outputs = model(inputs)
      loss = loss_fn(outputs, targets)

      optimizer.zero_grad()
      loss.backward()
      optimizer.step()
  ```

---

### **總結**
- 梯度下降法是深度學習模型訓練的核心，優化技術則在梯度下降法的基礎上進一步改進收斂速度和穩定性。  
- 動量法、RMSProp 和 Adam 是常用的優化技術，能適應不同的損失函數形狀和數據特性。  
- 在實際應用中，選擇適合的優化方法和參數對模型性能有重要影響。

如果需要補充更深入的數學推導或實例，請隨時告訴我！