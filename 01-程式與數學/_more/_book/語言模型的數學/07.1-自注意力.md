#### **第 7 章 Transformer 的數學**

---

#### **7.1 自注意力機制的數學原理**

自注意力機制（Self-Attention）是 Transformer 架構的核心，它使模型能夠在處理序列數據時，根據輸入序列中的每個元素的關聯性動態地調整對其他元素的關注程度。這使得 Transformer 能夠捕捉到長距離的依賴關係，並有效處理序列中的不同部分之間的交互。

本節將深入探討自注意力機制的數學原理，並詳細說明如何計算每個詞向量的注意力權重，以及如何將這些權重應用到最終的表示學習中。

---

#### **7.1.1 自注意力的基本概念**

自注意力的基本目的是計算序列中每個詞對其他詞的「注意力」，並根據這些注意力權重加權平均地生成新的表示。這一過程可以表示為：


```math
\text{Attention}(Q, K, V) = \text{softmax}\left( \frac{QK^T}{\sqrt{d_k}} \right) V

```

其中：
-  $Q$  是查詢矩陣（Query matrix），通常表示為詞向量的映射。
-  $K$  是鍵矩陣（Key matrix），表示序列中每個詞的特徵表示。
-  $V$  是值矩陣（Value matrix），通常與鍵矩陣具有相同的維度。
-  $d_k$  是鍵的維度（即每個詞向量的維度），它用來對內積進行縮放，避免內積值過大。

這些矩陣  $Q$ 、 $K$ 、 $V$  是由輸入詞向量經過線性變換得到的，通常通過權重矩陣  $W_Q$ 、 $W_K$ 、 $W_V$  來進行映射：


```math
Q = X W_Q, \quad K = X W_K, \quad V = X W_V

```

其中  $X$  是輸入序列的詞向量矩陣， $W_Q$ 、 $W_K$ 、 $W_V$  是需要學習的權重矩陣。

---

#### **7.1.2 計算注意力權重**

在自注意力機制中，我們首先計算每對詞之間的相似度，這是通過計算查詢向量  $Q$  和鍵向量  $K$  之間的內積來實現的。為了避免內積值過大，我們通常對其進行縮放，這就是為什麼在公式中有一個  $\sqrt{d_k}$  的原因。

每一個查詢向量  $q_i$  和鍵向量  $k_j$  之間的相似度（注意力得分）可以表示為：


```math
\text{score}(q_i, k_j) = \frac{q_i \cdot k_j^T}{\sqrt{d_k}}

```

這個分數用來計算每個元素對其他元素的關注程度，然後通過軟最大（Softmax）函數來歸一化，使得每個注意力權重都在 0 和 1 之間，並且總和為 1：


```math
\text{Attention Weight}_{ij} = \text{softmax} \left( \frac{q_i \cdot k_j^T}{\sqrt{d_k}} \right) = \frac{e^{\frac{q_i \cdot k_j^T}{\sqrt{d_k}}}}{\sum_{k=1}^{n} e^{\frac{q_i \cdot k_k^T}{\sqrt{d_k}}}}

```

這些注意力權重  $\text{Attention Weight}_{ij}$  表示了在處理第  $i$  個詞時，第  $j$  個詞對其的影響。

---

#### **7.1.3 融合信息與生成新的表示**

有了注意力權重之後，我們使用這些權重對值矩陣  $V$  進行加權求和，以生成新的詞向量表示。這樣，第  $i$  個詞的最終表示可以表示為：


```math
\text{output}_i = \sum_{j=1}^{n} \text{Attention Weight}_{ij} v_j

```

這表示第  $i$  個詞的表示是通過加權平均其對其他詞的關注程度來生成的。

---

#### **7.1.4 多頭自注意力（Multi-Head Attention）**

為了讓模型能夠學習不同的表示子空間，Transformer 使用了多頭自注意力（Multi-Head Attention）機制。這種方法將查詢、鍵和值矩陣分成多個「頭」，每個頭分別計算注意力，然後將這些注意力的輸出拼接在一起，最後再經過一個線性變換。這樣可以捕捉到更豐富的語言模式和長距離依賴。

假設有  $h$  個注意力頭，則多頭注意力的計算過程如下：

1. 對於每一個頭  $i$ ，計算  $Q_i$ 、 $K_i$ 、 $V_i$ ：

```math
   Q_i = X W_Q^{(i)}, \quad K_i = X W_K^{(i)}, \quad V_i = X W_V^{(i)}

```

2. 計算每個頭的自注意力：

```math
   \text{head}_i = \text{Attention}(Q_i, K_i, V_i)

```

3. 將所有頭的輸出拼接起來並進行線性變換：

```math
   \text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \text{head}_2, \dots, \text{head}_h) W_O

```

其中， $W_O$  是學習的線性權重矩陣，將多個頭的結果融合成最終的輸出。

---

#### **7.1.5 自注意力在序列建模中的優勢**

自注意力機制相對於傳統的 RNN 或 CNN，有幾個顯著的優勢：
1. **並行處理：** 在 RNN 中，序列中的每個元素必須依賴於前一個元素的計算，這使得處理速度受到限制。而自注意力機制可以同時處理序列中的所有元素，因此具有更好的並行性能。
   
2. **長距離依賴：** 自注意力可以直接捕捉序列中任意兩個詞之間的關係，而不受距離的影響。這使得 Transformer 能夠有效地處理長距離依賴的問題。

3. **可解釋性：** 自注意力機制提供了每個詞對其他詞的關注程度，可以直觀地理解模型是如何基於上下文來生成預測的。

---

#### **7.1.6 小結**

自注意力機制是 Transformer 模型的核心，它允許模型在處理序列數據時動態地捕捉長距離的依賴關係。自注意力的計算過程包括查詢、鍵、值的映射、相似度計算、注意力權重的計算和加權平均。多頭自注意力進一步提高了模型的表達能力，使得 Transformer 成為處理語言和其他序列數據的強大工具。