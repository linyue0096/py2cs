#### **6.2 損失函數設計與學習目標**

損失函數（Loss Function）是神經網絡訓練過程中的核心要素，它衡量了模型預測與實際標籤之間的誤差。設計合適的損失函數是神經網絡訓練成功的關鍵，因為它直接影響到模型的學習目標以及最終性能。選擇哪種損失函數取決於具體的學習任務，例如回歸、分類或生成任務等。

本節將介紹損失函數的數學原理、常見的損失函數及其應用，並分析不同損失函數在不同學習目標下的選擇原則。

---

#### **6.2.1 損失函數的數學定義**

損失函數是一個用來衡量預測結果和真實標籤之間差異的函數。在神經網絡訓練中，損失函數通常是模型的預測  $\hat{y}$  與真實值  $y$  之間的某種度量。常見的損失函數包括均方誤差、交叉熵等。

對於單一樣本，損失函數可以表示為：


```math
L(\hat{y}, y)

```

其中：
-  $\hat{y}$  是模型的預測值。
-  $y$  是真實標籤。

在神經網絡的訓練過程中，損失函數會對所有訓練樣本進行求和或平均，從而得到整體損失。整體損失函數  $L$  將作為反向傳播的目標，用來計算權重的更新。

---

#### **6.2.2 常見的損失函數**

1. **均方誤差損失（Mean Squared Error, MSE）**

   均方誤差損失主要用於回歸問題，衡量模型預測值和實際值之間的差異。其數學形式為：


```math
   L_{\text{MSE}}(\hat{y}, y) = \frac{1}{N} \sum_{i=1}^{N} (\hat{y}_i - y_i)^2

```

   其中：
   -  $\hat{y}_i$  是第  $i$  個樣本的預測值。
   -  $y_i$  是第  $i$  個樣本的真實值。
   -  $N$  是樣本數量。

   MSE 的目標是最小化預測值與真實值之間的平方誤差，這樣可以使得模型對每個樣本的預測更為精確。

2. **交叉熵損失（Cross-Entropy Loss）**

   交叉熵損失常用於分類問題，尤其是多分類問題。它衡量了兩個概率分佈之間的差異，通常用來評估模型的預測概率分佈與實際的真實標籤分佈之間的差異。對於二分類問題，交叉熵損失可以表示為：


```math
   L_{\text{CE}}(\hat{y}, y) = - \left( y \log(\hat{y}) + (1 - y) \log(1 - \hat{y}) \right)

```

   其中：
   -  $\hat{y}$  是模型預測的概率。
   -  $y$  是實際的標籤（0 或 1）。

   對於多分類問題，交叉熵損失的形式為：


```math
   L_{\text{CE}}(\hat{y}, y) = - \sum_{i=1}^{C} y_i \log(\hat{y}_i)

```

   其中  $C$  是類別數， $y_i$  是真實標籤的指示函數， $\hat{y}_i$  是模型對第  $i$  類的預測概率。

   交叉熵損失在概率模型中尤為有效，能夠強制模型提高對真實類別的預測概率。

3. **Hinge 損失（Hinge Loss）**

   Hinge 損失主要用於支持向量機（SVM）等分類問題，特別是二分類問題。Hinge 損失對於正確分類的樣本，其損失為零，對於錯誤分類的樣本，損失呈線性增長。其數學公式為：


```math
   L_{\text{hinge}}(\hat{y}, y) = \max(0, 1 - y \cdot \hat{y})

```

   其中：
   -  $\hat{y}$  是模型預測的輸出。
   -  $y$  是真實標籤（取值為  $\pm 1$ ）。

   Hinge 損失的目標是讓模型的預測與真實標籤之間有足夠的間隔，從而提高分類邊界的穩定性。

4. **KL 散度（Kullback-Leibler Divergence）**

   KL 散度是一種衡量兩個概率分佈差異的損失函數，通常用於變分推斷和生成模型（如生成對抗網絡）中。對於概率分佈  $p$  和  $q$ ，KL 散度定義為：


```math
   D_{\text{KL}}(p || q) = \sum_{i} p(i) \log \left( \frac{p(i)}{q(i)} \right)

```

   KL 散度不是對稱的，即  $D_{\text{KL}}(p || q) \neq D_{\text{KL}}(q || p)$ ，這意味著它度量的是從分佈  $q$  逼近分佈  $p$  的效果。

---

#### **6.2.3 損失函數與學習目標的關聯**

損失函數的選擇直接影響模型的學習目標。對於不同的任務，需要選擇不同的損失函數來引導模型的訓練過程。

- **回歸任務**：對於回歸問題，目標是預測一個連續值，因此通常使用均方誤差（MSE）來作為損失函數。MSE 最小化預測值和真實值之間的誤差，讓模型能夠產生更精確的預測。

- **二分類問題**：對於二分類問題，交叉熵損失是最常用的選擇。交叉熵損失最小化的是預測類別的概率與實際標籤的差異，從而提高模型對真實類別的預測準確度。

- **多分類問題**：多分類問題中，通常會使用多類別交叉熵損失（categorical cross-entropy），它能夠將每個類別的預測概率與真實標籤進行比較，並最大化對正確類別的預測概率。

- **生成模型與變分推斷**：在生成模型中，KL 散度損失通常被用來衡量生成分佈和真實數據分佈之間的差異，從而優化生成模型的生成能力。

---

#### **6.2.4 小結**

本節介紹了損失函數的數學基礎，並分析了不同類型任務中常用的損失函數。損失函數的設計不僅影響模型的學習效果，還決定了訓練過程中的學習目標。因此，根據具體的任務需求選擇適合的損失函數是深度學習模型設計的重要一環。在後續的章節中，我們將繼續探討如何利用損失函數與優化方法來改進神經網絡的訓練效果。