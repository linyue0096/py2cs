#### **8.2 掩碼語言模型（例如 BERT）**

---

掩碼語言模型（Masked Language Model, MLM）是現代自然語言處理（NLP）領域中的另一類強大模型，最著名的代表是 BERT（Bidirectional Encoder Representations from Transformers）。與自回歸模型（如 GPT）不同，掩碼語言模型的核心在於對輸入文本進行部分掩碼，然後學習從上下文中預測被掩碼的部分。這一方法使得模型能夠利用雙向上下文信息，從而捕捉更為豐富的語言結構。

在本節中，我們將深入探討掩碼語言模型的數學基礎，並以 BERT 為例來具體介紹其工作原理和數學框架。

---

#### **8.2.1 掩碼語言模型的數學基礎**

掩碼語言模型的目標是基於上下文來預測被掩碼的詞語。在訓練過程中，部分輸入的詞語會被隨機掩碼（通常是用特殊的掩碼符號 `[MASK]` 來代替），然後模型的任務是根據其餘部分的上下文來預測這些被掩碼的詞語。

數學上，掩碼語言模型的目標是最大化以下條件概率：


```math
P(w_1, w_2, \dots, w_T | \hat{w}_1, \hat{w}_2, \dots, \hat{w}_T)

```

其中， $\hat{w}_i$  是掩碼符號，表示該位置的詞語被掩碼， $w_i$  是真實的詞語。訓練過程中，模型被要求最大化對掩碼位置的預測，即：


```math
\mathcal{L}_{MLM} = - \sum_{i=1}^T \mathbb{I}(w_i = \hat{w}_i) \log P(w_i | w_1, w_2, \dots, w_{i-1}, w_{i+1}, \dots, w_T)

```

這裡的  $\mathbb{I}(\cdot)$  是指示函數，當該位置為掩碼位置時，對應的真實詞語  $w_i$  會被用來計算損失。該損失函數強調了模型對掩碼位置的預測精度。

---

#### **8.2.2 BERT 模型的數學框架**

BERT 模型使用了 Transformer 的編碼器架構，並利用掩碼語言模型進行訓練。BERT 主要的創新在於它的雙向性：它不僅利用左側的上下文來預測某一位置的詞語，還利用右側的上下文。這一特性使得 BERT 相對於傳統的單向模型（如 GPT）能夠捕捉更豐富的語言信息。

BERT 模型的核心是基於 Transformer 編碼器的結構。在 BERT 中，給定一個詞序列  $w_1, w_2, \dots, w_T$ ，每個詞  $w_i$  會被映射到一個詞向量。這些詞向量會被送入多層的 Transformer 編碼器中，並且每一層都通過自注意力機制來計算上下文關聯。

數學上，BERT 的運行過程可以表示為：


```math
H^{(l)} = \text{Transformer}_{l}(H^{(l-1)})

```

其中， $H^{(l)}$  是第  $l$  層的輸出，並且  $H^{(0)}$  是輸入詞向量的初始表示。每一層的 Transformer 編碼器會基於自注意力機制來捕捉詞語間的依賴關係，從而生成該層的輸出。

最終的輸出  $H^{(L)}$  會被送入一個線性層，並且經過 Softmax 函數來預測每個掩碼位置的詞語：


```math
P(w_i | w_1, w_2, \dots, w_{i-1}, w_{i+1}, \dots, w_T) = \text{Softmax}(W_2 \cdot H_i^{(L)})

```

其中， $W_2$  是最終的投影矩陣，將輸出的詞向量映射到詞表的維度。這個 Softmax 操作會生成每個位置的預測分佈，並通過最大化對數似然來進行訓練。

---

#### **8.2.3 BERT 的掩碼策略與訓練過程**

在 BERT 的訓練過程中，通常會使用一種稱為 "隨機掩碼" 的策略，這意味著在每次訓練時，模型隨機選擇一些詞語進行掩碼處理。具體來說，每個詞語有 15% 的概率被掩碼，其中有 80% 的情況下詞語會被替換為特殊的掩碼符號 `[MASK]`，有 10% 的情況下詞語會被替換為隨機詞，還有 10% 的情況下詞語保持不變。這樣的策略能夠促使模型學習到更多的語言規律，並且防止模型僅僅依賴於單一的掩碼模式。

BERT 訓練過程的總損失函數可以表示為：


```math
\mathcal{L} = \mathcal{L}_{MLM} + \mathcal{L}_{NSP}

```

其中， $\mathcal{L}_{MLM}$  是掩碼語言模型的損失， $\mathcal{L}_{NSP}$  是下一句預測（Next Sentence Prediction, NSP）的損失。NSP 用於捕捉句子間的關聯性，通過對比模型預測的兩句話是否連貫來進行訓練。這一損失是 BERT 訓練中的另一個關鍵部分。

---

#### **8.2.4 掩碼語言模型與自回歸模型的比較**

- **上下文利用：**
  - **自回歸模型（如 GPT）** 僅使用單向上下文（從左到右），即每次生成一個詞時，只能依賴先前生成的詞語。
  - **掩碼語言模型（如 BERT）** 則使用雙向上下文，同時考慮到前後文的信息，這使得 BERT 在理解語境方面更為強大。

- **生成方式：**
  - **自回歸模型** 通常是用來生成序列的模型，一次生成一個詞並且依賴於前面的詞語。
  - **掩碼語言模型** 主要用於理解任務，它通過預測被掩碼的部分來學習語言結構。

- **訓練目標：**
  - **自回歸模型** 的目標是最大化條件概率，預測每個詞的條件分佈。
  - **掩碼語言模型** 的目標是預測掩碼的詞語，並且利用雙向上下文來提高預測精度。

---

#### **8.2.5 小結**

掩碼語言模型（如 BERT）通過將部分詞語掩碼來進行訓練，並利用雙向上下文來預測這些掩碼詞語，從而使模型能夠捕捉語言中的複雜結構和關聯。BERT 的核心創新在於其雙向訓練策略，它能夠更全面地理解語境。與自回歸模型相比，BERT 更適用於理解任務而非生成任務。這些技術和方法使得 BERT 成為當前 NLP 領域最強大的模型之一，並在許多下游任務中取得了突破性的成果。