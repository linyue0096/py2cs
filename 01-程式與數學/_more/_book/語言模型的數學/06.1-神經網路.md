#### **6.1 神經網絡的數學建模**

神經網絡是一種模仿生物神經系統結構和功能的計算模型，其核心由多層感知器（MLP, Multi-Layer Perceptron）結構組成。每一層由若干神經元組成，每個神經元接收來自前一層的輸入並進行加權和偏置計算，最後通過激活函數產生輸出。整體神經網絡的數學建模是通過這些層之間的變換來實現的。

本節將介紹神經網絡的數學表示、層的結構、加權求和、激活函數及其對應的數學公式。

---

#### **6.1.1 神經網絡層的結構**

在神經網絡中，每個神經元（或稱為節點）接收來自前一層神經元的輸入，並經過加權求和後傳遞到激活函數。這一過程可以用數學公式來描述。

假設神經網絡包含  $L$  層，層  $l$  的神經元數量為  $n_l$ ，並且每個神經元接收來自上一層的輸入。對於層  $l$ ，其輸入  $\mathbf{x}^{(l)}$  和輸出  $\mathbf{y}^{(l)}$  之間的關係可以表示為：


```math
\mathbf{y}^{(l)} = f^{(l)} \left( \mathbf{W}^{(l)} \mathbf{x}^{(l-1)} + \mathbf{b}^{(l)} \right)

```

其中：
-  $\mathbf{x}^{(l-1)}$  是上一層的輸出，對於第一層，這是模型的輸入數據。
-  $\mathbf{W}^{(l)}$  是層  $l$  的權重矩陣，負責將上一層的輸出映射到當前層的輸入空間。
-  $\mathbf{b}^{(l)}$  是層  $l$  的偏置向量，用於調整輸出。
-  $f^{(l)}$  是激活函數，它將線性變換的結果轉換為非線性輸出。

這樣，每一層的運算可以表示為一個線性變換加上偏置項，然後經過一個非線性激活函數。

---

#### **6.1.2 激活函數**

激活函數是神經網絡中的關鍵部分，它決定了神經元的輸出，並引入了非線性特徵，使得神經網絡能夠學習和擬合更複雜的函數。常見的激活函數包括：

- **Sigmoid 函數**：
  

```math
  \sigma(x) = \frac{1}{1 + e^{-x}}

```
  
  Sigmoid 函數將輸出範圍壓縮至 (0, 1)，常用於二分類問題中，但由於梯度消失問題，它在深層網絡中已較少使用。

- **Tanh 函數**：


```math
  \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}

```
  
  Tanh 函數將輸出範圍壓縮至 (-1, 1)，比 Sigmoid 函數具有更強的表達能力，且其對稱性使得在訓練過程中更穩定。

- **ReLU 函數**（Rectified Linear Unit）：


```math
  \text{ReLU}(x) = \max(0, x)

```
  
  ReLU 是目前最常用的激活函數，它能夠有效解決梯度消失問題，並且計算效率高，但存在“死神經元”問題（部分神經元在訓練過程中無法激活）。

- **Leaky ReLU 函數**：


```math
  \text{Leaky ReLU}(x) = \max(\alpha x, x)

```
  
  Leaky ReLU 是 ReLU 的變種，當輸入為負數時，它會給予一個小的斜率  $\alpha$ ，以減少死神經元問題。

- **Softmax 函數**：

  Softmax 函數通常用於多分類問題，將每個輸入映射到 (0, 1) 範圍內，並且所有輸出之和為 1，表示每個類別的概率。


```math
  \text{Softmax}(z_i) = \frac{e^{z_i}}{\sum_{j} e^{z_j}}

```

---

#### **6.1.3 神經網絡的前向傳播**

前向傳播是神經網絡運行的基本過程。在這一過程中，模型的輸入數據經過每一層的計算，並最終生成輸出。假設一個具有  $L$  層的神經網絡，其輸入為  $\mathbf{x}$ ，則最終輸出的計算過程可以表示為：


```math
\mathbf{y}^{(L)} = f^{(L)} \left( \mathbf{W}^{(L)} \mathbf{y}^{(L-1)} + \mathbf{b}^{(L)} \right)

```

這一過程在每一層都會重複進行，直到達到最終的輸出層。前向傳播是神經網絡訓練過程中的一個重要步驟，它為後續的反向傳播提供了必要的信息。

---

#### **6.1.4 神經網絡的反向傳播**

反向傳播是神經網絡訓練的核心，通過計算損失函數對每個權重的梯度，並將梯度反向傳遞以更新權重和偏置。反向傳播的基本原理來自鏈式法則，用來計算每層的梯度。反向傳播的數學過程如下：

假設損失函數為  $L$ ，並且神經網絡的輸出為  $\hat{y}$ ，則損失函數對網絡參數的梯度可以表示為：


```math
\frac{\partial L}{\partial \mathbf{W}^{(l)}} = \frac{\partial L}{\partial \mathbf{y}^{(l)}} \cdot \frac{\partial \mathbf{y}^{(l)}}{\partial \mathbf{W}^{(l)}}

```

通過鏈式法則，這樣的梯度計算可以反向傳遞至每一層，並用來更新權重和偏置。

---

#### **6.1.5 小結**

本節介紹了神經網絡的數學建模，從神經網絡層的結構、激活函數到前向傳播和反向傳播的數學過程。這些數學工具是神經網絡設計和訓練的基礎，理解這些數學背景對於開發高效的深度學習模型至關重要。在接下來的章節中，我們將進一步深入探討神經網絡的訓練技巧、優化方法和應用。