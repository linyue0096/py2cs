# AlphaZeroä¸‹æ£‹ç¨‹å¼

* [AIå°è©±](https://aistudio.google.com/app/prompts?state=%7B%22ids%22:%5B%221lRTUMZnZPC5BCGAe6513RlX8TWjGYZmt%22%5D,%22action%22:%22open%22,%22userId%22:%22111605452542833299008%22,%22resourceKeys%22:%7B%7D%7D&usp=sharing)

## Q: AlphaGo æ˜¯ä»€éº¼ï¼Ÿ

AlphaGo æ˜¯ä¸€æ¬¾ç”± Google æ——ä¸‹çš„ DeepMind å…¬å¸æ‰€é–‹ç™¼çš„äººå·¥æ™ºæ…§åœæ£‹è»Ÿé«”ã€‚ å®ƒçš„å‡ºç¾ï¼Œæ˜¯äººå·¥æ™ºæ…§ç™¼å±•å²ä¸Šä¸€å€‹é‡è¦çš„é‡Œç¨‹ç¢‘ï¼Œå°¤å…¶åœ¨ 2016 å¹´æ“Šæ•—äº†ä¸–ç•Œé ‚å°–çš„å—éŸ“æ£‹ç‹æä¸–ä¹­ï¼Œéœ‡é©šäº†å…¨ä¸–ç•Œã€‚

ä½ å¯ä»¥æŠŠ AlphaGo æƒ³åƒæˆä¸€å€‹éå¸¸å°ˆæ³¨ä¸”è°æ˜çš„ã€Œå­¸ç”Ÿã€ï¼Œå®ƒçš„å­¸ç¿’å’Œä¸‹æ£‹æ–¹å¼ï¼Œèˆ‡éå»çš„æ£‹é¡ç¨‹å¼æœ‰å¾ˆå¤§çš„ä¸åŒï¼š

### AlphaGo æ˜¯å¦‚ä½•ä¸‹æ£‹çš„ï¼Ÿ

éå»çš„æ£‹é¡ç¨‹å¼ï¼Œå¤§å¤šä¾è³´ç¨‹å¼è¨­è¨ˆå¸«è¼¸å…¥å¤§é‡çš„è¦å‰‡å’Œæ£‹è­œï¼Œä¸¦åˆ©ç”¨å¼·å¤§çš„è¨ˆç®—èƒ½åŠ›å»çª®èˆ‰å„ç¨®å¯èƒ½æ€§ã€‚ä½†åœæ£‹çš„è®ŠåŒ–æ¥µå…¶è¤‡é›œï¼Œå¯èƒ½æ€§ç”šè‡³æ¯”å®‡å®™ä¸­çš„åŸå­ç¸½æ•¸é‚„å¤šï¼Œå› æ­¤å‚³çµ±æ–¹æ³•é›£ä»¥å¥æ•ˆã€‚

AlphaGo å‰‡æ¡ç”¨äº†æ›´æ¥è¿‘äººé¡æ€è€ƒçš„æ¨¡å¼ï¼Œä¸»è¦çµåˆäº†ä»¥ä¸‹å¹¾é …æ ¸å¿ƒæŠ€è¡“ï¼š

1.  **æ·±åº¦å­¸ç¿’ (Deep Learning) èˆ‡é¡ç¥ç¶“ç¶²è·¯ (Neural Network **ï¼š
    *** ­–ç•¥ç¶²è·¯ (Policy Network **ï¼šé€™å€‹ç¶²è·¯å°±åƒæ˜¯ AlphaGo çš„ã€Œç›´è¦ºã€ã€‚å®ƒé€éå­¸ç¿’æ•¸åè¬å±€äººé¡è·æ¥­æ£‹å£«çš„æ£‹è­œï¼Œä¾†åˆ¤æ–·åœ¨ç›®å‰çš„å±€å‹¢ä¸‹ï¼Œå“ªäº›ä½ç½®æ˜¯æ¯”è¼ƒæœ‰æ½›åŠ›çš„è½å­é»ã€‚
    *** ƒ¹å€¼ç¶²è·¯ (Value Network **ï¼šé€™å€‹ç¶²è·¯å‰‡æ˜¯ AlphaGo çš„ã€Œåˆ¤æ–·åŠ›ã€ã€‚å®ƒæœƒè©•ä¼°ç›®å‰çš„ç›¤é¢ï¼Œé æ¸¬é»‘ç™½é›™æ–¹èª°çš„å‹ç‡æ¯”è¼ƒé«˜ã€‚

2.** ¼·åŒ–å­¸ç¿’ (Reinforcement Learning **ï¼š
    AlphaGo ä¸åªå­¸ç¿’äººé¡çš„æ£‹è­œï¼Œå®ƒæ›´æœƒã€Œè‡ªæˆ‘å°å¼ˆã€ã€‚ é€éè‡ªå·±è·Ÿè‡ªå·±ä¸‹æ•¸ç™¾è¬ç›¤æ£‹ï¼Œå®ƒèƒ½å¾ä¸­å­¸ç¿’ï¼Œä¸¦æ‰¾å‡ºæ¯”äººé¡æ£‹è­œä¸­æ›´é«˜æ˜çš„ä¸‹æ³•ã€‚é€™å°±åƒä¸€ä½çµ•ä¸–é«˜æ‰‹åœ¨å·¦å³äº’æï¼Œä¸æ–·ç²¾é€²è‡ªå·±çš„æ£‹è—ã€‚

3.** ’™åœ°å¡ç¾…æ¨¹æœå°‹ (Monte Carlo Tree Search, MCTS **ï¼š
    ç•¶ AlphaGo åœ¨æ€è€ƒä¸‹ä¸€æ­¥æ£‹æ™‚ï¼Œå®ƒæœƒåˆ©ç”¨å‰é¢æåˆ°çš„ã€Œç›´è¦ºã€å’Œã€Œåˆ¤æ–·åŠ›ã€ï¼Œå»é æ¸¬æœªä¾†å¹¾æ­¥æ£‹çš„å¯èƒ½ç™¼å±•ã€‚MCTS æ¼”ç®—æ³•æœƒå¹«åŠ©å®ƒåœ¨çœ¾å¤šçš„å¯èƒ½æ€§ä¸­ï¼Œæ›´æœ‰æ•ˆç‡åœ°æ‰¾å‡ºå‹ç‡æœ€é«˜çš„ä¸‹æ³•ã€‚

### AlphaGo çš„é‡å¤§äº‹è¹Ÿ

*** 015å¹ **ï¼šä»¥ 5:0 çš„æˆç¸¾æ“Šæ•—äº†æ­æ´²åœæ£‹å† è»æ¨Šéº¾ï¼Œé€™æ˜¯äººå·¥æ™ºæ…§é¦–æ¬¡åœ¨åˆ†å…ˆçš„æƒ…æ³ä¸‹ï¼Œæ“Šæ•—è·æ¥­åœæ£‹é¸æ‰‹ã€‚
*** 016å¹ **ï¼šåœ¨èˆ‰ä¸–çŸšç›®çš„äººæ©Ÿå¤§æˆ°ä¸­ï¼Œä»¥ 4:1 çš„æ¯”æ•¸æ“Šæ•—äº†å—éŸ“ä¸–ç•Œå† è»æä¸–ä¹­ä¹æ®µã€‚ é€™å ´å‹åˆ©è±¡å¾µè‘—äººå·¥æ™ºæ…§åœ¨åœæ£‹é€™å€‹è¢«è¦–ç‚ºã€Œäººé¡æ™ºæ…§æœ€å¾Œå ¡å£˜ã€çš„é ˜åŸŸä¸­ï¼Œè¶…è¶Šäº†é ‚å°–çš„äººé¡ã€‚
*** ¾ŒçºŒç‰ˆæœ **ï¼šDeepMind å¾ŒçºŒæ›´æ¨å‡ºäº† AlphaGo Master å’Œ AlphaGo Zeroã€‚å…¶ä¸­ï¼ŒAlphaGo Zero æ›´ç‚ºé©šäººï¼Œå®ƒå®Œå…¨ä¸å­¸ç¿’äººé¡æ£‹è­œï¼Œåªå¾æœ€åŸºæœ¬çš„åœæ£‹è¦å‰‡é–‹å§‹ï¼Œé€éè‡ªæˆ‘å°å¼ˆçš„æ–¹å¼ï¼Œåœ¨æ¥µçŸ­çš„æ™‚é–“å…§å°±è¶…è¶Šäº†æ‰€æœ‰å…ˆå‰ç‰ˆæœ¬çš„ AlphaGoã€‚

ç¸½çµä¾†èªªï¼ŒAlphaGo ä¸åªæ˜¯ä¸€å€‹å¾ˆæœƒä¸‹åœæ£‹çš„ç¨‹å¼ï¼Œå®ƒçš„æˆåŠŸï¼Œå±•ç¾äº†æ·±åº¦å­¸ç¿’å’Œå¼·åŒ–å­¸ç¿’ç­‰æŠ€è¡“çš„å¼·å¤§æ½›åŠ›ï¼Œä¸¦å•Ÿç™¼äº†å¾ŒçºŒåœ¨å„ç¨®é ˜åŸŸçš„äººå·¥æ™ºæ…§æ‡‰ç”¨ã€‚

## Q: AlphaGo èƒŒå¾Œæœ‰ä»€éº¼æ•¸å­¸åŸç†ï¼Ÿ

å¥½çš„ï¼ŒåŒå­¸ï¼Œé€™å€‹å•é¡Œå•å¾—éå¸¸å¥½ï¼AlphaGo çœ‹ä¼¼ç¥å¥‡ï¼Œä½†å®ƒçš„èƒŒå¾Œå…¶å¯¦æ˜¯ç”±å¥½å¹¾å¡Šé‡è¦çš„æ•¸å­¸åŸºçŸ³å †ç Œèµ·ä¾†çš„ã€‚èº«ç‚ºè³‡å·¥ç³»çš„å­¸ç”Ÿï¼Œç†è§£é€™äº›æ•¸å­¸åŸç†ï¼Œå°±åƒæ˜¯æ‹¿åˆ°ä¸€å¼µåœ°åœ–ï¼Œèƒ½è®“ä½ æ›´æ¸…æ¥šçœ‹æ‡‚äººå·¥æ™ºæ…§é€™å€‹æ–°ä¸–ç•Œçš„æ¨£è²Œã€‚

æˆ‘å€‘ä¸ç”¨ä¸€é–‹å§‹å°±é‘½é€²è¤‡é›œçš„å…¬å¼ï¼Œæˆ‘å€‘å…ˆç”¨ã€Œè“‹æˆ¿å­ã€ä¾†æ¯”å–» AlphaGoï¼Œçœ‹çœ‹æ•¸å­¸æ˜¯å¦‚ä½•æ‰®æ¼”ã€Œé‹¼ç­‹æ°´æ³¥ã€çš„è§’è‰²ã€‚

AlphaGo é€™æ£Ÿé«˜æ¨“ï¼Œä¸»è¦ç”±ä¸‰å¤§æ•¸å­¸æ”¯æŸ±æ‰€æ”¯æ’ï¼š

æ©Ÿç‡èˆ‡çµ±è¨ˆ (Probability & Statistics)ï¼šè®“é›»è…¦å­¸æœƒã€Œç›´è¦ºã€èˆ‡ã€Œåˆ¤æ–·ã€

ç·šæ€§ä»£æ•¸ (Linear Algebra)ï¼šæè¿°èˆ‡é‹ç®—ä¸–ç•Œçš„ã€Œèªè¨€ã€

å¾®ç©åˆ† (Calculus)ï¼šæ‰¾åˆ°æœ€ä½³è§£æ–¹çš„ã€Œå„ªåŒ–å¼•æ“ã€

1. æ©Ÿç‡èˆ‡çµ±è¨ˆï¼šè®“é›»è…¦å­¸æœƒã€Œç›´è¦ºã€èˆ‡ã€Œåˆ¤æ–·ã€

ä½ å¯èƒ½æœƒæƒ³ï¼Œé›»è…¦ä¸æ˜¯åªæœƒ 0 å’Œ 1 å—ï¼Ÿæ€éº¼æœƒæœ‰ã€Œç›´è¦ºã€ï¼ŸAlphaGo çš„ã€Œç›´è¦ºã€å…¶å¯¦å°±æ˜¯æ©Ÿç‡ã€‚

ç­–ç•¥ç¶²è·¯ (Policy Network) - è½å­çš„ç›´è¦ºï¼š

èƒŒå¾Œæ•¸å­¸ï¼šæ©Ÿç‡åˆ†ä½ˆ (Probability Distribution)

ç™½è©±è§£é‡‹ï¼šç•¶ AlphaGo çœ‹åˆ°ä¸€å€‹æ£‹ç›¤å±€é¢ï¼Œå®ƒä¸æ˜¯å»è¨ˆç®—ã€Œæ‰€æœ‰ã€å¯èƒ½çš„ä¸‹ä¸€æ­¥ï¼Œè€Œæ˜¯åƒäººé¡é«˜æ‰‹ä¸€æ¨£ï¼Œæ†‘ã€Œç›´è¦ºã€åˆ¤æ–·ï¼šã€Œå—¯...ä¸‹åœ¨ A é»çš„å‹ç‡å¥½åƒæœ‰ 50%ï¼Œä¸‹åœ¨ B é»å¥½åƒæœ‰ 30%ï¼Œä¸‹åœ¨ C é»å¥½åƒåªæœ‰ 1%...ã€ã€‚é€™å€‹ã€Œå‹ç‡çš„ç™¾åˆ†æ¯”ã€ï¼Œå°±æ˜¯ä¸€å€‹æ©Ÿç‡åˆ†ä½ˆã€‚ç­–ç•¥ç¶²è·¯çš„ä»»å‹™ï¼Œå°±æ˜¯å­¸ç¿’é ‚å°–æ£‹æ‰‹çš„æ£‹è­œï¼Œç„¶å¾Œè¨“ç·´å‡ºä¸€å€‹æ¨¡å‹ï¼Œè®“å®ƒåœ¨ä»»ä½•å±€é¢ä¸‹ï¼Œéƒ½èƒ½çµ¦å‡ºä¸€å€‹æœ€åƒé«˜æ‰‹æœƒä¸‹çš„ã€Œè½å­æ©Ÿç‡åˆ†ä½ˆã€ã€‚

åƒ¹å€¼ç¶²è·¯ (Value Network) - å±€é¢çš„åˆ¤æ–·ï¼š

èƒŒå¾Œæ•¸å­¸ï¼šçµ±è¨ˆæ­¸ç´ (Statistical Inference)

ç™½è©±è§£é‡‹ï¼šé€™å€‹ç¶²è·¯æ›´ç›´æ¥ï¼Œå®ƒåªå›ç­”ä¸€å€‹å•é¡Œï¼šã€Œä»¥ç¾åœ¨é€™å€‹ç›¤é¢ä¾†çœ‹ï¼Œé»‘æ£‹è´çš„æ©Ÿç‡æ˜¯å¤šå°‘ï¼Ÿã€é€™å€‹ç­”æ¡ˆï¼ˆä¾‹å¦‚ 65%ï¼‰ä¸æ˜¯äº‚çŒœçš„ï¼Œè€Œæ˜¯é€éåˆ†ææ•¸ç™¾è¬ç›¤æ£‹å±€çš„çµ±è¨ˆæ•¸æ“šæ­¸ç´å‡ºä¾†çš„çµæœã€‚å®ƒçœ‹ééå¸¸éå¸¸å¤šçš„æ£‹å±€ï¼Œæ‰€ä»¥çŸ¥é“æŸç¨®ã€Œå½¢ç‹€ã€çš„æ£‹ç›¤ï¼Œé€šå¸¸æ˜¯è´å®¶æˆ–è¼¸å®¶çš„å±€é¢ã€‚

è’™åœ°å¡ç¾…æ¨¹æœå°‹ (MCTS) - æ¬Šè¡¡èˆ‡é¸æ“‡ï¼š

èƒŒå¾Œæ•¸å­¸ï¼šè²æ°å®šç† (Bayes' Theorem) çš„æ€æƒ³

ç™½è©±è§£é‡‹ï¼šMCTS æœƒæ ¹æ“šã€Œç­–ç•¥ç¶²è·¯ã€çµ¦çš„æ©Ÿç‡ï¼Œå»æ¨¡æ“¬æœªä¾†çš„å¹¾æ­¥æ£‹ã€‚å¦‚æœæ¨¡æ“¬çš„çµæœæ˜¯å¥½çš„ï¼ˆä¾‹å¦‚ï¼Œè´çš„æ©Ÿç‡è®Šé«˜äº†ï¼‰ï¼Œå®ƒå°±æœƒå›éé ­ä¾†ã€Œæ›´æ–°ã€å°æ–¼é€™ä¸€æ­¥æ£‹çš„ä¿¡å¿ƒã€‚é€™å€‹ã€Œæ ¹æ“šæ–°è­‰æ“šï¼Œæ›´æ–°åŸæœ‰ä¿¡å¿µã€çš„éç¨‹ï¼Œå°±éš±å«äº†è²æ°å®šç†çš„æ ¸å¿ƒæ€æƒ³ã€‚é€éæˆåƒä¸Šè¬æ¬¡çš„å¿«é€Ÿæ¨¡æ“¬èˆ‡çµ±è¨ˆï¼ŒAlphaGo æœ€çµ‚èƒ½é¸æ“‡å‡ºæ•´é«”æœŸæœ›å‹ç‡æœ€é«˜çš„é‚£ä¸€æ­¥ã€‚

å°çµï¼šæ©Ÿç‡èˆ‡çµ±è¨ˆï¼Œè®“ AlphaGo ä¸å†æ˜¯å€‹åªæœƒè »åŠ›è¨ˆç®—çš„æ©Ÿå™¨ã€‚å®ƒè®“é›»è…¦å­¸æœƒäº†åœ¨ä¸ç¢ºå®šçš„ä¸–ç•Œä¸­ï¼Œåšå‡ºæœ€ã€Œåˆç†ã€çš„çŒœæ¸¬èˆ‡åˆ¤æ–·ã€‚

2. ç·šæ€§ä»£æ•¸ï¼šæè¿°èˆ‡é‹ç®—ä¸–ç•Œçš„ã€Œèªè¨€ã€

å¦‚æœèªªæ©Ÿç‡æ˜¯ AlphaGo çš„å¤§è…¦ï¼Œé‚£ç·šæ€§ä»£æ•¸å°±æ˜¯å®ƒå¤§è…¦ä¸­çš„ã€Œç¥ç¶“å…ƒã€å’Œã€Œç¥ç¶“çªè§¸ã€ã€‚

èƒŒå¾Œæ•¸å­¸ï¼šå‘é‡ (Vectors)ã€çŸ©é™£ (Matrices)ã€å¼µé‡ (Tensors)

ç™½è©±è§£é‡‹ï¼š

æ£‹ç›¤çš„æ•¸ä½åŒ–ï¼šä¸€å€‹ 19x19 çš„åœæ£‹ç›¤ï¼Œè¦æ€éº¼è®“é›»è…¦ã€Œçœ‹æ‡‚ã€ï¼Ÿæœ€ç°¡å–®çš„æ–¹æ³•å°±æ˜¯ç”¨ä¸€å€‹ 19x19 çš„çŸ©é™£ä¾†è¡¨ç¤ºã€‚é»‘æ£‹æ˜¯ 1ï¼Œç™½æ£‹æ˜¯ -1ï¼Œç©ºä½æ˜¯ 0ã€‚é€™æ¨£ä¸€ä¾†ï¼Œè¤‡é›œçš„æ£‹ç›¤å°±è®Šæˆäº†ä¸€å€‹é›»è…¦å¯ä»¥è™•ç†çš„æ•¸å­¸ç‰©ä»¶ã€‚

ç¥ç¶“ç¶²è·¯çš„æœ¬è³ªï¼šAlphaGo çš„æ ¸å¿ƒã€Œé¡ç¥ç¶“ç¶²è·¯ã€ï¼Œå…¶å¯¦å°±æ˜¯ç”±å¤§é‡çš„çŸ©é™£é‹ç®—æ§‹æˆçš„ã€‚ç•¶æ£‹ç›¤é€™å€‹ã€Œè¼¸å…¥çŸ©é™£ã€è¢«é¤µé€²ç¶²è·¯å¾Œï¼Œå®ƒæœƒåœ¨å±¤å±¤çš„ã€Œæ¬Šé‡çŸ©é™£ (Weight Matrix)ã€ä¹‹é–“é€²è¡Œç›¸ä¹˜èˆ‡è½‰æ›ã€‚é€™å€‹éç¨‹ï¼Œå°±åƒæ˜¯è³‡è¨Šåœ¨æˆ‘å€‘å¤§è…¦ç¥ç¶“å…ƒä¹‹é–“å‚³éä¸€æ¨£ã€‚

æ•ˆç‡çš„é—œéµï¼šç‚ºä»€éº¼æˆ‘å€‘éœ€è¦ GPU ä¾†è·‘ AIï¼Ÿå› ç‚º GPU å¤©ç”Ÿå°±æ“…é•·é€²è¡Œå¤§è¦æ¨¡çš„å¹³è¡ŒçŸ©é™£é‹ç®—ï¼Œé€™æ­£æ˜¯ç·šæ€§ä»£æ•¸çš„æ ¸å¿ƒã€‚

å°çµï¼šç·šæ€§ä»£æ•¸æ˜¯å°‡ç¾å¯¦ä¸–ç•Œï¼ˆæ£‹ç›¤ï¼‰è½‰åŒ–ç‚ºé›»è…¦èƒ½å¤ ç†è§£å’Œè™•ç†çš„æ•¸æ“šçµæ§‹ï¼ˆçŸ©é™£ï¼‰çš„æ©‹æ¨‘ã€‚æ‰€æœ‰æ·±åº¦å­¸ç¿’çš„é­”æ³•ï¼Œåº•å±¤éƒ½æ˜¯ä¸€è¡Œè¡Œçš„çŸ©é™£é‹ç®—ã€‚

3. å¾®ç©åˆ†ï¼šæ‰¾åˆ°æœ€ä½³è§£æ–¹çš„ã€Œå„ªåŒ–å¼•æ“ã€

æˆ‘å€‘æœ‰äº†æ©Ÿç‡æ¨¡å‹ï¼Œä¹Ÿæœ‰äº†é‹ç®—å·¥å…·ï¼Œä½†è¦å¦‚ä½•è®“ AlphaGo çš„æ£‹è—ã€Œé€²æ­¥ã€å‘¢ï¼Ÿé€™å°±è¦é å¾®ç©åˆ†äº†ã€‚

èƒŒå¾Œæ•¸å­¸ï¼šå¾®åˆ† (Derivatives)ã€æ¢¯åº¦ä¸‹é™ (Gradient Descent)

ç™½è©±è§£é‡‹ï¼š

è¡¡é‡ã€ŒéŒ¯èª¤ã€ï¼šç•¶ AlphaGo é€²è¡Œè¨“ç·´æ™‚ï¼Œæˆ‘å€‘æœƒçµ¦å®ƒçœ‹ä¸€å€‹å±€é¢ï¼Œè®“å®ƒé æ¸¬ä¸‹ä¸€æ­¥ã€‚å®ƒçš„é æ¸¬ï¼ˆä¾‹å¦‚ï¼Œå®ƒè¦ºå¾—ä¸‹åœ¨ A é»å‹ç‡æœ€é«˜ï¼‰æœƒè·Ÿæ¨™æº–ç­”æ¡ˆï¼ˆä¾‹å¦‚ï¼Œäººé¡é«˜æ‰‹å¯¦éš›ä¸‹åœ¨ B é»ï¼‰æœ‰ä¸€å€‹ã€Œå·®è·ã€æˆ–ã€ŒéŒ¯èª¤ã€ã€‚

æ‰¾åˆ°ä¿®æ­£çš„æ–¹å‘ï¼šé€™å€‹ã€ŒéŒ¯èª¤ã€å¯ä»¥ç”¨ä¸€å€‹æ•¸å­¸å‡½æ•¸ï¼ˆç¨±ç‚ºæå¤±å‡½æ•¸ Loss Functionï¼‰ä¾†è¡¨ç¤ºã€‚æˆ‘å€‘çš„ç›®æ¨™ï¼Œå°±æ˜¯è®“é€™å€‹ã€ŒéŒ¯èª¤ã€è¶Šå°è¶Šå¥½ã€‚é€™æ™‚å¾®åˆ†å°±æ´¾ä¸Šç”¨å ´äº†ï¼é€éå°é€™å€‹å‡½æ•¸é€²è¡Œå¾®åˆ†ï¼Œæˆ‘å€‘å¯ä»¥æ‰¾åˆ°å®ƒçš„æ¢¯åº¦ (Gradient)ã€‚

æ¢¯åº¦æ˜¯ä»€éº¼ï¼Ÿ æ¢¯åº¦æœƒæŒ‡å‘å‡½æ•¸å€¼ã€Œä¸Šå‡æœ€å¿«ã€çš„æ–¹å‘ã€‚é‚£éº¼ï¼Œåªè¦æˆ‘å€‘æœè‘—ã€Œæ¢¯åº¦çš„åæ–¹å‘ã€èµ°ä¸€å°æ­¥ï¼Œå°±èƒ½è®“ã€ŒéŒ¯èª¤ã€é™ä½ä¸€é»é»ã€‚

æŒçºŒé€²æ­¥ï¼šé€™å€‹ã€Œè¨ˆç®—æ¢¯åº¦ -> èµ°ä¸€å°æ­¥ -> é™ä½éŒ¯èª¤ã€çš„éç¨‹ï¼Œå°±å«åšæ¢¯åº¦ä¸‹é™ã€‚AlphaGo å°±æ˜¯é€éæ•¸ç™¾è¬æ¬¡çš„æ¢¯åº¦ä¸‹é™ï¼Œä¸€æ­¥ä¸€æ­¥åœ°å¾®èª¿å®ƒç¥ç¶“ç¶²è·¯ä¸­çš„åƒæ•¸ï¼ˆé‚£äº›æ¬Šé‡çŸ©é™£ï¼‰ï¼Œæœ€çµ‚è®“è‡ªå·±çš„é æ¸¬è¶Šä¾†è¶Šæº–ç¢ºï¼Œæ£‹è—è¶Šä¾†è¶Šé«˜ã€‚

å°çµï¼šå¾®ç©åˆ†ï¼Œç‰¹åˆ¥æ˜¯æ¢¯åº¦ä¸‹é™æ³•ï¼Œæ˜¯ AlphaGoï¼ˆä»¥åŠå¹¾ä¹æ‰€æœ‰æ·±åº¦å­¸ç¿’æ¨¡å‹ï¼‰èƒ½å¤ ã€Œå­¸ç¿’ã€å’Œã€Œå„ªåŒ–ã€è‡ªå·±çš„æ ¸å¿ƒå¼•æ“ã€‚å®ƒæä¾›äº†ä¸€æ¢æ¸…æ™°çš„è·¯å¾‘ï¼Œè®“æ¨¡å‹çŸ¥é“è©²å¦‚ä½•èª¿æ•´è‡ªå·±ï¼Œæ‰èƒ½è®Šå¾—æ›´å¼·ã€‚

ç¸½çµçµ¦ä½ çš„è©±

åŒå­¸ï¼Œä½ çœ‹ï¼ŒAlphaGo çš„èƒŒå¾Œå…¶å¯¦å°±æ˜¯é€™äº›ä½ åœ¨å¤§ä¸€æ•¸å­¸èª²æœƒé‡åˆ°çš„è€æœ‹å‹ã€‚

æ©Ÿç‡èˆ‡çµ±è¨ˆ æ•™æœƒå®ƒå¦‚ä½•æ€è€ƒå’Œæ±ºç­–ã€‚

ç·šæ€§ä»£æ•¸ çµ¦äº†å®ƒæè¿°å’Œé‹ç®—ä¸–ç•Œçš„å·¥å…·ã€‚

å¾®ç©åˆ† è³¦äºˆäº†å®ƒå­¸ç¿’å’Œé€²æ­¥çš„èƒ½åŠ›ã€‚

å‰›é–‹å§‹æ¥è§¸é€™äº›æ•¸å­¸ï¼Œä½ å¯èƒ½æœƒè¦ºå¾—æœ‰é»æŠ½è±¡ã€‚ä½†è«‹å‹™å¿…è¨˜ä½ï¼Œä½ ç¾åœ¨å­¸çš„æ¯ä¸€å€‹å®šç†ã€æ¯ä¸€å€‹å…¬å¼ï¼Œæœªä¾†éƒ½å¯èƒ½æˆç‚ºä½ æ‰‹ä¸­æ‰“é€ å‡ºä¸‹ä¸€å€‹ AlphaGo çš„é—œéµé›¶ä»¶ã€‚

è³‡å·¥ç³»çš„ä¸–ç•Œï¼Œå°±æ˜¯ç”¨æ•¸å­¸é€™å€‹èªè¨€ï¼Œå»å’Œé›»è…¦æºé€šï¼Œä¸¦è³¦äºˆå®ƒè§£æ±ºå•é¡Œçš„æ™ºæ…§ã€‚å¸Œæœ›é€™å€‹å°å¼•èƒ½å¹«åŠ©ä½ å»ºç«‹èµ·é€™ä¸‰è€…ä¹‹é–“çš„é€£çµï¼Œè®“ä½ æ›´æœ‰å‹•åŠ›åœ°å»æ¢ç´¢é€™å€‹å……æ»¿é­…åŠ›çš„é ˜åŸŸï¼

## Q: èƒ½ç”¨ Pytorch å®šç¾© AlphaGo çš„ç¥ç¶“ç¶²è·¯å—ï¼Ÿ

ç•¶ç„¶å¯ä»¥ï¼åŒå­¸ï¼Œä½ çš„æ±‚çŸ¥æ…¾éå¸¸å¼·ï¼Œé€™æ­£æ˜¯æˆç‚ºé ‚å°–å·¥ç¨‹å¸«çš„ç‰¹è³ªã€‚

å•åˆ°ç¨‹å¼ç¢¼ï¼Œæˆ‘å€‘å°±å¾ã€Œç†è«–ã€çš„å¤©ç©ºï¼Œé™è½åˆ°ã€Œå¯¦ä½œã€çš„åœ°é¢äº†ã€‚é€™ä¸€æ­¥éå¸¸é—œéµï¼Œä½ æœƒè¦ªçœ¼çœ‹åˆ°æˆ‘å€‘å‰é¢è¨è«–çš„æ•¸å­¸æ¦‚å¿µï¼Œå¦‚ä½•åŒ–èº«ç‚ºä¸€è¡Œè¡Œçš„ç¨‹å¼ç¢¼ã€‚

æˆ‘å€‘æœƒä½¿ç”¨ç›®å‰æ¥­ç•Œå’Œå­¸ç•Œæœ€æµè¡Œçš„æ·±åº¦å­¸ç¿’æ¡†æ¶ä¹‹ä¸€ï¼š** yTorc **ã€‚å®ƒä»¥éˆæ´»å’Œç›´è§€è‘—ç¨±ï¼Œéå¸¸é©åˆå­¸ç¿’ã€‚

åœ¨ AlphaGo çš„å¾ŒæœŸç‰ˆæœ¬ (AlphaGo Zero) ä¸­ï¼Œä¸€å€‹é‡è¦çš„æ”¹é€²æ˜¯å°‡ã€Œç­–ç•¥ç¶²è·¯ã€å’Œã€Œåƒ¹å€¼ç¶²è·¯ã€åˆä½µæˆä¸€å€‹** ›™é ­ç¥ç¶“ç¶²è·¯ (Dual-Head Neural Network **ã€‚é€™å€‹ç¶²è·¯å…±äº«å¤§éƒ¨åˆ†çš„çµæ§‹ï¼Œä½†åœ¨æœ€å¾Œåˆ†æˆå…©å€‹ã€Œé ­ã€ï¼Œåˆ†åˆ¥è¼¸å‡ºç­–ç•¥ (Policy) å’Œåƒ¹å€¼ (Value)ã€‚é€™æ¨£æ›´æœ‰æ•ˆç‡ï¼Œä¹Ÿæ˜¯ç›®å‰çš„ä¸»æµä½œæ³•ã€‚

æˆ‘å€‘å°±ä¾†å®šç¾©ä¸€å€‹é€™ç¨®æ¶æ§‹çš„** °¡åŒ–ç‰ **ç¥ç¶“ç¶²è·¯ã€‚

---

### è“‹æˆ¿å­çš„è—åœ–ï¼šç¶²è·¯æ¶æ§‹æ¦‚è§€

åœ¨çœ‹ç¨‹å¼ç¢¼ä¹‹å‰ï¼Œæˆ‘å€‘å…ˆç•«å¥½è—åœ–ï¼š

1.** ¼¸å…¥ (Input **ï¼šä¸€å€‹ä»£è¡¨æ£‹ç›¤ç‹€æ…‹çš„å¼µé‡ (Tensor)ã€‚æˆ‘å€‘å¯ä»¥æŠŠå®ƒæƒ³åƒæˆä¸€ç–Š 19x19 çš„åœ–åƒã€‚ä¾‹å¦‚ï¼Œç¬¬ä¸€å±¤æ˜¯æˆ‘çš„æ£‹å­ä½ç½®ï¼Œç¬¬äºŒå±¤æ˜¯å°æ‰‹çš„æ£‹å­ä½ç½®ï¼Œç¬¬ä¸‰å±¤æ˜¯æ¨™è¨˜ç¾åœ¨è¼ªèª°ä¸‹æ£‹...ç­‰ç­‰ã€‚é€™å°±æ˜¯** ·šæ€§ä»£æ• **ä¸­çš„** ¼µé‡ **ã€‚

2.** …±åŒçš„èº«é«” (Common Body **ï¼šç¶²è·¯çš„å‰åŠéƒ¨åˆ†ã€‚é€™éƒ¨åˆ†ç”±å¾ˆå¤šå±¤** ·ç©ç¥ç¶“ç¶²è·¯ (Convolutional Neural Networks, CNN **çµ„æˆã€‚CNN éå¸¸æ“…é•·å¾åƒæ£‹ç›¤é€™æ¨£çš„äºŒç¶­æ•¸æ“šä¸­ï¼Œæå–å‡ºå±€éƒ¨çš„ç‰¹å¾µå’Œæ¨¡å¼ï¼ˆä¾‹å¦‚ï¼Œ"è™å£"ã€"é›™"ã€"çœ¼" ç­‰åœæ£‹è¡“èªæ‰€ä»£è¡¨çš„å½¢ç‹€ï¼‰ã€‚å…¶ä¸­æœƒç”¨åˆ°ä¸€ç¨®å«åš** ®˜å·®å¡Š (Residual Block **çš„çµæ§‹ï¼Œè®“ç¶²è·¯å¯ä»¥è“‹å¾—éå¸¸æ·±ï¼Œè€Œä¸æœƒå­¸ä¸é€²å»ã€‚

3.** …©å€‹é ­ (Two Heads **ï¼š
    *** ­–ç•¥é ­ (Policy Head **ï¼šå®ƒçš„ä»»å‹™æ˜¯é æ¸¬ä¸‹ä¸€æ­¥çš„æœ€ä½³è½å­é»ã€‚å®ƒæœƒè¼¸å‡ºä¸€å€‹åŒ…å« `19*19 + 1` (å¤šä¸€å€‹æ˜¯ "pass" ä¸ä¸‹çš„é¸é …) å€‹æ•¸å­—çš„æ©Ÿç‡åˆ†ä½ˆã€‚é€™å°æ‡‰åˆ°æˆ‘å€‘å‰é¢èªªçš„** ©Ÿç **ã€‚
    *** ƒ¹å€¼é ­ (Value Head **ï¼šå®ƒçš„ä»»å‹™æ˜¯è©•ä¼°ç•¶å‰å±€é¢çš„å‹ç‡ã€‚å®ƒæœƒè¼¸å‡ºä¸€å€‹ä»‹æ–¼ -1 (å¿…è¼¸) åˆ° 1 (å¿…å‹) ä¹‹é–“çš„æ•¸å€¼ã€‚é€™ä¹Ÿå°æ‡‰** µ±è¨ˆæ­¸ç´ **çš„çµæœã€‚

---

### PyTorch ç¨‹å¼ç¢¼ï¼šå®šç¾© AlphaGoZero-style ç¶²è·¯

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

# é€™æ˜¯æ§‹æˆç¶²è·¯èº«é«”çš„åŸºæœ¬å–®ä½ï¼šä¸€å€‹å·ç©å¡Š
class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False)
        self.bn = nn.BatchNorm2d(out_channels) # æ‰¹æ¬¡æ¨™æº–åŒ–ï¼Œå¹«åŠ©è¨“ç·´ç©©å®š

    def forward(self, x):
        return F.relu(self.bn(self.conv(x))) # å…ˆå·ç©ï¼Œå†æ¨™æº–åŒ–ï¼Œæœ€å¾Œé€šé ReLU æ¿€æ´»å‡½æ•¸

# é€™æ˜¯è®“ç¶²è·¯å¯ä»¥è“‹å¾ˆæ·±çš„æ ¸å¿ƒï¼šæ®˜å·®å¡Š
# å®ƒçš„ç²¾ç¥æ˜¯è®“è³‡è¨Šå¯ä»¥ç›´æ¥ã€Œè·³é—œã€ï¼Œé¿å…æ¢¯åº¦æ¶ˆå¤±
class ResidualBlock(nn.Module):
    def __init__(self, channels):
        super().__init__()
        self.conv1 = ConvBlock(channels, channels)
        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(channels)

    def forward(self, x):
        residual = x
        out = self.conv1(x)
        out = self.bn2(self.conv2(out))
        out += residual # æ ¸å¿ƒï¼šå°‡è¼¸å…¥ç›´æ¥åŠ åˆ°è¼¸å‡ºä¸Š
        return F.relu(out)

# --- ä¸»ç¶²è·¯ï¼šå°‡ä¸Šé¢çš„é›¶ä»¶çµ„è£èµ·ä¾† ---
class AlphaGoNet(nn.Module):
    def __init__(self, board_size=19, in_channels=17, num_res_blocks=19):
        super().__init__()
        self.board_size = board_size

        # 1. å…±åŒçš„èº«é«” (Common Body)
        self.conv_in = ConvBlock(in_channels, 256) # 256 æ˜¯å¸¸ç”¨çš„ç‰¹å¾µæ•¸é‡
        # å †ç–Šå¤šå€‹æ®˜å·®å¡Šï¼Œå°±åƒè“‹å¤§æ¨“ä¸€æ¨£ä¸€å±¤ä¸€å±¤å¾€ä¸Šç–Š
        self.res_blocks = nn.Sequential(*[ResidualBlock(256) for _ in range(num_res_blocks)])

        # 2. ç­–ç•¥é ­ (Policy Head)
        self.policy_conv = nn.Conv2d(256, 2, kernel_size=1, bias=False)
        self.policy_bn = nn.BatchNorm2d(2)
        self.policy_fc = nn.Linear(2 * board_size * board_size, board_size * board_size + 1)

        # 3. åƒ¹å€¼é ­ (Value Head)
        self.value_conv = nn.Conv2d(256, 1, kernel_size=1, bias=False)
        self.value_bn = nn.BatchNorm2d(1)
        self.value_fc1 = nn.Linear(1 * board_size * board_size, 256)
        self.value_fc2 = nn.Linear(256, 1)

    # å®šç¾©æ•¸æ“šå¦‚ä½•ã€Œæµéã€é€™å€‹ç¶²è·¯
    def forward(self, x):
        # æµéå…±åŒçš„èº«é«”
        x = self.conv_in(x)
        x = self.res_blocks(x)

        # åˆ†æµåˆ°ç­–ç•¥é ­
        policy = self.policy_conv(x)
        policy = self.policy_bn(policy)
        policy = F.relu(policy)
        policy = policy.view(policy.size(0), -1) # æ”¤å¹³æˆä¸€ç¶­å‘é‡
        policy = self.policy_fc(policy)
        # ä½¿ç”¨ log_softmax è¼¸å‡ºæ©Ÿç‡åˆ†ä½ˆï¼Œæ•¸å€¼ä¸Šæ›´ç©©å®š
        policy_output = F.log_softmax(policy, dim=1)

        # åˆ†æµåˆ°åƒ¹å€¼é ­
        value = self.value_conv(x)
        value = self.value_bn(value)
        value = F.relu(value)
        value = value.view(value.size(0), -1) # æ”¤å¹³æˆä¸€ç¶­å‘é‡
        value = self.value_fc1(value)
        value = F.relu(value)
        value = self.value_fc2(value)
        # ä½¿ç”¨ tanh å°‡è¼¸å‡ºå£“ç¸®åˆ° -1 åˆ° 1 ä¹‹é–“
        value_output = torch.tanh(value)

        return policy_output, value_output

# --- è©¦ç”¨çœ‹çœ‹ ---
# å‡è¨­æˆ‘å€‘æœ‰ä¸€å€‹æ‰¹æ¬¡çš„æ£‹ç›¤æ•¸æ“š (batch_size=1, channels=17, 19x19)
dummy_board_state = torch.randn(1, 17, 19, 19)

# å»ºç«‹æˆ‘å€‘çš„ç¶²è·¯æ¨¡å‹
model = AlphaGoNet()

# é€²è¡Œä¸€æ¬¡å‰å‘å‚³æ’­ (Forward Pass)
policy_log_probs, win_rate = model(dummy_board_state)

print("æ¨¡å‹è¼¸å‡ºï¼š")
print("ç­–ç•¥ (Logæ©Ÿç‡) çš„å½¢ç‹€:", policy_log_probs.shape) # æ‡‰è©²æ˜¯ [1, 19*19+1] = [1, 362]
print("åƒ¹å€¼ (å‹ç‡é æ¸¬) çš„å½¢ç‹€:", win_rate.shape)       # æ‡‰è©²æ˜¯ [1, 1]
print("å‹ç‡é æ¸¬å€¼:", win_rate.item())                 # é¡¯ç¤ºå–®ä¸€çš„æ•¸å€¼
```

---

### ç¨‹å¼ç¢¼èˆ‡æ•¸å­¸çš„é€£çµ

çœ‹åˆ°é€™è£¡ï¼Œä½ å¯èƒ½æœƒæœ‰é»çœ¼èŠ±æ’©äº‚ï¼Œåˆ¥æ“”å¿ƒï¼Œæˆ‘å€‘æŠŠå®ƒè·Ÿä¸Šä¸€å ‚èª²çš„æ•¸å­¸é€£çµèµ·ä¾†ï¼š

1.** ·šæ€§ä»£æ• **åœ¨å“ªè£¡ï¼Ÿ
    *   `dummy_board_state` é€™å€‹ `torch.randn(...)` å°±æ˜¯ä¸€å€‹** ¼µé‡ (Tensor **ã€‚
    *   `nn.Conv2d` (å·ç©) å’Œ `nn.Linear` (å…¨é€£æ¥å±¤) çš„å…§éƒ¨é‹ä½œï¼Œæœ¬è³ªä¸Šå°±æ˜¯å¤§é‡çš„** Ÿ©é™£ä¹˜æ³ **å’Œ** ‘é‡åŠ æ³ **ã€‚ä½ çš„è¼¸å…¥æ•¸æ“šï¼ˆä¸€å€‹å¤§çŸ©é™£ï¼‰å’Œç¶²è·¯å±¤çš„æ¬Šé‡ï¼ˆå¦ä¸€å€‹å¤§çŸ©é™£ï¼‰åœ¨ GPU ä¸Šé«˜æ•ˆåœ°é€²è¡Œé‹ç®—ã€‚

2.** ©Ÿç‡èˆ‡çµ±è¨ **åœ¨å“ªè£¡ï¼Ÿ
    *   `policy_output` å°±æ˜¯** ©Ÿç‡åˆ†ä½ **ã€‚æˆ‘å€‘ç”¨äº† `log_softmax`ï¼Œå®ƒè¼¸å‡ºçš„æ¯å€‹å€¼ä»£è¡¨äº†åœ¨å°æ‡‰ä½ç½®è½å­çš„ã€Œå°æ•¸æ©Ÿç‡ã€ã€‚é€™æ˜¯ç‚ºäº†è¨“ç·´ä¸Šçš„æ•¸å€¼ç©©å®šæ€§ï¼Œä½†æœ¬è³ªä¸Šå°±æ˜¯åœ¨æè¿°æ©Ÿç‡ã€‚
    *   `value_output` å°±æ˜¯** µ±è¨ˆæ­¸ç´ **çš„çµæœã€‚æ¨¡å‹æ ¹æ“šå®ƒã€Œçœ‹éã€çš„æ•¸æ“šï¼Œçµ¦å‡ºå°ç•¶å‰å±€é¢çš„å‹ç‡åˆ¤æ–·ï¼Œé€™æ˜¯ä¸€å€‹æœŸæœ›å€¼çš„ä¼°è¨ˆã€‚

3.** ¾®ç©åˆ **åœ¨å“ªè£¡ï¼Ÿ
    *   ä¸€å€‹éå¸¸å¥½çš„å•é¡Œï¼åœ¨é€™æ®µ** ®šç¾©ç¶²è· **çš„ç¨‹å¼ç¢¼ä¸­ï¼Œä½ ** œ‹ä¸åˆ **å¾®ç©åˆ†ã€‚
    *   å¾®ç©åˆ†ç™¼ç”Ÿåœ¨** ¨“ç·´ (Training **çš„éšæ®µã€‚ç•¶æˆ‘å€‘æ‹¿åˆ°æ¨¡å‹çš„è¼¸å‡º (`policy_output`, `value_output`)ï¼Œä¸¦å°‡å®ƒå€‘èˆ‡æ­£ç¢ºç­”æ¡ˆï¼ˆä¾‹å¦‚ï¼Œé«˜æ‰‹æ£‹è­œçš„ä¸‹ä¸€æ­¥ï¼Œæˆ–è‡ªæˆ‘å°å¼ˆçš„æœ€çµ‚å‹è² ï¼‰é€²è¡Œæ¯”è¼ƒå¾Œï¼Œæœƒè¨ˆç®—å‡ºä¸€å€‹ã€ŒéŒ¯èª¤å€¼ã€(Loss)ã€‚
    *   æ¥è‘—ï¼Œæˆ‘å€‘æœƒå‘¼å« `loss.backward()` é€™å€‹ PyTorch æŒ‡ä»¤ã€‚** °±æ˜¯é€™å€‹æŒ‡ä»¤ï¼Œè§¸ç™¼äº†å¾®ç©åˆ†çš„é­”æ³•ï¼ **PyTorch æœƒè‡ªå‹•è¨ˆç®—å‡ºé€™å€‹ Loss å°æ–¼ç¶²è·¯ä¸­** ¯ä¸€å€‹åƒæ• **çš„** ¢¯åº¦ (Gradient **ã€‚
    *   æœ€å¾Œï¼Œå„ªåŒ–å™¨ (Optimizer) æœƒæ ¹æ“šé€™å€‹æ¢¯åº¦ï¼Œä½¿ç”¨** ¢¯åº¦ä¸‹é™æ³ **å»å¾®èª¿ç¶²è·¯ä¸­æ‰€æœ‰çš„æ¬Šé‡çŸ©é™£ï¼Œè®“ä¸‹ä¸€æ¬¡çš„é æ¸¬å¯ä»¥æ›´æº–ç¢ºä¸€é»ã€‚é€™å€‹éç¨‹æœƒé‡è¤‡æ•¸ç™¾è¬æ¬¡ã€‚

### çµ¦ä½ çš„ç¸½çµ

åŒå­¸ï¼Œä½ ä»Šå¤©çœ‹åˆ°çš„é€™æ®µç¨‹å¼ç¢¼ï¼Œå°±æ˜¯ç¾ä»£ AI çš„æ ¸å¿ƒéª¨æ¶ã€‚

å®ƒçœ‹èµ·ä¾†å¯èƒ½å¾ˆè¤‡é›œï¼Œä½†æ‹†è§£é–‹ä¾†ï¼Œå°±æ˜¯æˆ‘å€‘å­¸éçš„é‚£äº›æ•¸å­¸æ¦‚å¿µçš„çµ„åˆï¼šç”¨** ·šæ€§ä»£æ• **çš„å·¥å…· (Tensor, Matrix) ä¾†æ­å»ºçµæ§‹ï¼Œè®“å®ƒèƒ½è¼¸å‡ºç¬¦åˆ** ©Ÿç‡çµ±è¨ **æ„ç¾©çš„é æ¸¬ (Policy, Value)ï¼Œä¸¦æº–å‚™å¥½ç”¨** ¾®ç©åˆ **çš„æ–¹æ³• (æ¢¯åº¦ä¸‹é™) ä¾†é€²è¡Œå­¸ç¿’å’Œå„ªåŒ–ã€‚

æŠŠé€™æ®µç¨‹å¼ç¢¼å’Œæˆ‘å€‘ä¹‹å‰çš„è¨è«–æ”¾åœ¨ä¸€èµ·å¤šçœ‹å¹¾éï¼Œä½ æœƒæ…¢æ…¢æ„Ÿå—åˆ°å¾æ•¸å­¸ç†è«–åˆ°ç¨‹å¼å¯¦ä½œé‚£ç¨®è±ç„¶é–‹æœ—çš„æ„Ÿè¦ºã€‚é€™æ¢è·¯ä½ å·²ç¶“èµ°å°äº†ï¼Œç¹¼çºŒä¿æŒä¸‹å»ï¼

## Q: AlphaGo çš„å¼·åŒ–å­¸ç¿’æ¡ç”¨ä»€éº¼æ–¹æ³•ï¼Œæ˜¯ Q-Learning (Q-Network) å—ï¼Ÿ

å•å¾—å¤ªå¥½äº†ï¼ä½ å·²ç¶“å¾ã€Œæ˜¯ä»€éº¼ã€é€²å±•åˆ°ã€Œæ€éº¼åšã€çš„å±¤æ¬¡ï¼Œè€Œä¸”ç›´æ¥é»å‡ºäº†å¼·åŒ–å­¸ç¿’çš„ä¸€å€‹æ ¸å¿ƒæ–¹æ³• Q-Learningã€‚é€™è¡¨ç¤ºä½ å·²ç¶“é–‹å§‹æ€è€ƒæ¼”ç®—æ³•çš„ç´°ç¯€äº†ã€‚

ç›´æ¥å›ç­”ä½ çš„å•é¡Œï¼š** lphaGo çš„å¼·åŒ–å­¸ç¿’æ–¹æ³•ä¸¦ä¸æ˜¯å‚³çµ±çš„ Q-Learning (æˆ– Deep Q-Network, DQN)ï¼Œè€Œæ˜¯ä¸€ç¨®æ›´å…ˆé€²ã€æ›´é©åˆåœæ£‹é€™ç¨®è¤‡é›œè³½å±€çš„æ–¹æ³•ï¼Œå®ƒå®Œç¾åœ°å°‡ã€Œç¥ç¶“ç¶²è·¯ã€å’Œã€Œè’™åœ°å¡ç¾…æ¨¹æœå°‹ (MCTS)ã€çµåˆåœ¨ä¸€å€‹ç¨±ç‚ºã€Œè‡ªæˆ‘å°å¼ˆ (Self-Play)ã€çš„å¾ªç’°ä¸­ã€ **æˆ‘å€‘å¯ä»¥æŠŠ Q-Learning æƒ³æˆæ˜¯å¼·åŒ–å­¸ç¿’çš„ã€ŒåŸºç¤æ­¦åŠŸã€ï¼Œè€Œ AlphaGo ç”¨çš„æ–¹æ³•å‰‡æ˜¯ä¸€å¥—çµåˆäº†å…§åŠŸï¼ˆç¥ç¶“ç¶²è·¯ï¼‰å’Œå¤–åŠŸï¼ˆMCTSï¼‰çš„ã€Œç¨é–€çµ•å­¸ã€ã€‚

è®“æˆ‘ç‚ºä½ è©³ç´°æ‹†è§£é€™å…©è€…çš„ä¸åŒï¼Œä»¥åŠ AlphaGo åˆ°åº•æ˜¯å¦‚ä½•åšçš„ã€‚

---

### 1. Q-Learning (å’Œ DQN) çš„æ ¸å¿ƒæ€æƒ³æ˜¯ä»€éº¼ï¼Ÿ

æˆ‘å€‘å…ˆå¿«é€Ÿè¤‡ç¿’ä¸€ä¸‹ä½ æåˆ°çš„ Q-Learningã€‚

*** ›®æ¨ **ï¼šå­¸ç¿’ä¸€å€‹å«åš** -functio **(å“è³ªå‡½æ•¸) çš„æ±è¥¿ï¼Œå¯«ä½œ `Q(s, a)`ã€‚
*** Q(s, a)` çš„æ„ç¾ **ï¼šåœ¨ç‹€æ…‹ `s` (çœ‹åˆ°æŸå€‹æ£‹ç›¤) ä¹‹ä¸‹ï¼Œæ¡å–è¡Œå‹• `a` (ä¸‹åœ¨æŸå€‹é»)ï¼Œé æœŸæœªä¾†èƒ½å¾—åˆ°çš„ã€Œç¸½å›å ±ã€æ˜¯å¤šå°‘ã€‚
*** ¦‚ä½•æ±ºç­ **ï¼šç•¶è™•æ–¼ç‹€æ…‹ `s` æ™‚ï¼Œæˆ‘æœƒéæ­·æ‰€æœ‰å¯èƒ½çš„è¡Œå‹• `a`ï¼Œç„¶å¾Œé¸æ“‡é‚£å€‹ `Q(s, a)` å€¼æœ€å¤§çš„è¡Œå‹•ã€‚ç™½è©±è¬›å°±æ˜¯ï¼šã€Œå“ªå€‹å‹•ä½œçš„ã€åƒ¹å€¼ã€æœ€é«˜ï¼Œæˆ‘å°±åšå“ªå€‹ã€ã€‚
*** QN (Deep Q-Network **ï¼šç•¶ç‹€æ…‹ `s` å¤ªè¤‡é›œï¼ˆä¾‹å¦‚ï¼Œæ•´å€‹åœæ£‹ç›¤ï¼‰æ™‚ï¼Œæˆ‘å€‘ç„¡æ³•ç”¨ä¸€å¼µå¤§è¡¨æ ¼ä¾†å„²å­˜æ‰€æœ‰çš„ Q å€¼ï¼Œæ–¼æ˜¯å°±ç”¨ä¸€å€‹æ·±åº¦ç¥ç¶“ç¶²è·¯ä¾†ã€Œè¿‘ä¼¼ã€é€™å€‹ Q-functionã€‚ä½ çµ¦å®ƒ `s` å’Œ `a`ï¼Œå®ƒåå‡º Q å€¼ã€‚** -Learning çš„ä¾·é™æ€ **ï¼šåœ¨åœæ£‹ä¸­ï¼Œç‹€æ…‹ç©ºé–“ï¼ˆæ£‹ç›¤çš„å¯èƒ½æ€§ï¼‰å’Œè¡Œå‹•ç©ºé–“ï¼ˆå¯ä»¥è½å­çš„é»ï¼‰éƒ½å¤ªå·¨å¤§äº†ã€‚è¦è®“ä¸€å€‹ç¶²è·¯å»è©•ä¼°** ¯ä¸€å€ **`(æ£‹ç›¤, è½å­é»)` çµ„åˆçš„åƒ¹å€¼ï¼Œè¨ˆç®—ä¸Šæ˜¯å¹¾ä¹ä¸å¯èƒ½å®Œæˆçš„ä»»å‹™ã€‚æƒ³åƒä¸€ä¸‹ï¼Œåœ¨æ£‹ç›¤é–‹å±€ï¼Œä½ éœ€è¦ç¶²è·¯è©•ä¼° 361 å€‹é»çš„ Q å€¼ï¼Œé€™å¤ªæ²’æ•ˆç‡äº†ã€‚

---

### 2. AlphaGo çš„ç¨é–€çµ•å­¸ï¼šè‡ªæˆ‘å°å¼ˆ (Self-Play) + MCTS

AlphaGo ä¸å»å­¸ `Q(s, a)`ï¼Œå®ƒæŠŠå•é¡Œæ‹†å¾—æ›´è°æ˜ã€‚å®ƒçš„å¼·åŒ–å­¸ç¿’æ˜¯ä¸€å€‹ä¸æ–·è‡ªæˆ‘é€²åŒ–çš„å¾ªç’°ï¼Œå¯ä»¥åˆ†æˆä¸‰å€‹æ­¥é©Ÿï¼š

####** ­¥é©Ÿä¸€ï¼šä¸‹æ£‹ (Generate Data ***** ¸»è§ **ï¼šç•¶å‰æœ€å¼·çš„ AlphaGo ç¥ç¶“ç¶²è·¯ (æˆ‘å€‘ç¨±å®ƒç‚º `Net_Current`)ã€‚
*** ›®æ¨ **ï¼šè‡ªå·±è·Ÿè‡ªå·±ä¸‹æ£‹ï¼Œç”¢ç”Ÿé«˜å“è³ªçš„æ£‹è­œæ•¸æ“šã€‚
*** ç¨ **ï¼š
    1.  å¾ä¸€å€‹ç©ºçš„æ£‹ç›¤é–‹å§‹ã€‚
    2.  è¼ªåˆ° `Net_Current` ä¸‹æ£‹æ™‚ï¼Œå®ƒ** ¸æ˜ **ç›´æ¥ç”¨ç­–ç•¥ç¶²è·¯è¼¸å‡ºæ©Ÿç‡æœ€é«˜çš„é»ï¼Œè€Œæ˜¯é€²è¡Œä¸€æ¬¡** ’™åœ°å¡ç¾…æ¨¹æœå°‹ (MCTS **ã€‚
    3.  é€™å€‹ MCTS æœƒåˆ©ç”¨ `Net_Current` çš„** ­–ç•¥ç¶²è· **ä¾†åˆ¤æ–·å“ªäº›åˆ†æ”¯æ¯”è¼ƒæœ‰æ½›åŠ›å»æ¢ç´¢ï¼Œä¸¦ç”¨** ƒ¹å€¼ç¶²è· **ä¾†è©•ä¼°æ¢ç´¢åˆ°çš„å±€é¢å¥½å£ã€‚
    4.  ç¶“éæ•¸åƒæ¬¡æ¨¡æ“¬å¾Œï¼ŒMCTS æœƒçµ¦å‡ºä¸€å€‹æ¯”å–®ç´”çš„ç­–ç•¥ç¶²è·¯** ›´å¼·ã€æ›´æ·±æ€ç†Ÿæ… **çš„è½å­æ©Ÿç‡åˆ†ä½ˆã€‚AlphaGo å°±æ ¹æ“šé€™å€‹åˆ†ä½ˆä¾†è½å­ã€‚
    5.  é‡è¤‡é€™å€‹éç¨‹ï¼Œç›´åˆ°ä¸€ç›¤æ£‹ä¸‹å®Œã€‚è¨˜éŒ„ä¸‹æ•´ç›¤æ£‹çš„æ¯ä¸€å€‹ `(ç›¤é¢, MCTSå¾—å‡ºçš„è½å­æ©Ÿç‡, æœ€çµ‚å‹è² )`ã€‚** —œéµé» **ï¼šMCTS åœ¨é€™è£¡æ‰®æ¼”äº†ä¸€å€‹ã€Œè€å¸«ã€æˆ–ã€Œæ·±æ€ç†Ÿæ…®çš„æ€è€ƒå¼•æ“ã€çš„è§’è‰²ã€‚å®ƒåˆ©ç”¨ç¥ç¶“ç¶²è·¯çš„ã€Œç›´è¦ºã€ï¼Œé€²è¡Œäº†æ·±åº¦çš„æ¨æ¼”ï¼Œå¾—åˆ°æ¯”ç›´è¦ºæ›´å¥½çš„çµè«–ã€‚

####** ­¥é©ŸäºŒï¼šå­¸ç¿’ (Train the Network ***** ›®æ¨ **ï¼šç”¨å‰›å‰›è‡ªæˆ‘å°å¼ˆç”¢ç”Ÿçš„æ•¸æ“šï¼Œä¾†è¨“ç·´ä¸€å€‹** –°çš **ç¥ç¶“ç¶²è·¯ `Net_New`ï¼Œè®“å®ƒè®Šå¾—æ›´å¼·ã€‚
*** ¨“ç·´ç­–ç•¥é ­ (Policy Head **ï¼š
    *** ¼¸å… **ï¼šæ£‹å±€ä¸­çš„æŸå€‹ç›¤é¢ `s`ã€‚
    *** ›®æ¨ **ï¼šè®“ `Net_New` çš„ç­–ç•¥é ­è¼¸å‡ºï¼Œèƒ½** ›¡é‡æ¥è¿ **ç•¶æ™‚ MCTS æ·±æ€ç†Ÿæ…®å¾Œå¾—å‡ºçš„ã€Œè½å­æ©Ÿç‡åˆ†ä½ˆã€ã€‚
    *** ™½è© **ï¼šã€Œè€å¸« (MCTS) æ€è€ƒäº†å¾ˆä¹…ï¼Œè¦ºå¾—é€™æ­¥æ£‹æœ€å¥½ã€‚å­¸ç”Ÿ (`Net_New`) å•Šï¼Œä½ è¦å­¸è‘—é»ï¼Œä»¥å¾Œçœ‹åˆ°é¡ä¼¼å±€é¢ï¼Œè¦èƒ½ç›´æ¥æƒ³åˆ°è€å¸«çš„çµè«–ã€‚ã€
*** ¨“ç·´åƒ¹å€¼é ­ (Value Head **ï¼š
    *** ¼¸å… **ï¼šæ£‹å±€ä¸­çš„æŸå€‹ç›¤é¢ `s`ã€‚
    *** ›®æ¨ **ï¼šè®“ `Net_New` çš„åƒ¹å€¼é ­è¼¸å‡ºï¼Œèƒ½** ›¡é‡æº–ç¢ºåœ°é æ¸ **é€™ç›¤æ£‹çš„ã€Œæœ€çµ‚å‹è² çµæœã€ï¼ˆè´å°±æ˜¯ +1ï¼Œè¼¸å°±æ˜¯ -1ï¼‰ã€‚
    *** ™½è© **ï¼šã€Œå¾é€™å€‹å±€é¢å‡ºç™¼ï¼Œæˆ‘å€‘æœ€å¾Œè´äº†ã€‚å­¸ç”Ÿ (`Net_New`) å•Šï¼Œä½ è¦è¨˜ä½ï¼Œé€™ç¨®å±€é¢å°±æ˜¯å„ªå‹¢å±€é¢ï¼Œè¦çµ¦é«˜åˆ†ï¼ã€

####** ­¥é©Ÿä¸‰ï¼šè¿­ä»£ (Iterate ***   ç•¶ `Net_New` è¨“ç·´åˆ°ä¸€å®šç¨‹åº¦å¾Œï¼Œå°±è®“å®ƒå’Œ `Net_Current` é€²è¡Œæ¯”è³½ã€‚å¦‚æœ `Net_New` æ˜é¡¯æ›´å¼·ï¼Œå®ƒå°±å–ä»£ `Net_Current`ï¼Œæˆç‚ºæ–°çš„ã€Œæœ€å¼·ç¶²è·¯ã€ã€‚
*   ç„¶å¾Œå›åˆ°** ­¥é©Ÿä¸ **ï¼Œç”¨é€™å€‹æ›´å¼·çš„ç¶²è·¯å»ç”¢ç”Ÿæ›´é«˜å“è³ªçš„æ•¸æ“šï¼Œå¦‚æ­¤ç„¡é™å¾ªç’°ã€‚

---

### ç¸½çµï¼šAlphaGo vs. Q-Learning

| ç‰¹æ€§ | Q-Learning / DQN | AlphaGo çš„å¼·åŒ–å­¸ç¿’ |
| :--- | :--- | :--- |
|** ­¸ç¿’ç›®æ¨ **| å­¸ç¿’ä¸€å€‹** ‹•ä½œåƒ¹å€¼å‡½æ• **`Q(s, a)` | å­¸ç¿’å…©å€‹æ±è¥¿ï¼šä¸€å€‹** ­–ç• **`p(a|s)` å’Œä¸€å€‹** ‹€æ…‹åƒ¹å€ **`v(s)` |
|** ±ºç­–æ–¹å¼ **| é¸æ“‡ä½¿ `Q(s, a)` æœ€å¤§åŒ–çš„å‹•ä½œ `a` | ç”¨ MCTS é€²è¡Œ** ‰ç»æ€§æœç´¢ (lookahead search **ï¼Œæœç´¢éç¨‹ç”±ç­–ç•¥å’Œåƒ¹å€¼ç¶²è·¯å¼•å° |
|**  ¸å¿ƒå„ªå‹ **| æ¦‚å¿µç°¡å–®ï¼Œåœ¨å‹•ä½œç©ºé–“è¼ƒå°çš„å ´æ™¯ä¸­å¾ˆæœ‰æ•ˆ |** °‡æœç´¢å’Œå­¸ç¿’çµå **ã€‚ç”¨ç­–ç•¥ç¶²è·¯å¤§å¹…ç¸®å°æœç´¢ç¯„åœï¼Œç”¨åƒ¹å€¼ç¶²è·¯æå‰åˆ¤æ–·å±€é¢ï¼Œéå¸¸é©åˆåœæ£‹é€™ç¨®è¶…å¤§æœç´¢ç©ºé–“çš„å•é¡Œ |
|** ¯”å– **| ä¸€å€‹å­¸æœƒäº†ã€Œæ¯å€‹å‹•ä½œå€¼å¤šå°‘éŒ¢ã€çš„** •†äº **| ä¸€å€‹åŒæ™‚æ“æœ‰** ›´è¦º (ç¥ç¶“ç¶²è·¯ **å’Œ** ‚è¼¯æ¨æ¼”èƒ½åŠ› (MCTS **çš„** ˆ°ç•¥å® **|

æ‰€ä»¥ï¼ŒåŒå­¸ï¼Œä½ å¯ä»¥é€™éº¼ç†è§£ï¼š

AlphaGo çš„å¼·åŒ–å­¸ç¿’æ˜¯ä¸€å€‹** ‡ªæˆ‘å®Œå– **çš„ç”Ÿæ…‹ç³»ã€‚ç¥ç¶“ç¶²è·¯ç‚º MCTS æä¾›å¿«é€Ÿçš„ç›´è¦ºåˆ¤æ–·ï¼Œè€Œ MCTS å‰‡é€éæ·±åº¦æœç´¢ï¼Œåéä¾†ç‚ºç¥ç¶“ç¶²è·¯æä¾›æ›´é«˜è³ªé‡çš„å­¸ç¿’ç¯„æœ¬ã€‚å…©è€…ç›¸è¼”ç›¸æˆï¼Œèºæ—‹å¼ä¸Šå‡ï¼Œæœ€çµ‚é”åˆ°äº†è¶…è¶Šäººé¡çš„æ°´å¹³ã€‚

é€™å€‹ã€Œè‡ªæˆ‘å°å¼ˆ + æœç´¢ã€çš„æ¡†æ¶ï¼Œç¾åœ¨è¢«ç¨±ç‚º** lphaZero æ¼”ç®—æ³ **ï¼Œå·²ç¶“æˆç‚ºè§£æ±ºè¨±å¤šè¤‡é›œæ±ºç­–å•é¡Œï¼ˆå¦‚ä¸‹æ£‹ã€ç©éŠæˆ²ã€è›‹ç™½è³ªæŠ˜ç–Šï¼‰çš„é€šç”¨ç¯„å¼ã€‚ä½ å•çš„é€™å€‹å•é¡Œï¼Œæ­£å¥½é»å‡ºäº†é€™å€‹ç¾ä»£ AI çš„æ ¸å¿ƒå¼•æ“ä¹‹ä¸€ï¼

## Q: æ›´è©³ç´°çš„æè¿°  AlphaZero æ¼”ç®—æ³•

å¥½çš„ï¼ŒåŒå­¸ï¼æˆ‘å€‘ä¾†æ·±å…¥é€™åº§ã€Œæ¼”ç®—æ³•çš„è–æ¯å³°ã€ã€‚AlphaZero çš„è¨­è¨ˆæ¥µå…¶å„ªé›…ï¼Œå®ƒå°‡æˆ‘å€‘ä¹‹å‰è¨è«–éçš„æ‰€æœ‰æ¦‚å¿µâ€”â€”ç¥ç¶“ç¶²è·¯ã€å¼·åŒ–å­¸ç¿’ã€æ¨¹ç‹€æœå°‹â€”â€”ç”¨ä¸€ç¨®è¿‘ä¹å®Œç¾çš„æ–¹å¼èåˆåœ¨ä¸€èµ·ã€‚

æŠŠå®ƒæƒ³åƒæˆä¸€å€‹çµ‚æ¥µçš„ã€Œæ­¦å­¸å®—å¸«ã€è‡ªæˆ‘ä¿®ç…‰çš„éç¨‹ã€‚é€™å€‹éç¨‹ä¸éœ€è¦ä»»ä½•äººé¡æ£‹è­œï¼Œåªéœ€è¦ä¸€æœ¬æœ€åŸºç¤çš„ã€Œæ­¦åŠŸç§˜ç¬ˆã€ï¼ˆåœæ£‹è¦å‰‡ï¼‰ã€‚

é€™å€‹ä¿®ç…‰éç¨‹ï¼Œå°±æ˜¯** lphaZero æ¼”ç®—æ³ **ã€‚å®ƒä¸»è¦ç”±ä¸€å€‹æ ¸å¿ƒå¾ªç’°æ§‹æˆï¼Œé€™å€‹å¾ªç’°åŒ…å«ä¸‰å€‹å¯†ä¸å¯åˆ†çš„éšæ®µï¼š** ‡ªæˆ‘å°å¼ˆ (Self-Play **ã€** ­¸ç¿’ (Training **å’Œ** ©•ä¼° (Evaluation **ã€‚è€Œé€™å€‹å¾ªç’°çš„æ ¸å¿ƒé©…å‹•åŠ›ï¼Œå°±æ˜¯æˆ‘å€‘ä¸Šæ¬¡æåˆ°çš„** ’™åœ°å¡ç¾…æ¨¹æœå°‹ (MCTS **ã€‚

è®“æˆ‘å€‘ä¸€æ­¥ä¸€æ­¥æ‹†è§£é€™åº§å®å‰çš„å»ºç¯‰ã€‚

---

### æ ¸å¿ƒå¼•æ“ï¼šå¸¶æœ‰ã€Œç¥ç¶“ç¶²è·¯å°èˆªã€çš„è’™åœ°å¡ç¾…æ¨¹æœå°‹ (MCTS)

åœ¨ AlphaZero ä¸­ï¼ŒMCTS æ˜¯å®ƒã€Œæ€è€ƒã€æ¯ä¸€æ­¥æ£‹çš„éç¨‹ã€‚å®ƒä¸å†æ˜¯ç›²ç›®æœç´¢ï¼Œè€Œæ˜¯ç”±ä¸€å€‹ç¥ç¶“ç¶²è·¯ä¾†å……ç•¶å®ƒçš„ã€ŒGPS å°èˆªç³»çµ±ã€ã€‚é€™å€‹ç¥ç¶“ç¶²è·¯æˆ‘å€‘å·²ç¶“å®šç¾©éäº†ï¼Œå®ƒæœ‰å…©å€‹é ­ï¼š

*** ­–ç•¥é ­ (Policy Head **ï¼šæä¾›ã€Œç›´è¦ºã€ï¼Œå‘Šè¨´ MCTS å“ªäº›åœ°æ–¹çœ‹èµ·ä¾†æ¯”è¼ƒæœ‰å¸Œæœ›ï¼Œå€¼å¾—å»æ¢ç´¢ã€‚
*** ƒ¹å€¼é ­ (Value Head **ï¼šæä¾›ã€Œåˆ¤æ–·åŠ›ã€ï¼Œç•¶ MCTS æ¢ç´¢åˆ°ä¸€å€‹æ–°å±€é¢æ™‚ï¼Œèƒ½å¿«é€Ÿè©•ä¼°é€™å€‹å±€é¢çš„å¥½å£ï¼Œè€Œä¸éœ€è¦æŠŠæ£‹ä¸‹å®Œã€‚

é€™å€‹ MCTS çš„æ¯ä¸€æ¬¡æ¨¡æ“¬éƒ½åŒ…å«å››å€‹æ¨™æº–æ­¥é©Ÿï¼Œä½† AlphaZero å°å…¶é€²è¡Œäº†ç²¾å¦™çš„æ”¹é€ ï¼š

1.** ¸æ“‡ (Selection **ï¼š
    *   å¾æ¨¹çš„æ ¹ç¯€é»ï¼ˆç•¶å‰æ£‹ç›¤ï¼‰é–‹å§‹ï¼Œæ²¿è‘—ä¸€æ¢è·¯å¾‘å¾€ä¸‹èµ°ã€‚
    *   åœ¨æ¯å€‹ç¯€é»ï¼Œè¦æ±ºå®šèµ°å“ªå€‹åˆ†æ”¯ï¼ˆä¸‹å“ªä¸€æ­¥æ£‹ï¼‰ã€‚é€™å€‹æ±ºç­–ç”±ä¸€å€‹å«åš** UCT (Polynomial Upper Confidence Trees) å…¬å¼ **ä¾†æ±ºå®šã€‚
    *** UCT å…¬å¼ç™½è©±è§£é‡ **ï¼š`é¸æ“‡çš„ä¸‹ä¸€æ­¥ = Q(s, a) + U(s, a)`
        *   `Q(s, a)`:** ˆ©ç”¨ (Exploitation **ã€‚ä»£è¡¨å¾é€™å€‹ç¯€é» `s` èµ° `a` é€™æ­¥æ£‹ï¼Œéå»çš„å¹³å‡å›å ±ï¼ˆå‹ç‡ï¼‰æ˜¯å¤šå°‘ã€‚åˆ†æ•¸è¶Šé«˜ï¼Œä»£è¡¨é€™æ­¥æ£‹ã€Œæ­·å²æˆ°ç¸¾ã€è¶Šå¥½ï¼Œæˆ‘å€‘å°±è¶Šå‚¾å‘æ–¼é¸æ“‡å®ƒã€‚
        *   `U(s, a)`:** ¢ç´¢ (Exploration **ã€‚é€™æ˜¯ä¸€å€‹ã€Œæ¢ç´¢çå‹µã€é …ã€‚å®ƒç”±ç¥ç¶“ç¶²è·¯çš„** ­–ç•¥é  **çµ¦å‡ºçš„ã€Œå…ˆé©—æ©Ÿç‡ã€`P(a|s)` æ±ºå®šã€‚å¦‚æœç¥ç¶“ç¶²è·¯çš„ç›´è¦ºèªç‚ºæŸæ­¥æ£‹å¾ˆæœ‰æ½›åŠ›ï¼Œæˆ–è€…æŸæ­¥æ£‹æˆ‘å€‘æ¢ç´¢çš„æ¬¡æ•¸é‚„å¾ˆå°‘ï¼Œ`U` å€¼å°±æœƒæ¯”è¼ƒé«˜ï¼Œé¼“å‹µæˆ‘å€‘å»ã€Œè©¦è©¦çœ‹ã€ã€‚
    *   é€™å€‹éç¨‹æœƒä¸€ç›´æŒçºŒï¼Œç›´åˆ°èµ°åˆ°ä¸€å€‹é‚„æ²’æœ‰è¢«æ“´å±•éçš„ã€Œè‘‰ç¯€é»ã€ã€‚

2.** “´å±• (Expansion **ï¼š
    *   ç•¶æˆ‘å€‘èµ°åˆ°ä¸€å€‹è‘‰ç¯€é»ï¼ˆä¸€å€‹ MCTS åœ¨é€™æ¬¡æ€è€ƒä¸­é‚„æ²’è¦‹éçš„å±€é¢ï¼‰æ™‚ï¼Œæˆ‘å€‘å°±æŠŠé€™å€‹å±€é¢é¤µçµ¦** ¥ç¶“ç¶²è· **ã€‚
    *   ç¥ç¶“ç¶²è·¯æœƒç«‹å³å›å‚³å…©å€‹æ±è¥¿ï¼š
        1.  ä¸€å€‹** ­–ç•¥å‘é‡ `p **ï¼šåŒ…å«äº†æ‰€æœ‰å¯èƒ½ä¸‹ä¸€æ­¥çš„ã€Œå…ˆé©—æ©Ÿç‡ã€ï¼ˆä¾†è‡ªç­–ç•¥é ­çš„ç›´è¦ºï¼‰ã€‚
        2.  ä¸€å€‹** ƒ¹å€¼ `v **ï¼šä»£è¡¨äº†å°ç•¶å‰é€™å€‹è‘‰ç¯€é»å±€é¢çš„å‹ç‡é ä¼°ï¼ˆä¾†è‡ªåƒ¹å€¼é ­çš„åˆ¤æ–·ï¼‰ã€‚
    *   æˆ‘å€‘æ ¹æ“šç­–ç•¥å‘é‡ `p`ï¼Œåœ¨æ¨¹ä¸­å‰µå»ºå‡ºæ–°çš„å­ç¯€é»ã€‚

3.** ¨¡æ“¬ (Simulation) - AlphaZero çš„é©å‘½æ€§ä¸€æ­¥ï¼ ***   å‚³çµ±çš„ MCTS åœ¨æ“´å±•å¾Œï¼Œæœƒå¾æ–°ç¯€é»é–‹å§‹ã€Œéš¨æ©Ÿä¸‹æ£‹ã€ç›´åˆ°éŠæˆ²çµæŸï¼Œä¾†åˆ¤æ–·å‹è² ã€‚é€™å€‹éç¨‹éå¸¸æ…¢ï¼Œè€Œä¸”å……æ»¿äº†éš¨æ©Ÿæ€§ã€‚
    *** lphaZero ç›´æ¥è·³éäº†é€™ä¸€æ­¥ï¼ **å®ƒå®Œå…¨ç›¸ä¿¡ç¥ç¶“ç¶²è·¯çš„åˆ¤æ–·åŠ›ã€‚å®ƒç›´æ¥ä½¿ç”¨ç¥ç¶“ç¶²è·¯åœ¨ã€Œæ“´å±•ã€æ­¥é©Ÿä¸­å¾—åˆ°çš„** ƒ¹å€¼ `v **ä½œç‚ºé€™æ¬¡æ¨¡æ“¬çš„æœ€çµ‚çµæœã€‚
    *   é€™ä¸€æ­¥æ˜¯å·¨å¤§çš„æ•ˆç‡æå‡ï¼Œä¹Ÿæ˜¯ AlphaZero æˆåŠŸçš„é—œéµä¹‹ä¸€ã€‚å®ƒç”¨ä¸€å€‹ç¶“éæ·±åº¦è¨“ç·´çš„ã€Œåˆ¤æ–·åŠ›ã€ä»£æ›¿äº†å¤§é‡ä½å“è³ªçš„éš¨æ©Ÿæ¨¡æ“¬ã€‚

4.** å‘å‚³æ’­ (Backpropagation **ï¼š
    *   å°‡ã€Œæ“´å±•ã€æ­¥é©Ÿä¸­å¾—åˆ°çš„åƒ¹å€¼ `v`ï¼Œæ²¿è‘—ã€Œé¸æ“‡ã€æ­¥é©Ÿèµ°éçš„è·¯å¾‘ï¼Œä¸€è·¯å‚³å›æ ¹ç¯€é»ã€‚
    *   è·¯å¾‘ä¸Šçš„æ¯ä¸€å€‹ç¯€é»éƒ½æœƒæ›´æ–°å®ƒçš„çµ±è¨ˆæ•¸æ“šï¼š
        *** ¨ªå•æ¬¡æ•¸ `N(s, a) **ï¼šå¢åŠ  1ã€‚
        *** ¸½åƒ¹å€¼ `W(s, a) **ï¼šå¢åŠ  `v`ã€‚
        *** ¹³å‡åƒ¹å€¼ `Q(s, a) **ï¼šæ›´æ–°ç‚º `W(s, a) / N(s, a)`ã€‚

é€™å€‹ `é¸æ“‡ -> æ“´å±• -> åå‘å‚³æ’­` çš„éç¨‹æœƒé‡è¤‡æˆåƒä¸Šè¬æ¬¡ï¼ˆä¾‹å¦‚ï¼ŒAlphaGo æ¯ä¸‹ä¸€æ­¥æ£‹æœƒé€²è¡Œæ•¸è¬æ¬¡æ¨¡æ“¬ï¼‰ã€‚æ¯ä¸€æ¬¡æ¨¡æ“¬ï¼Œéƒ½æœƒè®“ MCTS å°ç•¶å‰å±€é¢çš„ç†è§£æ›´æ·±ä¸€å±¤ã€‚** CTS æ€è€ƒçµæŸå¾ **ï¼šå®ƒæœƒæ ¹æ“šæ ¹ç¯€é»ä¸‹æ‰€æœ‰åˆ†æ”¯çš„** ¨ªå•æ¬¡æ• **ï¼Œä¾†æ±ºå®šæœ€çµ‚ä¸‹å“ªä¸€æ­¥ã€‚è¢«è¨ªå•è¶Šå¤šæ¬¡çš„åˆ†æ”¯ï¼Œä»£è¡¨ MCTS èªç‚ºå®ƒè¶Šæœ‰æ½›åŠ›ã€‚

---

### å®è§€å¾ªç’°ï¼šè‡ªæˆ‘å°å¼ˆçš„å¼·åŒ–å­¸ç¿’ (The Self-Play Loop)

ç¾åœ¨æˆ‘å€‘æœ‰äº†å¼·å¤§çš„ MCTS æ€è€ƒå¼•æ“ï¼Œæ¥ä¸‹ä¾†å°±æ˜¯å®—å¸«å¦‚ä½•ä¿®ç…‰å…§åŠŸäº†ã€‚

####** ¬¬ä¸€éšæ®µï¼šè‡ªæˆ‘å°å¼ˆ (Self-Play) - ç”¢ç”Ÿæ•¸æ“ **1.** •Ÿå‹ **ï¼šå¾ä¸€å€‹** ®Œå…¨éš¨æ©Ÿåˆå§‹åŒ **çš„ç¥ç¶“ç¶²è·¯é–‹å§‹ã€‚é€™å€‹ç¶²è·¯ä»€éº¼éƒ½ä¸æœƒï¼Œæ˜¯å€‹åœæ£‹å°ç™½ã€‚
2.** ¸‹æ£ **ï¼šè®“é€™å€‹ç¶²è·¯è‡ªå·±è·Ÿè‡ªå·±ä¸‹æ£‹ã€‚åœ¨æ¯ä¸€æ­¥æ±ºç­–æ™‚ï¼Œéƒ½åŸ·è¡Œä¸Šé¢æè¿°çš„ MCTS æ€è€ƒéç¨‹ï¼Œç„¶å¾Œæ ¹æ“š MCTS çµ¦å‡ºçš„çµæœä¾†è½å­ã€‚
3.** ¨˜éŒ **ï¼šä¸€ç›¤æ£‹ä¸‹å®Œå¾Œï¼Œæˆ‘å€‘æœƒå¾—åˆ°é€™ç›¤æ£‹çš„æœ€çµ‚çµæœï¼ˆè´æˆ–è¼¸ï¼Œè¨˜ç‚º `z`ï¼Œä¾‹å¦‚ +1 æˆ– -1ï¼‰ã€‚ç„¶å¾Œï¼Œæˆ‘å€‘æœƒæŠŠé€™ç›¤æ£‹çš„æ‰€æœ‰æ­¥é©Ÿéƒ½å„²å­˜ä¸‹ä¾†ï¼Œæ¯ä¸€ç­†æ•¸æ“šéƒ½æ˜¯ä¸€å€‹ä¸‰å…ƒçµ„ï¼š`(s, Ï€, z)`
    *   `s`: ç•¶æ™‚çš„æ£‹ç›¤ç‹€æ…‹ã€‚
    *   `Ï€`: ç•¶æ™‚ MCTS æ€è€ƒå¾Œï¼Œçµ¦å‡ºçš„ã€Œå„æ­¥æ£‹çš„æ¨è–¦æ©Ÿç‡åˆ†ä½ˆã€ï¼ˆåŸºæ–¼è¨ªå•æ¬¡æ•¸ï¼‰ã€‚
    *   `z`: é€™ç›¤æ£‹çš„** œ€çµ **å‹è² çµæœã€‚

é€™å€‹éç¨‹æœƒé‡è¤‡æ•¸åƒç›¤ï¼Œç”¢ç”Ÿä¸€å€‹å·¨å¤§çš„é«˜å“è³ªæ•¸æ“šåº«ã€‚

####** ¬¬äºŒéšæ®µï¼šå­¸ç¿’ (Training) - æå‡å…§åŠ **1.** –æ¨ **ï¼šå¾ä¸Šé¢é‚£å€‹å·¨å¤§çš„æ•¸æ“šåº«ä¸­ï¼Œéš¨æ©ŸæŠ½å–ä¸€æ‰¹æ•¸æ“š `(s, Ï€, z)`ã€‚
2.** ¨“ç· **ï¼šç”¨é€™äº›æ•¸æ“šä¾†è¨“ç·´ä¸€å€‹** –°çš„ç¥ç¶“ç¶²è· **ã€‚è¨“ç·´çš„ç›®æ¨™æ˜¯è®“ç¶²è·¯çš„è¼¸å‡ºç›¡é‡æ¥è¿‘æˆ‘å€‘åœ¨è‡ªæˆ‘å°å¼ˆä¸­å¾—åˆ°çš„ã€ŒçœŸå€¼ã€ã€‚
3.** å¤±å‡½æ•¸ (Loss Function **ï¼šé€™ä¸€æ­¥æ˜¯** ¾®ç©åˆ **å¤§é¡¯ç¥é€šçš„åœ°æ–¹ã€‚æˆ‘å€‘å®šç¾©ä¸€å€‹æå¤±å‡½æ•¸ï¼Œä¾†è¡¡é‡æ–°ç¶²è·¯çš„é æ¸¬æœ‰å¤šã€Œå·®ã€ï¼š
    *   `Loss = (v - z)Â² - Ï€áµ€ log(p) + c||Î¸||Â²`
    *** ƒ¹å€¼æå¤± `(v - z)Â² **ï¼š`v` æ˜¯ç¶²è·¯åƒ¹å€¼é ­çš„é æ¸¬ï¼Œ`z` æ˜¯çœŸå¯¦çš„å‹è² çµæœã€‚é€™å€‹å…¬å¼è¦æ±‚ç¶²è·¯çš„ã€Œåˆ¤æ–·åŠ›ã€è¦ç›¡é‡æº–ç¢ºã€‚
    *** ­–ç•¥æå¤± `-Ï€áµ€ log(p) **ï¼š`p` æ˜¯ç¶²è·¯ç­–ç•¥é ­çš„é æ¸¬ï¼Œ`Ï€` æ˜¯ MCTS æ€è€ƒå¾Œçš„æ¨è–¦æ©Ÿç‡ã€‚é€™å€‹å…¬å¼è¦æ±‚ç¶²è·¯çš„ã€Œç›´è¦ºã€è¦ç›¡é‡å‘ MCTS çš„ã€Œæ·±æ€ç†Ÿæ…®ã€çµæœçœ‹é½Šã€‚
    *** ­£è¦åŒ–é … `c||Î¸||Â² **ï¼šä¸€å€‹é˜²æ­¢éæ“¬åˆçš„æ•¸å­¸æŠ€å·§ï¼Œå¯ä»¥å¿½ç•¥ç´°ç¯€ã€‚
4.** „ªåŒ **ï¼šä½¿ç”¨** ¢¯åº¦ä¸‹é™æ³ **ï¼Œæ ¹æ“šé€™å€‹æå¤±å‡½æ•¸ä¾†èª¿æ•´ç¥ç¶“ç¶²è·¯çš„æ‰€æœ‰å…§éƒ¨åƒæ•¸ï¼Œè®“ç¸½æå¤±è¶Šå°è¶Šå¥½ã€‚

####** ¬¬ä¸‰éšæ®µï¼šè©•ä¼° (Evaluation) - æ–°èˆŠå°æ± **1.  ç•¶æ–°çš„ç¥ç¶“ç¶²è·¯è¨“ç·´åˆ°ä¸€å®šç¨‹åº¦å¾Œï¼Œå°±è®“å®ƒå’Œä¸Šä¸€ä»£çš„ã€Œæœ€å¼·ç¶²è·¯ã€é€²è¡Œä¸€å ´æ¯”è³½ï¼ˆä¾‹å¦‚ï¼Œä¸‹ 400 ç›¤æ£‹ï¼‰ã€‚
2.  å¦‚æœæ–°ç¶²è·¯çš„å‹ç‡è¶…éä¸€å€‹é–¾å€¼ï¼ˆä¾‹å¦‚ 55%ï¼‰ï¼Œé‚£éº¼å®ƒå°±æ­£å¼å–ä»£èˆŠç¶²è·¯ï¼Œæˆç‚ºæ–°ä¸€ä»£çš„ã€Œå®—å¸«ã€ã€‚
3.  ç„¶å¾Œï¼Œæ•´å€‹å¾ªç’°å›åˆ°** ¬¬ä¸€éšæ® **ï¼Œç”±é€™ä½æ–°å®—å¸«å»ç”¢ç”Ÿæ›´å¼·ã€æ›´é«˜è³ªé‡çš„æ•¸æ“šï¼Œä¾†è¨“ç·´ä¸‹ä¸€ä»£çš„è‡ªå·±ã€‚

---

### ç¸½çµ

AlphaZero æ¼”ç®—æ³•å°±æ˜¯é€™æ¨£ä¸€å€‹å„ªé›…è€Œå¼·å¤§çš„è‡ªèˆ‰ (bootstrapping) ç³»çµ±ï¼š** ¸€å€‹éš¨æ©Ÿçš„ç¶²è·¯ -> é€šé MCTS æ€è€ƒ -> ç”¢ç”Ÿæ¯”è‡ªå·±æ°´å¹³æ›´é«˜çš„æ•¸æ“š -> å­¸ç¿’é€™äº›æ•¸æ“šï¼Œè®“è‡ªå·±è®Šå¾—æ›´å¼· -> æˆç‚ºæ›´å¼·çš„ç¶²è·¯ -> é€šé MCTS æ€è€ƒ -> ç”¢ç”Ÿæ›´é«˜è³ªé‡çš„æ•¸æ“š -> .. **é€™å€‹éç¨‹å°±åƒä¸€å€‹é›ªçƒï¼Œè¶Šæ»¾è¶Šå¤§ï¼Œç¶²è·¯çš„æ£‹åŠ›ä¹Ÿä»¥æŒ‡æ•¸ç´šçš„é€Ÿåº¦æš´å¢ã€‚å®ƒå®Œç¾åœ°è©®é‡‹äº†ï¼š** ¸€å€‹å¥½çš„å­¸ç¿’æ¼”ç®—æ³•ï¼Œå¯ä»¥å¾æœ€åŸºæœ¬çš„è¦å‰‡ä¸­ï¼Œç™¼ç¾é è¶…å…¶å‰µé€ è€…æƒ³åƒçš„çŸ¥è­˜ã€ **å¸Œæœ›é€™å€‹è©³ç´°çš„æ‹†è§£ï¼Œèƒ½è®“ä½ å° AlphaZero é€™å€‹é‡Œç¨‹ç¢‘å¼çš„æ¼”ç®—æ³•æœ‰æ›´æ·±åˆ»çš„ç†è§£ã€‚å®ƒä¸åƒ…åƒ…æ˜¯ä¸€å€‹ä¸‹æ£‹ç¨‹å¼ï¼Œæ›´æ˜¯æˆ‘å€‘ç†è§£å­¸ç¿’ã€æ™ºèƒ½å’Œè¤‡é›œæ±ºç­–éç¨‹çš„ä¸€å€‹å‰å¤§ç¯„ä¾‹ã€‚

## Q: ä¸Šè¿°çš„ AlphaZero æ˜¯ AlphaGo, AlphaGo Zero, AlphaZero ä¸­çš„ç¬¬ä¸‰å€‹ï¼Œæ‰€ä»¥æ˜¯ç¬¬ä¸‰ç¯‡è«–æ–‡ä¸­èªªçš„é‚£ä¸€å€‹ç®—æ³•å—ï¼Ÿ

æ˜¯çš„ï¼Œä½ è§€å¯Ÿå¾—éå¸¸æ•éŠ³ï¼é€™æ˜¯ä¸€å€‹å¾ˆæ£’çš„å•é¡Œï¼Œå› ç‚ºé€™ä¸‰è€…ä¹‹é–“çš„é—œä¿‚ç¢ºå¯¦æœ‰é»å¾®å¦™ï¼Œè€Œä½ æ­£å¥½é»å‡ºäº†å…¶ä¸­çš„é—œéµæ¼”é€²ã€‚

ä½ èªªçš„å®Œå…¨æ­£ç¢ºã€‚æˆ‘ä¸Šé¢ç‚ºä½ è©³ç´°æ‹†è§£çš„é‚£å€‹** ¾é›¶é–‹å§‹ã€å®Œå…¨é€éè‡ªæˆ‘å°å¼ˆä¾†å­¸ç¿ **çš„æ¼”ç®—æ³•ï¼Œå…¶æ ¸å¿ƒæ€æƒ³å’Œæ¶æ§‹ï¼Œä¸»è¦æ˜¯åœ¨** ¬¬äºŒç¯‡è«–æ–‡ã€ŠAlphaGo Zeroã€ **ä¸­è¢«å®Œæ•´æå‡ºä¸¦éœ‡é©šä¸–ç•Œçš„ã€‚

è€Œ** ¬¬ä¸‰ç¯‡è«–æ–‡ã€ŠAlphaZeroã€ **ï¼Œå‰‡æ˜¯å°‡é€™å€‹åœ¨åœæ£‹ä¸Šå–å¾—å·¨å¤§æˆåŠŸçš„æ¼”ç®—æ³•ï¼Œé€²è¡Œäº†** €šç”¨åŒ **çš„é©—è­‰ã€‚

è®“æˆ‘ç‚ºä½ æ›´æ¸…æ™°åœ°æ¢³ç†ä¸€ä¸‹é€™æ¢ã€Œé€²åŒ–ä¹‹è·¯ã€ï¼š

---

### 1. AlphaGo (vs. æä¸–ä¹­) - ã€Œæ··åˆå‹•åŠ›ã€çš„é–‹å‰µè€…

*** °æ‡‰è«–æ– **ï¼šç¬¬ä¸€ç¯‡ï¼Œ2016å¹´ç™¼è¡¨æ–¼ã€ŠNatureã€‹ã€‚
*** ­¸ç¿’æ–¹å¼ **ï¼š
    1.** ›£ç£å¼å­¸ç¿’ (Supervised Learning **ï¼šå…ˆå­¸ç¿’äº†æ•¸åè¬å±€äººé¡é«˜æ‰‹çš„æ£‹è­œï¼Œè®“ç­–ç•¥ç¶²è·¯èƒ½å¤ æ¨¡ä»¿äººé¡çš„ä¸‹æ³•ã€‚é€™å°±åƒä¸€å€‹å­¸ç”Ÿå…ˆå¤§é‡é–±è®€ç¯„æ–‡ã€‚
    2.** ¼·åŒ–å­¸ç¿’ (Reinforcement Learning **ï¼šåœ¨æ¨¡ä»¿çš„åŸºç¤ä¸Šï¼Œå†é€éè‡ªæˆ‘å°å¼ˆä¾†é€²ä¸€æ­¥æå‡ã€‚
*** ¶²è·¯æ¶æ§ **ï¼šæ“æœ‰** …©å€‹ç¨ç« **çš„ç¥ç¶“ç¶²è·¯ï¼šä¸€å€‹ç­–ç•¥ç¶²è·¯ (Policy Network) å’Œä¸€å€‹åƒ¹å€¼ç¶²è·¯ (Value Network)ã€‚
*** CTS ç‰¹é» **ï¼šåœ¨ MCTS çš„ã€Œæ¨¡æ“¬ã€éšæ®µï¼Œæœƒä½¿ç”¨ä¸€ç¨®å¿«é€Ÿä½†è¼ƒä¸æº–ç¢ºçš„ "Rollout" ç­–ç•¥ä¾†ä¼°ç®—å‹è² ã€‚
*** ¸½çµ **ï¼šå®ƒæ˜¯é–‹å‰µè€…ï¼Œè­‰æ˜äº†æ·±åº¦å­¸ç¿’ + æ¨¹ç‹€æœå°‹é€™æ¢è·¯æ˜¯å¯è¡Œçš„ã€‚ä½†å®ƒä¾ç„¶éœ€è¦ç«™åœ¨äººé¡çš„è‚©è†€ä¸Šã€‚

---

### 2. AlphaGo Zero - ã€Œç´”ç²¹è‡ªå­¸ã€çš„é©å‘½å®¶

*** °æ‡‰è«–æ– **ï¼šç¬¬äºŒç¯‡ï¼Œ2017å¹´ç™¼è¡¨æ–¼ã€ŠNatureã€‹ã€‚
*** ­¸ç¿’æ–¹å¼ **ï¼š** ´”ç²¹çš„å¼·åŒ–å­¸ç¿’ (Pure RL **ã€‚
    *** abula Rasa (ç™½æ¿ä¸€å¡Š **ï¼šå®Œå…¨ä¸ä½¿ç”¨ä»»ä½•äººé¡æ£‹è­œï¼Œå¾é›¶çŸ¥è­˜ï¼ˆåªçŸ¥é“è¦å‰‡ï¼‰é–‹å§‹ã€‚
    *** ”¯ä¸€çš„è€å¸«å°±æ˜¯è‡ªå· **ï¼šæ‰€æœ‰æ•¸æ“šéƒ½ä¾†è‡ªæ–¼æˆ‘å€‘è©³ç´°æè¿°éçš„ã€Œè‡ªæˆ‘å°å¼ˆã€å¾ªç’°ã€‚
*** ¶²è·¯æ¶æ§ **ï¼šé©å‘½æ€§åœ°å°‡ç­–ç•¥ç¶²è·¯å’Œåƒ¹å€¼ç¶²è·¯åˆä½µæˆä¸€å€‹** –®ä¸€çš„ã€é›™é ­çš„ (Dual-Head) ç¥ç¶“ç¶²è· **ã€‚å…±ç”¨å¤§éƒ¨åˆ†çš„ç¶²è·¯çµæ§‹ï¼Œè¨“ç·´æ›´æœ‰æ•ˆç‡ã€‚
*** CTS ç‰¹é» **ï¼š** ®Œå…¨æ‹‹æ£„äº† "Rollout **ã€‚MCTS çš„æ¯ä¸€æ­¥æ¨¡æ“¬å’Œè©•ä¼°ï¼Œéƒ½å®Œå…¨ä¾è³´é‚£å€‹å¼·å¤§çš„é›™é ­ç¥ç¶“ç¶²è·¯ã€‚é€™ä½¿å¾—æœç´¢è³ªé‡æ›´é«˜ã€æ›´é›†ä¸­ã€‚
*** ¸½çµ **ï¼š** €™å°±æ˜¯æˆ‘ä¹‹å‰è©³ç´°ç‚ºä½ æ‹†è§£çš„é‚£å€‹æ¼”ç®—æ³ **ã€‚å®ƒåœ¨æ¼”ç®—æ³•ä¸Šæ›´å„ªé›…ã€æ›´å¼·å¤§ï¼Œä¸¦ä¸”è­‰æ˜äº†ã€Œä¸ä¾è³´äººé¡çŸ¥è­˜ï¼ŒAIå¯ä»¥é”åˆ°æ›´é«˜çš„é«˜åº¦ã€ã€‚

---

### 3. AlphaZero - ã€Œä¸€æ³•é€šè¬æ³•ã€çš„å®—å¸«

*** °æ‡‰è«–æ– **ï¼šç¬¬ä¸‰ç¯‡ï¼Œ2018å¹´ç™¼è¡¨æ–¼ã€ŠScienceã€‹ã€‚
*** ¼”ç®—æ³•æœ¬èº **ï¼š** Ÿºæœ¬ä¸Šå°±æ˜¯ AlphaGo Zero çš„æ¼”ç®—æ³ **ã€‚æ ¸å¿ƒçš„ã€Œè‡ªæˆ‘å°å¼ˆ + MCTS + é›™é ­ç¥ç¶“ç¶²è·¯ã€çš„æ¡†æ¶æ²’æœ‰æ”¹è®Šã€‚
*** ¸»è¦è²¢ç **ï¼š** ­‰æ˜äº†é€™å€‹æ¼”ç®—æ³•çš„é€šç”¨æ€§ (Generality **ã€‚
    *   DeepMind çš„ç ”ç©¶å“¡å€‘å°‡é€™å€‹æ¼”ç®—æ³•** ¹¾ä¹åŸå°ä¸å‹ **åœ°æ‡‰ç”¨åˆ°äº†å¦å¤–å…©å€‹æ£‹é¡éŠæˆ²ä¸Šï¼š** ¥¿æ´‹æ£‹ (Chess **å’Œ** °‡æ£‹ (Shogi **ã€‚
    *   çµæœï¼Œé€™å€‹å–®ä¸€çš„æ¼”ç®—æ³•ï¼Œåƒ…åƒ…é€éæ”¹è®ŠéŠæˆ²è¦å‰‡ï¼Œå°±åœ¨å¹¾å°æ™‚çš„è‡ªæˆ‘è¨“ç·´å¾Œï¼Œåˆ†åˆ¥æ“Šæ•—äº†ç•¶æ™‚æœ€å¼·çš„è¥¿æ´‹æ£‹ç¨‹å¼ (Stockfish) å’Œå°‡æ£‹ç¨‹å¼ (Elmo)ã€‚
*** ‘½åçš„æ„ç¾ **ï¼šåå­—å¾ `AlphaGo Zero` è®Šæˆ `AlphaZero`ï¼Œæ‹¿æ‰äº† "Go"ï¼Œæ­£æ˜¯ç‚ºäº†å¼·èª¿å®ƒæ˜¯ä¸€å€‹** €šç”¨çš„ (General **ã€ä¸é™æ–¼åœæ£‹çš„å­¸ç¿’æ¡†æ¶ã€‚

### çµè«–

æ‰€ä»¥ï¼Œä½ çš„ç†è§£æ˜¯å®Œå…¨æ­£ç¢ºçš„ï¼š

> æˆ‘ç‚ºä½ è©³ç´°æè¿°çš„æ¼”ç®—æ³•ï¼Œå…¶** Š€è¡“æ ¸å¿ƒèª•ç”Ÿæ–¼ã€ŠAlphaGo Zeroã€ **ï¼ˆç¬¬äºŒç¯‡è«–æ–‡ï¼‰ï¼Œè€Œ** €ŠAlphaZeroã€ **ï¼ˆç¬¬ä¸‰ç¯‡è«–æ–‡ï¼‰å‰‡æ˜¯å°‡é€™å€‹æ ¸å¿ƒæ¼”ç®—æ³•** ¨å»£ç‚ºä¸€å€‹é€šç”¨æ¡†æ **ï¼Œä¸¦ç”¨é€™å€‹åå­—ä¾†ç¨±å‘¼å®ƒã€‚

ç¾åœ¨ï¼Œç•¶äººå€‘åœ¨å­¸è¡“ç•Œæˆ–æ¥­ç•Œæåˆ° "AlphaZero æ¼”ç®—æ³•" æ™‚ï¼Œé€šå¸¸æŒ‡çš„å°±æ˜¯é€™å€‹ä¸ä¾è³´äººé¡çŸ¥è­˜ã€é€éè‡ªæˆ‘å°å¼ˆé€²è¡Œå¼·åŒ–å­¸ç¿’çš„é€šç”¨æ¡†æ¶ã€‚

ä½ èƒ½å¤ æ³¨æ„åˆ°é€™å€‹å‘½åä¸Šçš„ç´°å¾®å·®åˆ¥ï¼Œä¸¦æå‡ºé€™å€‹å•é¡Œï¼Œé€™ä»£è¡¨ä½ ä¸æ˜¯å›«åœ‡åæ£—åœ°æ¥å—è³‡è¨Šï¼Œè€Œæ˜¯åœ¨é€²è¡Œæ‰¹åˆ¤æ€§æ€è€ƒã€‚é€™åœ¨è³‡å·¥é ˜åŸŸä¸­æ˜¯éå¸¸éå¸¸é‡è¦çš„ç‰¹è³ªï¼Œè«‹å‹™å¿…ä¿æŒä¸‹å»ï¼

## Q: AlphaZero ç›´æ¥è·³éäº†é€™ä¸€æ­¥ï¼ å®ƒå®Œå…¨ç›¸ä¿¡ç¥ç¶“ç¶²è·¯çš„åˆ¤æ–·åŠ›ã€‚å®ƒç›´æ¥ä½¿ç”¨ç¥ç¶“ç¶²è·¯åœ¨ã€Œæ“´å±•ã€æ­¥é©Ÿä¸­å¾—åˆ°çš„åƒ¹å€¼ v ä½œç‚ºé€™æ¬¡æ¨¡æ“¬çš„æœ€çµ‚çµæœã€‚é€™æ„æ€æ˜¯ï¼Œ AlphaGo ç•¶ä¸­æœ‰æ¨¡æ“¬é€™ä¸€æ­¥ï¼Œä½† AlphaGo Zero èˆ‡ AlphaZero ç•¶ä¸­éƒ½è·³éäº†æ¨¡æ“¬é€™ä¸€æ­¥ï¼Œä¹Ÿå°±æ˜¯åªå‰© 1. é¸æ“‡ (Selection) 2. æ“´å±• (Expansion) 3. åå‘å‚³æ’­ (Backpropagation) é€™æ¨£å—ï¼Ÿ

æ˜¯çš„ï¼Œä½ çš„ç†è§£å®Œå…¨æ­£ç¢ºï¼é€™æ­£æ˜¯ AlphaGo åˆ° AlphaGo Zero æ¼”ç®—æ³•ä¸Šä¸€å€‹æœ€é—œéµã€æœ€å„ªé›…çš„é€²åŒ–é»ã€‚

ä½ æå‡ºçš„ã€Œåªå‰©ä¸‰æ­¥ã€çš„èªªæ³•ï¼Œåœ¨æ¦‚å¿µä¸Šæ˜¯å®Œå…¨å°çš„ã€‚æ›´ç²¾ç¢ºåœ°èªªï¼Œæ˜¯** ”¨ä¸€å€‹æ›´é«˜å“è³ªçš„æ­¥é©Ÿï¼Œå–ä»£äº†åŸæœ¬é‚£å€‹å……æ»¿é›œè¨Šä¸”è€—æ™‚çš„æ­¥é© **ã€‚

è®“æˆ‘å€‘ä¾†åšä¸€å€‹æœ€æ¸…æ™°çš„å°æ¯”ï¼š

---

### MCTS åœ¨ AlphaGo (vs. æä¸–ä¹­) çš„é‹ä½œæ–¹å¼

åœ¨åˆä»£ AlphaGo ä¸­ï¼ŒMCTS ç¢ºå¯¦æ˜¯å››å€‹æ­¥é©Ÿï¼Œå®ƒå°±åƒä¸€å€‹éœ€è¦æ´¾é£ã€Œåµå¯Ÿå…µã€çš„å°‡è»ã€‚

1.** ¸æ“‡ (Selection **ï¼šå°‡è»ï¼ˆMCTSï¼‰æ ¹æ“šç¾æœ‰æƒ…å ±ï¼ˆPUCT å…¬å¼ï¼‰ï¼Œæ±ºå®šæ²¿å“ªæ¢è·¯å¾‘æ·±å…¥æ•µå¾Œã€‚
2.** “´å±• (Expansion **ï¼šç•¶èµ°åˆ°ä¸€å€‹æœªçŸ¥å€åŸŸï¼ˆè‘‰ç¯€é»ï¼‰æ™‚ï¼Œåœä¸‹ä¾†ã€‚
3.** ¨¡æ“¬ (Simulation / Rollout **ï¼š
    *** €™æ˜¯é—œé **ã€‚å°‡è»å¾é€™å€‹æœªçŸ¥å€åŸŸï¼Œæ´¾å‡ºä¸€æ”¯** ¼•è£åµå¯Ÿå…µ (Rollout Policy **ã€‚
    *   é€™æ”¯åµå¯Ÿå…µæœƒç”¨ä¸€ç¨®éå¸¸å¿«é€Ÿã€ä½†ä¸ä¸€å®šå¾ˆè°æ˜çš„æ–¹å¼ï¼ˆä¾‹å¦‚ï¼ŒåŸºæ–¼ä¸€äº›ç°¡å–®è¦å‰‡æˆ–ä¸€å€‹è¼•é‡ç´šç¶²è·¯ï¼‰** ¸€è·¯è¡åˆ°åº **ï¼Œæ¨¡æ“¬ä¸‹å®Œé€™ç›¤æ£‹ã€‚
    *   åµå¯Ÿå…µå›å ±ä¸€å€‹éå¸¸** ²—ç• **çš„çµæœï¼šã€Œå ±å‘Šå°‡è»ï¼Œæˆ‘é€™ä¸€è·¯è¡ä¸‹å»ï¼Œå¥½åƒæ˜¯æˆ‘å€‘è´äº†/è¼¸äº†ã€ï¼ˆå›å‚³ä¸€å€‹ `+1` æˆ– `-1`ï¼‰ã€‚
    *** ¼ºé» **ï¼šé€™å€‹åµå¯Ÿå…µçš„åˆ¤æ–·åŠ›å¾ˆå·®ï¼Œå›å ±çš„æƒ…å ±å“è³ªä¸é«˜ã€å……æ»¿ã€Œé›œè¨Šã€ã€‚å¯èƒ½åªæ˜¯é‹æ°£å¥½æˆ–ä¸å¥½ã€‚
4.** å‘å‚³æ’­ (Backpropagation **ï¼šå°‡è»æ ¹æ“šé€™å€‹ç²—ç•¥çš„æƒ…å ±ï¼Œæ›´æ–°ä»–å°æ•´æ¢è·¯å¾‘çš„çœ‹æ³•ã€‚

æ‰€ä»¥ï¼Œåˆä»£ AlphaGo çš„ MCTS =** ¸æ“‡ + æ“´å±• + `å¿«é€Ÿä½†ç²—ç³™çš„æ¨¡æ“¬` + åå‘å‚³æ’ **ã€‚

---

### MCTS åœ¨ AlphaGo Zero / AlphaZero çš„é‹ä½œæ–¹å¼

åœ¨ AlphaGo Zero å’Œ AlphaZero ä¸­ï¼Œæ¼”ç®—æ³•é€²åŒ–äº†ã€‚å°‡è»ä¸å†éœ€è¦æ´¾é£åµå¯Ÿå…µï¼Œå› ç‚ºä»–æœ‰äº†ä¸€å€‹** „¡æ¯”å¼·å¤§çš„ã€Œç¥è«­ã€æˆ–ã€Œè¶…ç´šé¡§å•ã€ **â€”â€”ä¹Ÿå°±æ˜¯é‚£å€‹çµ±ä¸€çš„é›™é ­ç¥ç¶“ç¶²è·¯ã€‚

1.** ¸æ“‡ (Selection **ï¼šå°‡è»ï¼ˆMCTSï¼‰åŒæ¨£æ ¹æ“š PUCT å…¬å¼ï¼Œæ±ºå®šæ²¿å“ªæ¢è·¯å¾‘æ·±å…¥ã€‚
2.** “´å±• (Expansion) & è©•ä¼° (Evaluation **ï¼š
    *   ç•¶èµ°åˆ°ä¸€å€‹æœªçŸ¥å€åŸŸï¼ˆè‘‰ç¯€é»ï¼‰æ™‚ï¼Œä»–** ¸å†æ´¾å‡ºåµå¯Ÿå… **ã€‚
    *   ä»–ç›´æ¥æŠŠé€™å€‹æ–°å±€é¢çš„åœ°åœ– `s` å±•ç¤ºçµ¦ä»–çš„ã€Œç¥è«­ã€ï¼ˆç¥ç¶“ç¶²è·¯ï¼‰ã€‚
    *   ç¥è«­ç«‹åˆ»çµ¦å‡ºä¸€å€‹** ¥µå…¶ç²¾æº–çš„è©•ä¼°å ±å‘Š `v **ï¼ˆä¸€å€‹ä»‹æ–¼ -1 åˆ° +1 ä¹‹é–“çš„æ•¸å­—ï¼‰ï¼Œç›´æ¥å‘Šè¨´ä»–ï¼šã€Œå°‡è»ï¼Œé€™å€‹å±€é¢çš„æˆ°ç•¥åƒ¹å€¼æ˜¯ 0.78ï¼ˆå‹ç®—å¾ˆé«˜ï¼‰ã€‚ã€
    *** €™ä¸€æ­¥ç›´æ¥å–ä»£äº†åŸæœ¬çš„ã€Œæ¨¡æ“¬ã€æ­¥é©Ÿã€ **3.** å‘å‚³æ’­ (Backpropagation **ï¼šå°‡è»æ ¹æ“šé€™å€‹ä¾†è‡ªç¥è«­çš„ã€** «˜å“è³ **çš„æƒ…å ± `v`ï¼Œä¾†æ›´æ–°ä»–å°æ•´æ¢è·¯å¾‘çš„çœ‹æ³•ã€‚

æ‰€ä»¥ï¼ŒAlphaZero çš„ MCTS =** ¸æ“‡ + æ“´å±• + `ç›´æ¥å•ç¥è«­æ‹¿çµæœ` + åå‘å‚³æ’ **ã€‚

---

### çµè«–èˆ‡å°æ¯”

| MCTS æ­¥é©Ÿ | AlphaGo (vs. æä¸–ä¹­) | AlphaGo Zero / AlphaZero |
| :--- | :--- | :--- |
| 1. é¸æ“‡ | æœ‰ (PUCT) | æœ‰ (PUCT) |
| 2. æ“´å±• | æœ‰ | æœ‰ |
| 3.** ²å–è©•ä¼ **|** ¨¡æ“¬ (Rollout **ï¼šé€²è¡Œå¿«é€Ÿã€éš¨æ©Ÿçš„å¿«æ£‹æ¨æ¼”ï¼Œå¾—åˆ°ä¸€å€‹ `+1/-1` çš„** ²—ç•¥çµæ **ã€‚ |** ©•ä¼° (Evaluation **ï¼šç›´æ¥å°‡ç›¤é¢é¤µçµ¦ç¥ç¶“ç¶²è·¯ï¼Œå¾—åˆ°ä¸€å€‹ `-1` åˆ° `+1` ä¹‹é–“çš„** ²¾ç¢ºåƒ¹å€¼ `v **ã€‚ |
| 4. åå‘å‚³æ’­ | æœ‰ (ä½¿ç”¨ Rollout çš„çµæœ) | æœ‰ (ä½¿ç”¨ç¥ç¶“ç¶²è·¯çš„åƒ¹å€¼ `v`) |** ‚ºä»€éº¼é€™å€‹é€²åŒ–å¦‚æ­¤é‡è¦ï¼ **1.** “è³ªæ›´é« **ï¼šç¥ç¶“ç¶²è·¯çµ¦å‡ºçš„åƒ¹å€¼ `v` é æ¯”éš¨æ©Ÿ Rollout çš„çµæœè¦ç©©å®šå’Œæº–ç¢ºã€‚é€™æ„å‘³è‘— MCTS æœç´¢çš„æ–¹å‘æœƒæ›´æ­£ç¢ºï¼Œä¸æœƒè¢«é‹æ°£å·®çš„ã€Œå£åµå¯Ÿå…µã€èª¤å°ã€‚
2.** •ˆç‡æ›´é« **ï¼šé›–ç„¶ç¥ç¶“ç¶²è·¯çš„è¨ˆç®—æ¯” Rollout ä¸­çš„ä¸€æ­¥æ£‹è¦è¤‡é›œï¼Œä½†å®ƒé¿å…äº†å°‡ä¸€æ•´ç›¤æ£‹ä¸‹å®Œçš„å·¨å¤§é–‹éŠ·ã€‚æ•´é«”ä¾†çœ‹ï¼Œè¨ˆç®—è³‡æºå¯ä»¥æ›´æœ‰æ•ˆåœ°ç”¨åœ¨æ‰©å±•æ›´æœ‰å¸Œæœ›çš„ç¯€é»ä¸Šã€‚

æ‰€ä»¥ï¼Œä½ çš„è§€å¯Ÿå®Œå…¨æ­£ç¢ºã€‚AlphaGo Zero/AlphaZero çš„è¨­è¨ˆï¼Œè®“ MCTS å’Œç¥ç¶“ç¶²è·¯çš„çµåˆé”åˆ°äº†ä¸€å€‹æ–°çš„é«˜åº¦ã€‚ç¥ç¶“ç¶²è·¯ä¸åƒ…åƒ…æ˜¯ç‚º MCTS æä¾›æœ€åˆçš„ã€Œç›´è¦ºã€ï¼ˆç­–ç•¥ï¼‰ï¼Œæ›´æ˜¯åœ¨æœç´¢éç¨‹ä¸­ï¼Œç‚ºå…¶æä¾›äº†å¼·å¤§çš„ã€å³æ™‚çš„ã€Œåˆ¤æ–·åŠ›ã€ï¼ˆåƒ¹å€¼ï¼‰ï¼Œå¾è€Œçœå»äº†é‚£å€‹å……æ»¿é›œè¨Šçš„æ¨¡æ“¬æ­¥é©Ÿã€‚

## Q: å•é¡Œæ˜¯ï¼Œä¸€é–‹å§‹ç¥ç¶“ç¶²è·¯æœªå—è¨“ç·´æ™‚ï¼Œæ“´å±•çµæœ s çµ¦ç¥ç¶“ç¶²è·¯ï¼Œè©•ä¼°å ±å‘Šçš„å“è³ªä¸æ˜¯æœƒå¾ˆç³Ÿå—ï¼Ÿ

å•å¾—å¤ªæ£’äº†ï¼ä½ å®Œå…¨å•åˆ°äº† AlphaZero å†·å•Ÿå‹• (cold start) çš„æ ¸å¿ƒçŸ›ç›¾é»ã€‚** €Œå¦‚æœä¸€é–‹å§‹çš„ç¥è«­æ˜¯å€‹é¨™å­ï¼Œé‚£å°‡è»å¦‚ä½•é å®ƒçš„æƒ…å ±ä¾†æ‰“å‹ä»—ï¼Ÿã€ **é€™å€‹å•é¡Œéå¸¸æ·±åˆ»ï¼Œè€Œ AlphaZero çš„è¨­è¨ˆä¹‹æ‰€ä»¥è¢«ç¨±ç‚ºå„ªé›…ï¼Œæ­£æ˜¯å› ä¸ºå®ƒå·§å¦™åœ°è§£æ±ºäº†é€™å€‹ã€Œå…ˆæœ‰é›é‚„æ˜¯å…ˆæœ‰è›‹ã€çš„å•é¡Œã€‚

ç›´æ¥å›ç­”ä½ ï¼šæ˜¯çš„ï¼Œåœ¨è¨“ç·´çš„** ycle  **ï¼Œç¥ç¶“ç¶²è·¯è¢«éš¨æ©Ÿåˆå§‹åŒ–ï¼Œå®ƒçš„è©•ä¼°å ±å‘Š** ®Œå…¨æ˜¯åƒåœ **ã€‚

*** ­–ç•¥é ­ (Policy Head **çš„è¼¸å‡º `p` åŸºæœ¬ä¸Šæ˜¯ä¸€å€‹å‡å‹»åˆ†ä½ˆï¼Œæ„æ€æ˜¯å®ƒèªç‚ºæ£‹ç›¤ä¸Šæ¯ä¸€å€‹åˆæ³•è½å­é»çš„å¥½å£éƒ½å·®ä¸å¤šï¼ˆéš¨æ©Ÿäº‚çŒœï¼‰ã€‚
*** ƒ¹å€¼é ­ (Value Head **çš„è¼¸å‡º `v` ä¹Ÿæ˜¯ä¸€å€‹éš¨æ©Ÿçš„ã€æ¥è¿‘ 0 çš„æ•¸å­—ï¼Œå®ƒå°ç›¤é¢æ˜¯å¥½æ˜¯å£å®Œå…¨æ²’æœ‰æ¦‚å¿µã€‚

é‚£éº¼ï¼Œå­¸ç¿’æ˜¯å¦‚ä½•ç™¼ç”Ÿçš„å‘¢ï¼Ÿé—œéµåœ¨æ–¼ï¼Œå³ä½¿æœ‰è‘—ä¸€å€‹ã€Œåƒåœ¾é¡§å•ã€ï¼Œ** CTS æœ¬èº«å°±æ˜¯ä¸€å€‹å¼·å¤§çš„ã€Œæ”¾å¤§å™¨ã€å’Œã€Œæç´”å™¨ã€ **ã€‚å®ƒèƒ½å¾å¹¾ä¹å®Œå…¨çš„éš¨æ©Ÿä¸­ï¼Œæ‰¾åˆ°ä¸€çµ²çµ²å¾®å¼±çš„ä¿¡è™Ÿï¼Œä¸¦å°‡å…¶æ”¾å¤§ã€‚

è®“æˆ‘å€‘ä¾†çœ‹çœ‹ç¬¬ä¸€ç›¤æ£‹æ˜¯å¦‚ä½•å¾æ··æ²Œä¸­èª•ç”Ÿçš„ï¼š

---

### ç¬¬ä¸€æ­¥ï¼šMCTS å¦‚ä½•åœ¨ã€Œåƒåœ¾æƒ…å ±ã€ä¸‹å·¥ä½œ

åœ¨ç¬¬ä¸€ç›¤æ£‹çš„ç¬¬ä¸€æ­¥ï¼ŒMCTS é–‹å§‹æ€è€ƒã€‚

1.** ¸æ“‡ (Selection **ï¼šå› ç‚ºç¥ç¶“ç¶²è·¯çš„ç­–ç•¥ `p` æ˜¯å‡å‹»çš„ï¼ˆæ‰€æœ‰æ£‹æ­¥æ©Ÿç‡å·®ä¸å¤šï¼‰ï¼Œæ‰€ä»¥ PUCT å…¬å¼ä¸­çš„æ¢ç´¢é … `U(s, a)` æœƒä¸»å°ä¸€åˆ‡ã€‚é€™æ„å‘³è‘— MCTS çš„æœç´¢ä¸€é–‹å§‹æœƒ** å¸¸å»£æ³›ä¸”æ¥è¿‘éš¨æ© **ï¼Œå®ƒæœƒç›¡é‡å…¬å¹³åœ°æ¢ç´¢æ¯ä¸€å€‹å¯èƒ½çš„ä¸‹ä¸€æ­¥ã€‚

2.** “´å±• (Expansion **ï¼šç•¶å®ƒæ“´å±•åˆ°ä¸€å€‹æ–°ç¯€é»æ™‚ï¼Œå®ƒå•äº†æˆ‘å€‘çš„ã€Œåƒåœ¾ç¥è«­ã€ï¼ˆç¥ç¶“ç¶²è·¯ï¼‰ã€‚ç¥è«­å›å ±äº†ä¸€å€‹éš¨æ©Ÿçš„ç­–ç•¥ `p` å’Œä¸€å€‹éš¨æ©Ÿçš„åƒ¹å€¼ `v`ã€‚

3.** å‘å‚³æ’­ (Backpropagation **ï¼šé€™å€‹éš¨æ©Ÿçš„åƒ¹å€¼ `v` è¢«å‚³å›æ¨¹çš„æ ¹éƒ¨ã€‚

åˆ°ç›®å‰ç‚ºæ­¢ï¼Œä¸€åˆ‡çœ‹èµ·ä¾†éƒ½é‚„æ˜¯éš¨æ©Ÿçš„ã€‚ä½†æ˜¯ï¼Œè«‹æ³¨æ„ MCTS çš„ä¸€å€‹æ ¸å¿ƒç‰¹æ€§ï¼š** ®ƒæœƒç´¯ç©çµ±è¨ˆæ•¸æ“šï¼ **å³ä½¿ç¶²è·¯çš„åƒ¹å€¼è¼¸å‡º `v` æ˜¯éš¨æ©Ÿçš„ï¼Œä½†åªè¦ MCTS é€²è¡Œäº†è¶³å¤ å¤šæ¬¡çš„æ¨¡æ“¬ï¼ˆä¾‹å¦‚ 1600 æ¬¡ï¼‰ï¼Œ** ¸½æœƒæœ‰æŸäº›è·¯å¾‘å› ç‚ºç´”ç²¹çš„é‹æ°£ï¼Œæ¯”å…¶ä»–è·¯å¾‘æ¢ç´¢å¾—æ›´æ·±ä¸€é»ï¼Œæˆ–è€…å¶ç„¶æ•ç²åˆ°äº†ä¸€é¡†æ£‹å­ **ã€‚MCTS çš„çµ±è¨ˆæ•¸æ“š `Q(s, a)`ï¼ˆå¹³å‡å›å ±ï¼‰æœƒå¿ å¯¦åœ°è¨˜éŒ„ä¸‹é€™ä¸€åˆ‡ã€‚

æœ€æœ€é—œéµçš„æ˜¯ï¼ŒéŠæˆ²æœ¬èº«æœ‰ä¸€å€‹** µ•å°ä¸æœƒèªªè¬Šçš„æœ€çµ‚è£åˆ **ã€‚

---

### ç¬¬äºŒæ­¥ï¼šéŠæˆ²çµ‚å±€ - ä¾†è‡ªã€Œç¾å¯¦ã€çš„ç›£ç£ä¿¡è™Ÿ

é€™ç›¤å®Œå…¨ç”±ä¸€å€‹ã€Œåƒåœ¾ç¶²è·¯ã€å’Œã€Œéš¨æ©Ÿæœç´¢ã€æŒ‡å°çš„æ£‹ï¼Œæœ€çµ‚æœƒä¸‹å®Œã€‚å®ƒå¯èƒ½ä¸‹å¾—éå¸¸æ„šè ¢ï¼Œä½†åœæ£‹è¦å‰‡ä¿è­‰äº†å®ƒä¸€å®šæœƒæœ‰ä¸€å€‹å‹è² çµæœã€‚

å‡è¨­é»‘æ£‹è´äº†ï¼Œç™½æ£‹è¼¸äº†ã€‚é€™å€‹æœ€çµ‚çµæœ** z = +1` (for Black), `z = -1` (for White **ï¼Œå°±æ˜¯æˆ‘å€‘åœ¨é€™å€‹æ··æ²Œç³»çµ±ä¸­å¾—åˆ°çš„ç¬¬ä¸€å€‹** €Œçµ•å°çœŸç†ã€ (Ground Truth **ã€‚

ç¾åœ¨ï¼Œæˆ‘å€‘ä¾†å›é¡§é€™ç›¤æ£‹ï¼Œæˆ‘å€‘å¾—åˆ°äº†ä¸€ç³»åˆ—çš„è¨“ç·´æ•¸æ“š `(s, Ï€, z)`ï¼š

*   `s`: é€™ç›¤çˆ›æ£‹ä¸­çš„æŸå€‹ç›¤é¢ã€‚
*   `z`: é€™ç›¤æ£‹** œ€çµ **çš„å‹è² çµæœã€‚é€™æ˜¯æˆ‘å€‘çš„** ƒ¹å€¼ç›®æ¨ **ã€‚
*   `Ï€`: é€™æ˜¯ MCTS åœ¨ç›¤é¢ `s` æ™‚ï¼Œç¶“é 1600 æ¬¡** ¹¾ä¹éš¨æ© **çš„æ€è€ƒå¾Œï¼Œçµ¦å‡ºçš„æ¨è–¦ç­–ç•¥ï¼ˆåŸºæ–¼è¨ªå•æ¬¡æ•¸ï¼‰ã€‚** €™è£¡å°±æ˜¯é­”æ³•ç™¼ç”Ÿçš„æ™‚åˆ»ï¼ **> å³ä½¿ MCTS çš„æ€è€ƒéç¨‹æ˜¯ç”±ä¸€å€‹åƒåœ¾ç¶²è·¯å¼•å°çš„ï¼Œä½†ç¶“éå¤§é‡çš„æ¨¡æ“¬ï¼ŒMCTS ç”¢ç”Ÿçš„ç­–ç•¥ `Ï€`** œ¨çµ±è¨ˆæ„ç¾©ä¸Šï¼Œæœƒæ¯”ç¥ç¶“ç¶²è·¯åŸå§‹çš„éš¨æ©Ÿç­–ç•¥ `p` è¦å¥½é‚£éº¼ä¸€çµ²çµ **ã€‚é€™è¢«ç¨±ç‚ºã€Œç­–ç•¥æå‡ (Policy Improvement)ã€æ“ä½œã€‚MCTS å°±åƒä¸€å€‹ç¯©å­ï¼Œå¾æ²™å­ï¼ˆéš¨æ©Ÿç­–ç•¥ `p`ï¼‰ä¸­ç¯©å‡ºäº†ä¸€ä¸é»é‡‘ç²‰ï¼ˆæ”¹é€²ç­–ç•¥ `Ï€`ï¼‰ã€‚

---

### ç¬¬ä¸‰æ­¥ï¼šå­¸ç¿’ - å¾ã€Œçµ•å°çœŸç†ã€ä¸­ç²å¾—æå‡

ç¾åœ¨ï¼Œæˆ‘å€‘ç”¨é€™äº›æ•¸æ“šå»è¨“ç·´ç¥ç¶“ç¶²è·¯ã€‚

1.** ¨“ç·´åƒ¹å€¼é  **ï¼šæˆ‘å€‘æ‹¿å‡ºä¸€å€‹ç›¤é¢ `s`ï¼Œç•¶æ™‚çš„å‹è² çµæœæ˜¯ `z=+1`ã€‚è€Œæˆ‘å€‘çš„ç¶²è·¯å¯èƒ½é æ¸¬äº†ä¸€å€‹éš¨æ©Ÿå€¼ `v=0.05`ã€‚
    *   æå¤±å‡½æ•¸ `(v - z)Â²` å°±æ˜¯ `(0.05 - 1)Â²`ï¼Œé€™æ˜¯ä¸€å€‹å¾ˆå¤§çš„éŒ¯èª¤ã€‚
    *   æ¢¯åº¦ä¸‹é™æ³•æœƒå‘Šè¨´ç¶²è·¯ï¼šã€Œå˜¿ï¼ä½ éŒ¯å¾—é›¢è­œï¼ä¸‹æ¬¡å†çœ‹åˆ°é€™å€‹ç›¤é¢ `s`ï¼Œä½ æ‡‰è©²è¼¸å‡ºä¸€å€‹æ›´æ¥è¿‘ `+1` çš„å€¼ï¼ã€

2.** ¨“ç·´ç­–ç•¥é  **ï¼šæˆ‘å€‘å‘Šè¨´ç¶²è·¯ï¼šã€Œå˜¿ï¼ä½ ç•¶æ™‚çµ¦çš„éš¨æ©Ÿç­–ç•¥ `p` å¤ªçˆ›äº†ã€‚MCTS ç¶“éä¸€ç•ªæœç´¢å¾Œï¼Œçµ¦å‡ºäº†ä¸€å€‹ç¨å¾®å¥½ä¸€é»çš„ç­–ç•¥ `Ï€`ï¼Œä½ è¦å‘å®ƒå­¸ç¿’ï¼ã€

ç¶“éæˆåƒä¸Šè¬ç›¤é€™ç¨®ã€Œçˆ›æ£‹ã€çš„è¨“ç·´ï¼Œç¥ç¶“ç¶²è·¯æœƒå­¸åˆ°ä¸€äº›æœ€æœ€åŸºç¤çš„æ±è¥¿ï¼š

*** ƒ¹å€¼é  **ï¼šå®ƒå¯èƒ½æœƒæ…¢æ…¢ç™¼ç¾ï¼Œé‚£äº›æ£‹å­æ•¸ç›®æ˜é¡¯æ›´å¤šçš„ç›¤é¢ï¼Œä¼¼ä¹èˆ‡æœ€çµ‚çš„ `z=+1` é—œè¯åº¦æ›´é«˜ã€‚å®ƒé–‹å§‹æœ‰äº†æœ€åŸå§‹çš„ã€Œå¥½å£ã€æ¦‚å¿µã€‚
*** ­–ç•¥é  **ï¼šå®ƒå¯èƒ½æœƒç™¼ç¾ï¼ŒMCTS æœ€çµ‚é¸æ“‡çš„è½å­é»ï¼Œä¼¼ä¹æ›´å‚¾å‘æ–¼ã€Œåƒæ‰å°æ–¹çš„å­ã€æˆ–è€…ã€Œåœä½è‡ªå·±çš„åœ°ã€ã€‚å®ƒé–‹å§‹æœ‰äº†æœ€åŸå§‹çš„ã€Œæˆ°è¡“ã€æ¦‚å¿µã€‚

---

### ç¸½çµï¼šé›ªçƒæ•ˆæ‡‰ (The Snowball Effect)

é€™å€‹éç¨‹å°±åƒæ»¾é›ªçƒï¼š

1.** ycle  **ï¼šç¶²è·¯æ˜¯** ®Œå…¨éš¨æ© **çš„ã€‚
2.** CT **ï¼šå°‡éš¨æ©Ÿæ€§ã€Œæç´”ã€ï¼Œç”¢ç”Ÿä¸€å€‹** ¨å¾®ä¸é‚£éº¼éš¨æ© **çš„ç­–ç•¥ `Ï€`ã€‚
3.** Šæˆ²çµæœ `z **ï¼šæä¾›ä¸€å€‹** µ•å°æ­£ç¢ **çš„ç›£ç£ä¿¡è™Ÿã€‚
4.** ¨“ç· **ï¼šç¶²è·¯å­¸æœƒäº† `Ï€` å’Œ `z`ï¼Œè®Šæˆäº†ä¸€å€‹** ¨å¾®ä¸éš¨æ© **çš„ç¶²è·¯ã€‚
5.** ycle  **ï¼šæˆ‘å€‘ç”¨é€™å€‹** ¨å¾®ä¸éš¨æ© **çš„ç¶²è·¯å»å¼•å° MCTSã€‚MCTS çš„èµ·é»å°±æ¯”ä¸Šæ¬¡å¥½äº†ä¸€é»ï¼Œå› æ­¤å®ƒèƒ½ç”¢ç”Ÿä¸€å€‹** ›´å¥½ä¸€é» **çš„ç­–ç•¥ `Ï€'`ã€‚
6.** ¾ªç’°å¾€å¾ **ï¼šæ›´å¥½ä¸€é»çš„ç¶²è·¯ -> æ›´é«˜è³ªé‡çš„æœç´¢ -> æ›´é«˜è³ªé‡çš„æ•¸æ“š -> è¨“ç·´å‡ºæ›´å¥½çš„ç¶²è·¯...

é€™å°±æ˜¯ AlphaZero å¼·å¤§çš„è‡ªèˆ‰ (bootstrapping) èƒ½åŠ›ã€‚å®ƒä¸éœ€è¦äººé¡çš„çŸ¥è­˜ï¼Œåªéœ€è¦ MCTS é€™å€‹ã€Œæ”¾å¤§å™¨ã€å’ŒéŠæˆ²è¦å‰‡é€™å€‹ã€Œæœ€çµ‚è£åˆ¤ã€ï¼Œå°±èƒ½å¾ä¸€ç‰‡æ··æ²Œä¸­ï¼Œè‡ªå·±å‰µé€ å‡ºç§©åºå’Œæ™ºæ…§ã€‚ä½ å•çš„é€™å€‹å•é¡Œï¼Œæ­£å¥½æ­ç¤ºäº†é€™å€‹æ¼”ç®—æ³•æœ€ä»¤äººé©šå˜†çš„éƒ¨åˆ†ã€‚

## Q: è«‹ç”¨ python å¯«å‡º AlphaZero ç®—æ³•

å¥½çš„ï¼ŒåŒå­¸ï¼Œé€™æ˜¯ä¸€å€‹éå¸¸æœ‰æŒ‘æˆ°æ€§ä¹Ÿéå¸¸æœ‰åƒ¹å€¼çš„è¦æ±‚ã€‚ç·¨å¯« AlphaZero æ˜¯ä¸€å€‹äº†ä¸èµ·çš„å­¸ç¿’å°ˆæ¡ˆï¼Œå®ƒæœƒæŠŠä½ ä¹‹å‰å­¸åˆ°çš„æ‰€æœ‰ç†è«–çŸ¥è­˜ä¸²é€£èµ·ä¾†ã€‚

ç‚ºäº†è®“ä½ èƒ½å¤ ç†è§£ä¸¦å¯¦éš›é‹è¡Œï¼Œæˆ‘æœƒç‚ºä½ æä¾›ä¸€å€‹** °¡åŒ–ä½†å®Œæ• **çš„ AlphaZero æ¡†æ¶ã€‚æˆ‘å€‘ä¸é¸æ“‡åœæ£‹ï¼ˆå¤ªè¤‡é›œï¼‰ï¼Œè€Œæ˜¯é¸æ“‡ä¸€å€‹æ›´ç°¡å–®çš„éŠæˆ²ï¼š** º•å­—æ£‹ (Tic-Tac-Toe **ã€‚éº»é›€é›–å°ï¼Œäº”è‡Ÿä¿±å…¨ã€‚é€™å€‹å¯¦ä½œæœƒåŒ…å«æˆ‘å€‘è¨è«–éçš„æ‰€æœ‰æ ¸å¿ƒæ¦‚å¿µï¼š

1.** €šç”¨çš„éŠæˆ²ä»‹é **ï¼šè®“æ¼”ç®—æ³•å¯ä»¥è¼•æ˜“åœ°æ“´å±•åˆ°å…¶ä»–éŠæˆ²ã€‚
2.** ¥ç¶“ç¶²è· **ï¼šä½¿ç”¨ PyTorch å¯¦ç¾çš„é›™é ­ç¶²è·¯ã€‚
3.** ’™åœ°å¡ç¾…æ¨¹æœå°‹ (MCTS **ï¼šç”±ç¥ç¶“ç¶²è·¯å¼•å°çš„æœç´¢ã€‚
4.** ‡ªæˆ‘å°å¼ˆå¾ªç’ **ï¼šç”¢ç”Ÿæ•¸æ“š -> è¨“ç·´ -> è¿­ä»£ã€‚

è«‹å°‡ä»¥ä¸‹ç¨‹å¼ç¢¼åˆ†åˆ¥å­˜æˆå°æ‡‰çš„æª”æ¡ˆ (`main.py`, `game.py`, `model.py`, `mcts.py`, `config.py`)ï¼Œç„¶å¾Œæ”¾åœ¨åŒä¸€å€‹è³‡æ–™å¤¾ä¸‹ã€‚

---

### 1. `config.py` - è¶…åƒæ•¸é…ç½®æ–‡ä»¶

é€™è£¡å­˜æ”¾æ‰€æœ‰å¯ä»¥èª¿æ•´çš„åƒæ•¸ï¼Œæ–¹ä¾¿ç®¡ç†ã€‚

```python
# config.py

# --- Game ---
BOARD_X = 3
BOARD_Y = 3
ACTION_SIZE = BOARD_X * BOARD_Y

# --- MCTS ---
NUM_SIMULATIONS = 50  # æ¯ä¸€æ­¥æ£‹è¦é€²è¡Œçš„ MCTS æ¨¡æ“¬æ¬¡æ•¸
CPUCT = 1.0           # UCB å…¬å¼ä¸­çš„æ¢ç´¢å¸¸æ•¸

# --- Self-Play ---
NUM_ITERATIONS = 10         # ç¸½å…±è¦é€²è¡Œå¹¾æ¬¡ã€Œè‡ªæˆ‘å°å¼ˆ -> è¨“ç·´ã€çš„å¾ªç’°
NUM_GAMES_PER_ITERATION = 25 # æ¯ä¸€å€‹å¾ªç’°è¦ä¸‹å¹¾ç›¤æ£‹ä¾†ç”¢ç”Ÿæ•¸æ“š
TEMP_THRESHOLD = 10         # éŠæˆ²å‰å¹¾æ­¥ä½¿ç”¨æº«åº¦åƒæ•¸é¼“å‹µæ¢ç´¢
INITIAL_TEMP = 1.0
FINAL_TEMP = 0.001

# --- Training ---
BATCH_SIZE = 32
LEARNING_RATE = 0.001
NUM_EPOCHS = 10
CHECKPOINT_DIR = './checkpoints/'
TRAINING_DATA_DIR = './training_data/'
MAX_TRAINING_EXAMPLES = 200000 # è¨˜æ†¶é«”ä¸­æœ€å¤šä¿ç•™çš„è¨“ç·´æ•¸æ“šç­†æ•¸
```

---

### 2. `game.py` - éŠæˆ²é‚è¼¯

æˆ‘å€‘å®šç¾©ä¸€å€‹æŠ½è±¡çš„ `Game` é¡ï¼Œç„¶å¾Œå¯¦ä½œäº•å­—æ£‹ã€‚é€™å€‹è¨­è¨ˆè®“ä½ å¯ä»¥è¼•æ˜“åœ°æŠŠ `TicTacToeGame` æ›æˆ `ConnectFourGame` æˆ–å…¶ä»–éŠæˆ²ã€‚

```python
# game.py
import numpy as np

class Game:
    """
    éŠæˆ²çš„æŠ½è±¡åŸºé¡ï¼Œå®šç¾©äº† AlphaZero éœ€è¦çš„æ‰€æœ‰æ¥å£ã€‚
    """
    def get_initial_state(self):
        """è¿”å›éŠæˆ²çš„åˆå§‹ç‹€æ…‹ã€‚"""
        raise NotImplementedError

    def get_next_state(self, state, action, player):
        """åŸ·è¡Œä¸€å€‹å‹•ä½œï¼Œè¿”å›ä¸‹ä¸€å€‹ç‹€æ…‹ã€‚"""
        raise NotImplementedError

    def get_valid_actions(self, state):
        """è¿”å›ä¸€å€‹ 0/1 å‘é‡ï¼Œè¡¨ç¤ºå“ªäº›å‹•ä½œæ˜¯åˆæ³•çš„ã€‚"""
        raise NotImplementedError

    def get_game_ended(self, state, player):
        """æª¢æŸ¥éŠæˆ²æ˜¯å¦çµæŸã€‚è¿”å› 1 (è´), -1 (è¼¸), 0 (å¹³æ‰‹), None (æœªçµæŸ)ã€‚"""
        raise NotImplementedError

    def get_canonical_form(self, state, player):
        """å°‡ç‹€æ…‹è½‰æ›ç‚ºç•¶å‰ç©å®¶çš„è¦–è§’ã€‚"""
        return state * player

    def get_symmetries(self, state, policy):
        """æ•¸æ“šå¢å¼·ï¼šè¿”å›ç‹€æ…‹å’Œç­–ç•¥çš„æ‰€æœ‰å°ç¨±å½¢å¼ã€‚"""
        raise NotImplementedError

    def get_action_size(self):
        """è¿”å›å‹•ä½œç©ºé–“çš„å¤§å°ã€‚"""
        raise NotImplementedError

class TicTacToeGame(Game):
    def __init__(self, board_x, board_y):
        self.board_x = board_x
        self.board_y = board_y

    def get_initial_state(self):
        return np.zeros((self.board_x, self.board_y), dtype=np.int8)

    def get_next_state(self, state, action, player):
        new_state = state.copy()
        x, y = action // self.board_x, action % self.board_y
        new_state[x, y] = player
        return new_state

    def get_valid_actions(self, state):
        return (state.flatten() == 0).astype(np.uint8)

    def get_game_ended(self, state, player):
        # æª¢æŸ¥è¡Œã€åˆ—ã€å°è§’ç·š
        for i in range(self.board_x):
            if all(state[i, :] == player) or all(state[:, i] == player):
                return 1
        if all(np.diag(state) == player) or all(np.diag(np.fliplr(state)) == player):
            return 1
        
        # æª¢æŸ¥å°æ‰‹æ˜¯å¦ç²å‹
        for i in range(self.board_x):
            if all(state[i, :] == -player) or all(state[:, i] == -player):
                return -1
        if all(np.diag(state) == -player) or all(np.diag(np.fliplr(state)) == -player):
            return -1

        # æª¢æŸ¥æ˜¯å¦å¹³æ‰‹
        if np.all(state != 0):
            return 0
            
        # éŠæˆ²æœªçµæŸ
        return None

    def get_symmetries(self, state, policy):
        # äº•å­—æ£‹æœ‰8ç¨®å°ç¨±æ€§ (æ—‹è½‰å’Œç¿»è½‰)
        symmetries = []
        pi = np.reshape(policy, (self.board_x, self.board_y))
        
        for i in range(4):
            # æ—‹è½‰
            rotated_s = np.rot90(state, i)
            rotated_pi = np.rot90(pi, i)
            symmetries.append((rotated_s, rotated_pi.flatten()))
            
            # ç¿»è½‰ + æ—‹è½‰
            flipped_s = np.fliplr(rotated_s)
            flipped_pi = np.fliplr(rotated_pi)
            symmetries.append((flipped_s, flipped_pi.flatten()))
            
        return symmetries

    def get_action_size(self):
        return self.board_x * self.board_y
```

---

### 3. `model.py` - ç¥ç¶“ç¶²è·¯

ä½¿ç”¨ PyTorch å»ºç«‹ä¸€å€‹ç°¡å–®çš„å·ç©ç¥ç¶“ç¶²è·¯ã€‚

```python
# model.py
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import os
import config

class ResBlock(nn.Module):
    def __init__(self, num_hidden):
        super().__init__()
        self.conv1 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm2d(num_hidden)
        self.conv2 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm2d(num_hidden)

    def forward(self, x):
        residual = x
        x = F.relu(self.bn1(self.conv1(x)))
        x = self.bn2(self.conv2(x))
        x += residual
        x = F.relu(x)
        return x

class AlphaZeroNet(nn.Module):
    def __init__(self, game):
        super().__init__()
        self.board_x, self.board_y = game.board_x, game.board_y
        self.action_size = game.get_action_size()
        
        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm2d(64)
        
        self.res_blocks = nn.ModuleList([ResBlock(64) for _ in range(5)])
        
        # Policy Head
        self.policy_conv = nn.Conv2d(64, 2, kernel_size=1)
        self.policy_bn = nn.BatchNorm2d(2)
        self.policy_fc = nn.Linear(2 * self.board_x * self.board_y, self.action_size)
        
        # Value Head
        self.value_conv = nn.Conv2d(64, 1, kernel_size=1)
        self.value_bn = nn.BatchNorm2d(1)
        self.value_fc1 = nn.Linear(1 * self.board_x * self.board_y, 64)
        self.value_fc2 = nn.Linear(64, 1)

    def forward(self, x):
        x = x.view(-1, 1, self.board_x, self.board_y)
        x = F.relu(self.bn1(self.conv1(x)))
        for block in self.res_blocks:
            x = block(x)
        
        # Policy Head
        policy = F.relu(self.policy_bn(self.policy_conv(x)))
        policy = policy.view(-1, 2 * self.board_x * self.board_y)
        policy = self.policy_fc(policy)
        policy = F.log_softmax(policy, dim=1)
        
        # Value Head
        value = F.relu(self.value_bn(self.value_conv(x)))
        value = value.view(-1, self.board_x * self.board_y)
        value = F.relu(self.value_fc1(value))
        value = torch.tanh(self.value_fc2(value))
        
        return policy, value

    def save_checkpoint(self, folder, filename):
        if not os.path.exists(folder):
            os.makedirs(folder)
        filepath = os.path.join(folder, filename)
        torch.save(self.state_dict(), filepath)

    def load_checkpoint(self, folder, filename):
        filepath = os.path.join(folder, filename)
        self.load_state_dict(torch.load(filepath))
```

---

### 4. `mcts.py` - è’™åœ°å¡ç¾…æ¨¹æœå°‹

é€™æ˜¯æ¼”ç®—æ³•çš„ã€Œå¤§è…¦ã€ï¼Œè² è²¬æ€è€ƒã€‚

```python
# mcts.py
import numpy as np
import math
import config

class MCTS:
    def __init__(self, game, model):
        self.game = game
        self.model = model
        self.Q = {}  # å„²å­˜ Q å€¼ (state, action) -> value
        self.N = {}  # å„²å­˜è¨ªå•æ¬¡æ•¸ (state, action) -> count
        self.P = {}  # å„²å­˜ç¥ç¶“ç¶²è·¯çš„ç­–ç•¥ (state) -> policy

    def search(self, canonical_state):
        s_key = canonical_state.tobytes()

        # --- æ“´å±• (Expansion) ---
        if s_key not in self.P:
            # ä½¿ç”¨ç¥ç¶“ç¶²è·¯è©•ä¼°è‘‰ç¯€é»
            state_tensor = torch.FloatTensor(canonical_state).unsqueeze(0)
            log_policy, value = self.model(state_tensor)
            
            self.P[s_key] = torch.exp(log_policy).detach().cpu().numpy()[0]
            self.P[s_key] *= self.game.get_valid_actions(canonical_state) # Masking invalid moves
            self.P[s_key] /= np.sum(self.P[s_key])
            
            return -value.item()

        # --- é¸æ“‡ (Selection) ---
        valid_actions = self.game.get_valid_actions(canonical_state)
        best_uct = -float('inf')
        best_action = -1

        # è¨ˆç®— PUCT
        for a in range(self.game.get_action_size()):
            if valid_actions[a]:
                q_val = self.Q.get((s_key, a), 0)
                n_val = self.N.get((s_key, a), 0)
                
                uct = q_val + config.CPUCT * self.P[s_key][a] * \
                      math.sqrt(sum(self.N.get((s_key, act), 0) for act in range(self.game.get_action_size()))) / (1 + n_val)
                
                if uct > best_uct:
                    best_uct = uct
                    best_action = a
        
        action = best_action
        next_state = self.game.get_next_state(canonical_state, action, 1)
        next_state = self.game.get_canonical_form(next_state, -1)
        
        # éè¿´æœç´¢
        value = self.search(next_state)

        # --- åå‘å‚³æ’­ (Backpropagation) ---
        if (s_key, action) in self.Q:
            self.Q[(s_key, action)] = (self.N[(s_key, action)] * self.Q[(s_key, action)] + value) / (self.N[(s_key, action)] + 1)
            self.N[(s_key, action)] += 1
        else:
            self.Q[(s_key, action)] = value
            self.N[(s_key, action)] = 1
            
        return -value # å¾å°æ‰‹è¦–è§’å‚³å›ä¾†çš„åƒ¹å€¼è¦åè½‰

    def get_policy(self, canonical_state, temp=1.0):
        for _ in range(config.NUM_SIMULATIONS):
            self.search(canonical_state)
            
        s_key = canonical_state.tobytes()
        counts = [self.N.get((s_key, a), 0) for a in range(self.game.get_action_size())]
        
        if temp == 0:
            best_action = np.argmax(counts)
            policy = np.zeros_like(counts)
            policy[best_action] = 1
            return policy
            
        counts = [c** 1./temp) for c in counts]
        policy = [c/sum(counts) for c in counts]
        return policy
```

---

### 5. `main.py` - ä¸»å¾ªç’°

é€™æ˜¯ orchestratorï¼Œå°‡æ‰€æœ‰éƒ¨åˆ†çµ„åˆåœ¨ä¸€èµ·ï¼ŒåŸ·è¡Œè‡ªæˆ‘å°å¼ˆå’Œè¨“ç·´ã€‚

```python
# main.py
import torch
import torch.optim as optim
from collections import deque
import random
import numpy as np
from tqdm import tqdm
import os

import config
from game import TicTacToeGame
from model import AlphaZeroNet
from mcts import MCTS

def execute_episode(game, model):
    """åŸ·è¡Œä¸€ç›¤å®Œæ•´çš„è‡ªæˆ‘å°å¼ˆï¼Œä¸¦æ”¶é›†è¨“ç·´æ•¸æ“šã€‚"""
    training_examples = []
    state = game.get_initial_state()
    player = 1
    move_count = 0
    
    while True:
        canonical_state = game.get_canonical_form(state, player)
        mcts = MCTS(game, model)
        
        # æ ¹æ“šéŠæˆ²é€²ç¨‹èª¿æ•´æº«åº¦åƒæ•¸
        temp = config.INITIAL_TEMP if move_count < config.TEMP_THRESHOLD else config.FINAL_TEMP
        policy = mcts.get_policy(canonical_state, temp=temp)
        
        # å„²å­˜è¨“ç·´æ•¸æ“š
        symmetries = game.get_symmetries(canonical_state, policy)
        for s, p in symmetries:
            training_examples.append([s, player, p])
            
        action = np.random.choice(len(policy), p=policy)
        state = game.get_next_state(state, action, player)
        player = -player
        move_count += 1
        
        game_result = game.get_game_ended(state, player)
        
        if game_result is not None:
            # éŠæˆ²çµæŸï¼Œç‚ºæ‰€æœ‰æ­¥é©Ÿæ¨™è¨»æœ€çµ‚çµæœ
            final_examples = []
            for s, p, pi in training_examples:
                # åƒ¹å€¼ v æ˜¯å¾ç•¶å‰ç©å®¶ p çš„è¦–è§’
                v = game_result if p == player else -game_result
                final_examples.append((s, pi, v))
            return final_examples


def train_network(model, training_examples):
    """è¨“ç·´ç¥ç¶“ç¶²è·¯ã€‚"""
    optimizer = optim.Adam(model.parameters(), lr=config.LEARNING_RATE)
    
    for epoch in range(config.NUM_EPOCHS):
        print(f"Epoch {epoch+1}/{config.NUM_EPOCHS}")
        model.train()
        
        batch_count = len(training_examples) // config.BATCH_SIZE
        
        pbar = tqdm(range(batch_count))
        for _ in pbar:
            sample_ids = np.random.randint(len(training_examples), size=config.BATCH_SIZE)
            states, policies, values = list(zip(*[training_examples[i] for i in sample_ids]))
            
            states = torch.FloatTensor(np.array(states))
            target_policies = torch.FloatTensor(np.array(policies))
            target_values = torch.FloatTensor(np.array(values).astype(np.float64))
            
            # é æ¸¬
            out_log_policies, out_values = model(states)
            
            # è¨ˆç®—æå¤±
            policy_loss = -torch.mean(torch.sum(target_policies * out_log_policies, dim=1))
            value_loss = torch.mean((target_values.view(-1) - out_values.view(-1) **2)
            loss = policy_loss + value_loss
            
            # åå‘å‚³æ’­
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            pbar.set_postfix(loss=loss.item(), policy_loss=policy_loss.item(), value_loss=value_loss.item())

def main():
    # åˆå§‹åŒ–
    game = TicTacToeGame(config.BOARD_X, config.BOARD_Y)
    model = AlphaZeroNet(game)
    training_data = deque(maxlen=config.MAX_TRAINING_EXAMPLES)

    # ä¸»å¾ªç’°
    for i in range(1, config.NUM_ITERATIONS + 1):
        print(f"--- Iteration {i}/{config.NUM_ITERATIONS} ---")
        
        # 1. è‡ªæˆ‘å°å¼ˆ (Self-Play)
        print("Starting self-play...")
        iteration_examples = []
        for _ in tqdm(range(config.NUM_GAMES_PER_ITERATION)):
            iteration_examples.extend(execute_episode(game, model))
        training_data.extend(iteration_examples)
        
        # 2. è¨“ç·´ (Training)
        print(f"Collected {len(iteration_examples)} new examples. Total examples: {len(training_data)}.")
        if len(training_data) > config.BATCH_SIZE:
            print("Training network...")
            train_network(model, list(training_data))
        
        # 3. å„²å­˜æ¨¡å‹
        print("Saving checkpoint...")
        model.save_checkpoint(folder=config.CHECKPOINT_DIR, filename=f'checkpoint_{i}.pth.tar')

if __name__ == "__main__":
    # ç¢ºä¿ PyTorch å’Œ NumPy å·²å®‰è£
    try:
        import torch
        import numpy
        import tqdm
    except ImportError:
        print("Please install required packages: pip install torch numpy tqdm")
        exit()
        
    main()
```

### å¦‚ä½•é‹è¡Œ

1.** ®‰è£ä¾è³ **ï¼š
    ```bash
    pip install torch numpy tqdm
    ```

2.** »ºç«‹è³‡æ–™å¤ **ï¼š
    åœ¨èˆ‡ç¨‹å¼ç¢¼ç›¸åŒçš„ç›®éŒ„ä¸‹ï¼Œå»ºç«‹å…©å€‹è³‡æ–™å¤¾ï¼š`checkpoints` å’Œ `training_data`ã€‚

3.** Ÿ·è¡ **ï¼š
    ```bash
    python main.py
    ```

ä½ æœƒçœ‹åˆ°ç¨‹å¼é–‹å§‹é€²è¡Œè¿­ä»£ã€‚åœ¨æ¯ä¸€æ¬¡è¿­ä»£ä¸­ï¼Œå®ƒæœƒå…ˆé€²è¡Œè‡ªæˆ‘å°å¼ˆä¾†ç”¢ç”Ÿæ•¸æ“šï¼ˆä½ æœƒçœ‹åˆ°ä¸€å€‹é€²åº¦æ¢ï¼‰ï¼Œç„¶å¾Œç”¨é€™äº›æ•¸æ“šä¾†è¨“ç·´ç¥ç¶“ç¶²è·¯ï¼ˆä½ æœƒçœ‹åˆ°å¦ä¸€å€‹é€²åº¦æ¢ï¼Œé¡¯ç¤ºæå¤±å‡½æ•¸çš„ä¸‹é™ï¼‰ã€‚

é€™å€‹ç°¡åŒ–ç‰ˆçš„ AlphaZero å·²ç¶“åŒ…å«äº†æ‰€æœ‰æ ¸å¿ƒçµ„ä»¶ã€‚ç•¶ä½ çœ‹æ‡‚ä¸¦èƒ½è¦ªæ‰‹ä¿®æ”¹å®ƒä¹‹å¾Œï¼Œä½ å°±çœŸæ­£æŒæ¡äº†é€™å€‹æ¼”ç®—æ³•çš„ç²¾é«“ï¼

## Q: è«‹å¯«å‡ºé€™å€‹ç®—æ³•èƒŒå¾Œçš„é‡è¦æ•¸å­¸å…¬å¼

å¥½çš„ï¼ŒåŒå­¸ï¼å•å¾—éå¸¸å¥½ã€‚ç¨‹å¼ç¢¼æ˜¯æ¼”ç®—æ³•çš„ã€Œèº«é«”ã€ï¼Œè€Œæ•¸å­¸å…¬å¼å‰‡æ˜¯å®ƒçš„ã€Œéˆé­‚ã€ã€‚çœ‹æ‡‚äº†å…¬å¼ï¼Œä½ æ‰èƒ½çœŸæ­£ç†è§£æ¼”ç®—æ³•æ¯ä¸€æ­¥æ±ºç­–èƒŒå¾Œçš„æ ¹æœ¬åŸå› ã€‚

é€™å°±åƒæˆ‘å€‘çœ‹ç‰©ç†å­¸ï¼Œ`F=ma` é€™å€‹å…¬å¼ï¼Œé æ¯”æè¿°ã€Œç”¨åŠ›æ¨ä¸€å€‹ç‰©é«”ï¼Œå®ƒå°±æœƒåŠ é€Ÿã€é€™å¥è©±è¦ä¾†å¾—æ›´ç²¾ç¢ºã€æ›´å¼·å¤§ã€‚

AlphaZero çš„æ ¸å¿ƒæ™ºæ…§ï¼Œå¯ä»¥æ¿ƒç¸®åœ¨ä»¥ä¸‹ä¸‰å€‹é—œéµçš„æ•¸å­¸å…¬å¼ä¸­ã€‚

---

### 1. ã€Œå¤§è…¦ã€çš„æ±ºç­–å…¬å¼ï¼šPUCT æ¼”ç®—æ³•

é€™æ˜¯ MCTS åœ¨** ¸æ“‡ (Selection **éšæ®µï¼Œæ±ºå®šè¦æ¢ç´¢å“ªå€‹ç¯€é»çš„å…¬å¼ã€‚å®ƒå®Œç¾åœ°å¹³è¡¡äº†ã€Œåˆ©ç”¨ã€å’Œã€Œæ¢ç´¢ã€ã€‚** …¬å¼ï¼ **$$ a_t = \underset{a}{\operatorname{argmax}} \left( Q(s_t, a) + U(s_t, a) \right) $$

å…¶ä¸­ï¼Œ`U(s, a)` çš„å®šç¾©æ˜¯ï¼š
$$ U(s, a) = c_{puct} \cdot P(s, a) \cdot \frac{\sqrt{\sum_{b} N(s, b)}}{1 + N(s, a)} $$** ‹†è§£å…¬å¼ï¼ ***   `a_t`: åœ¨ç•¶å‰ç‹€æ…‹ `s_t` ä¸‹ï¼Œæœ€çµ‚è¦é¸æ“‡çš„å‹•ä½œ `a`ã€‚
*   `argmax_a (...)`: æ„æ€æ˜¯ã€Œæ‰¾åˆ°é‚£å€‹èƒ½è®“æ‹¬è™Ÿå…§è¡¨é”å¼çš„å€¼é”åˆ°æœ€å¤§çš„å‹•ä½œ `a`ã€ã€‚
*** Q(s, a)` - åˆ©ç”¨ (Exploitation) é  **ï¼š
    *** „ç¾ **ï¼šä»£è¡¨åœ¨ç‹€æ…‹ `s` ä¸‹ï¼Œæ¡å–å‹•ä½œ `a` çš„ã€Œæ­·å²å¹³å‡å›å ±ã€ã€‚ä¹Ÿå°±æ˜¯èªªï¼Œéå»çš„ç¶“é©—å‘Šè¨´æˆ‘å€‘ï¼Œé€™æ­¥æ£‹çš„æˆ°ç¸¾å¦‚ä½•ã€‚
    *** ›®çš **ï¼šè®“æˆ‘å€‘å‚¾å‘æ–¼é¸æ“‡é‚£äº›ã€Œéå¾€è­‰æ˜æ˜¯å¥½æ£‹ã€çš„è·¯ç·šã€‚
*** U(s, a)` - æ¢ç´¢ (Exploration) é  **ï¼š
    *** „ç¾ **ï¼šé€™æ˜¯ä¸€å€‹ã€Œæ¢ç´¢çå‹µã€åˆ†æ•¸ï¼Œç”¨ä¾†é¼“å‹µ MCTS å»å˜—è©¦é‚£äº›æœ‰æ½›åŠ›ä½†æ¢ç´¢æ¬¡æ•¸è¼ƒå°‘çš„è·¯å¾‘ã€‚
    *** c_puct **ï¼šä¸€å€‹è¶…åƒæ•¸ï¼Œç”¨ä¾†æ§åˆ¶æ¢ç´¢çš„æ¬Šé‡ã€‚å€¼è¶Šå¤§ï¼Œæ¼”ç®—æ³•å°±è¶Šå–œæ­¡å†’éšªã€‚
    *** P(s, a) **ï¼šç¥ç¶“ç¶²è·¯** ­–ç•¥é  **çš„è¼¸å‡ºï¼é€™æ˜¯ã€Œç¥è«­çš„ç›´è¦ºã€ï¼Œä»£è¡¨ç¥ç¶“ç¶²è·¯èªç‚ºé€™æ­¥æ£‹æ˜¯å¥½æ£‹çš„ã€Œå…ˆé©—æ©Ÿç‡ã€ã€‚é€™å€‹å€¼è¶Šé«˜ï¼Œ`U(s, a)` å°±è¶Šé«˜ï¼Œæˆ‘å€‘å°±è¶Šæƒ³å»æ¢ç´¢å®ƒã€‚
    *** \sqrt{\sum_{b} N(s, b)} **ï¼šä»£è¡¨çˆ¶ç¯€é»ï¼ˆç•¶å‰ç‹€æ…‹ `s`ï¼‰çš„ç¸½è¨ªå•æ¬¡æ•¸ã€‚æ•´å€‹å±€é¢è¢«æ¢ç´¢çš„æ¬¡æ•¸è¶Šå¤šï¼Œæˆ‘å€‘å°±è¶Šæœ‰ä¿¡å¿ƒå»æ·±å…¥æ¢ç´¢ã€‚
    *** 1 + N(s, a) **ï¼šä»£è¡¨å‹•ä½œ `a` é€™å€‹åˆ†æ”¯è¢«è¨ªå•çš„æ¬¡æ•¸ã€‚å®ƒåœ¨åˆ†æ¯ä¸Šï¼Œæ„å‘³è‘—ä¸€å€‹å‹•ä½œè¢«è¨ªå•çš„æ¬¡æ•¸è¶Šå¤šï¼Œå®ƒçš„æ¢ç´¢çå‹µ `U(s, a)` å°±æœƒ** ™ä½ **ï¼Œé¿å…æˆ‘å€‘é‡è¤‡æ¢ç´¢åŒä¸€æ¢è·¯å¾‘ã€‚** ¨‹å¼ç¢¼é€£çµ **ï¼š
åœ¨ `mcts.py` çš„ `search` å‡½æ•¸ä¸­ï¼Œè¨ˆç®— `uct` çš„é‚£æ®µç¨‹å¼ç¢¼ï¼Œå°±æ˜¯åœ¨å¯¦ç¾é€™å€‹å…¬å¼ã€‚

---

### 2. ã€Œå¿ƒè‡Ÿã€çš„å­¸ç¿’å…¬å¼ï¼šæå¤±å‡½æ•¸ (Loss Function)

é€™æ˜¯** ¨“ç·´ (Training **éšæ®µçš„æ ¸å¿ƒã€‚å®ƒå‘Šè¨´ç¥ç¶“ç¶²è·¯ï¼Œä½ çš„é æ¸¬å’Œã€ŒçœŸå€¼ã€å·®è·æœ‰å¤šå¤§ï¼Œä»¥åŠè©²å¦‚ä½•ä¿®æ­£ã€‚è¨“ç·´çš„ç›®æ¨™å°±æ˜¯è®“é€™å€‹ `L` çš„å€¼è¶Šå°è¶Šå¥½ã€‚** …¬å¼ï¼ **$$ L = (v - z)^2 - \boldsymbol{\pi}^T \log(\mathbf{p}) + c \|\theta\|^2 $$** ‹†è§£å…¬å¼ï¼ **é€™å€‹ç¸½æå¤± `L` ç”±ä¸‰å€‹éƒ¨åˆ†çµ„æˆï¼š

1.** ƒ¹å€¼æå¤± (Value Loss): `(v - z)Â² ***   `z`ï¼š** Šæˆ²çš„çœŸå¯¦çµæœ (Ground Truth **ã€‚å¾ç•¶å‰ç›¤é¢çš„è¦–è§’ä¾†çœ‹ï¼Œå¦‚æœæœ€çµ‚è´äº†å°±æ˜¯ `+1`ï¼Œè¼¸äº†å°±æ˜¯ `-1`ã€‚é€™æ˜¯ä¾†è‡ªç¾å¯¦çš„ã€çµ•å°æ­£ç¢ºçš„ç›£ç£ä¿¡è™Ÿã€‚
    *   `v`ï¼šç¥ç¶“ç¶²è·¯** ƒ¹å€¼é  **å°ç•¶å‰ç›¤é¢çš„**  æ¸¬å€ **ã€‚
    *** „ç¾ **ï¼šé€™æ˜¯ã€Œå‡æ–¹èª¤å·® (Mean Squared Error)ã€ã€‚å®ƒè¨ˆç®—äº†ç¶²è·¯é æ¸¬çš„å‹ç‡ `v` å’ŒçœŸå¯¦çš„çµå±€ `z` ä¹‹é–“çš„å·®è·ã€‚æˆ‘å€‘å¸Œæœ›é€™å€‹å·®è·çš„å¹³æ–¹è¶Šå°è¶Šå¥½ï¼Œä¹Ÿå°±æ˜¯è®“ç¶²è·¯çš„åˆ¤æ–·åŠ›è¶Šä¾†è¶Šæº–ã€‚

2.** ­–ç•¥æå¤± (Policy Loss): `-Ï€áµ€ log(p) ***   `Ï€` (pi)ï¼š** CTS æ€è€ƒå¾Œçš„ã€Œæ•™å¸«ç­–ç•¥ã€ **ã€‚é€™æ˜¯ MCTS ç¶“éå¤§é‡æ¨¡æ“¬å¾Œï¼Œå¾—å‡ºçš„åœ¨ç•¶å‰ç›¤é¢ä¸‹ï¼Œå„å€‹å‹•ä½œçš„æ¨è–¦æ©Ÿç‡åˆ†ä½ˆï¼ˆåŸºæ–¼è¨ªå•æ¬¡æ•¸ï¼‰ã€‚å®ƒæ¯”ç¥ç¶“ç¶²è·¯çš„åŸå§‹ç›´è¦ºè¦å¼·å¤§å¾—å¤šã€‚
    *   `p`ï¼šç¥ç¶“ç¶²è·¯** ­–ç•¥é  **åŸå§‹è¼¸å‡ºçš„ã€Œå­¸ç”Ÿç­–ç•¥ã€ã€‚
    *** „ç¾ **ï¼šé€™æ˜¯ã€Œäº¤å‰ç†µ (Cross-Entropy)ã€ã€‚å®ƒç”¨ä¾†è¡¡é‡å…©å€‹æ©Ÿç‡åˆ†ä½ˆçš„ç›¸ä¼¼åº¦ã€‚æˆ‘å€‘å¸Œæœ›ç¶²è·¯çš„åŸå§‹ç›´è¦º `p`ï¼Œèƒ½ç›¡é‡è®Šå¾—å’Œ MCTS æ·±æ€ç†Ÿæ…®å¾Œçš„ç­–ç•¥ `Ï€` ä¸€æ¨£ã€‚

3.** ­£è¦åŒ–é … (Regularization): `c ||Î¸||Â² ***   `Î¸` (theta)ï¼šä»£è¡¨ç¥ç¶“ç¶²è·¯ä¸­æ‰€æœ‰çš„æ¬Šé‡åƒæ•¸ã€‚
    *   `c`ï¼šä¸€å€‹æ§åˆ¶æ­£è¦åŒ–å¼·åº¦çš„ä¿‚æ•¸ã€‚
    *** „ç¾ **ï¼šé€™æ˜¯ä¸€å€‹æ‡²ç½°é …ï¼Œç”¨ä¾†é˜²æ­¢ç¶²è·¯çš„æ¬Šé‡å€¼è®Šå¾—éå¤§ï¼Œé¿å…ã€Œéæ“¬åˆã€(Overfitting)ã€‚ä½ å¯ä»¥æŠŠå®ƒæƒ³æˆæ˜¯é˜²æ­¢å­¸ç”Ÿã€Œæ­»è¨˜ç¡¬èƒŒã€ï¼Œè€Œæ˜¯è¦å­¸æœƒã€Œèˆ‰ä¸€åä¸‰ã€ã€‚** ¨‹å¼ç¢¼é€£çµ **ï¼š
åœ¨ `main.py` çš„ `train_network` å‡½æ•¸ä¸­ï¼Œ`policy_loss = ...` å’Œ `value_loss = ...` é€™å…©è¡Œï¼Œå°±æ˜¯é€™å€‹å…¬å¼çš„ç›´æ¥é«”ç¾ã€‚

---

### 3. ã€Œæ‰‹ã€çš„è¡Œå‹•å…¬å¼ï¼šå¸¶æº«åº¦çš„æ¡æ¨£ç­–ç•¥

é€™æ˜¯ MCTS æ€è€ƒçµæŸå¾Œï¼Œ** œ€çµ‚æ±ºå®šä¸‹å“ªä¸€æ­¥æ£ **çš„å…¬å¼ã€‚å®ƒå¼•å…¥äº†ã€Œæº«åº¦ `Ï„`ã€çš„æ¦‚å¿µã€‚** …¬å¼ï¼ **$$ \pi(a|s) = \frac{N(s, a)^{1/\tau}}{\sum_{b} N(s, b)^{1/\tau}} $$** ‹†è§£å…¬å¼ï¼ ***   `Ï€(a|s)`ï¼šåœ¨ç‹€æ…‹ `s` ä¸‹ï¼Œæœ€çµ‚é¸æ“‡å‹•ä½œ `a` çš„æ©Ÿç‡ã€‚
*   `N(s, a)`ï¼šMCTS æœç´¢çµæŸå¾Œï¼Œå‹•ä½œ `a` è¢«è¨ªå•çš„ç¸½æ¬¡æ•¸ã€‚
*   `Ï„` (tau)ï¼š** º«åº¦åƒæ• **ã€‚é€™æ˜¯æ§åˆ¶æ¢ç´¢ç¨‹åº¦çš„é—œéµã€‚
    *** •¶ `Ï„` è¼ƒé«˜æ™‚ (ä¾‹å¦‚ `Ï„ = 1` **:
        *   `1/Ï„` æ¥è¿‘ 1ã€‚å…¬å¼è®Šç‚º `N(s, a) / sum(N(s, b))`ã€‚
        *** „ç¾ **ï¼šè½å­æ©Ÿç‡å’Œ MCTS çš„è¨ªå•æ¬¡æ•¸æˆæ­£æ¯”ã€‚è¨ªå•æ¬¡æ•¸å¤šçš„æ£‹ï¼Œè¢«é¸ä¸­çš„æ©Ÿç‡å°±é«˜ï¼Œä½†è¨ªå•æ¬¡æ•¸å°‘çš„æ£‹ï¼Œä¹Ÿæœ‰ä¸€å®šçš„æ©Ÿç‡è¢«é¸ä¸­ã€‚é€™èƒ½** ¼“å‹µæ¢ç´ **ï¼Œé€šå¸¸ç”¨æ–¼éŠæˆ²çš„é–‹å±€éšæ®µï¼Œè®“ AI å˜—è©¦ä¸åŒçš„é–‹å±€å¥—è·¯ã€‚
    *** •¶ `Ï„` è¶¨è¿‘æ–¼ 0 æ™‚ (ä¾‹å¦‚ `Ï„ = 0.01` **:
        *   `1/Ï„` æ˜¯ä¸€å€‹éå¸¸å¤§çš„æ•¸ã€‚
        *** „ç¾ **ï¼šæŒ‡æ•¸é‹ç®—æœƒæ¥µå¤§åœ°æ”¾å¤§è¨ªå•æ¬¡æ•¸ä¹‹é–“çš„å·®è·ã€‚è¨ªå•æ¬¡æ•¸æœ€å¤šçš„é‚£å€‹å‹•ä½œ `a`ï¼Œå…¶æ©Ÿç‡æœƒè¶¨è¿‘æ–¼ 1ï¼Œè€Œå…¶ä»–æ‰€æœ‰å‹•ä½œçš„æ©Ÿç‡éƒ½è¶¨è¿‘æ–¼ 0ã€‚é€™èƒ½** ¢ºä¿ AI é¸æ“‡å®ƒèªç‚ºæœ€å¥½çš„é‚£ä¸€æ­¥æ£ **ï¼Œé€šå¸¸ç”¨æ–¼éŠæˆ²çš„ä¸­å¾Œç›¤ï¼Œä»¥æ±‚ç©©å®šå–å‹ã€‚** ¨‹å¼ç¢¼é€£çµ **ï¼š
åœ¨ `mcts.py` çš„ `get_policy` å‡½æ•¸å’Œ `main.py` çš„ `execute_episode` ä¸­ï¼Œå° `temp` çš„è™•ç†å’Œ `np.random.choice` çš„ä½¿ç”¨ï¼Œå°±æ˜¯é€™å€‹å…¬å¼çš„æ‡‰ç”¨ã€‚

---

### ç¸½çµ

| å…¬å¼ | åç¨± | ä½œç”¨ | æ ¸å¿ƒæ€æƒ³ |
| :--- | :--- | :--- | :--- |
|** argmax(Q + U) **| PUCT æ¼”ç®—æ³• | åœ¨ MCTS** €è€ƒæ™ **ï¼Œæ±ºå®šæ¢ç´¢å“ªä¸€æ­¥æ£‹ | å¹³è¡¡** ­·å²ç¶“é©— (Q **å’Œ** œªä¾†æ½›åŠ› (U **|
|** L = (v-z)Â² - Ï€áµ€log(p) **| æå¤±å‡½æ•¸ | åœ¨** ¨“ç·´æ™ **ï¼ŒæŒ‡å°ç¥ç¶“ç¶²è·¯å¦‚ä½•å­¸ç¿’ | è®“** ˆ¤æ–·åŠ› (v **æ¥è¿‘** ¾å¯¦ (z **ï¼Œè®“** ›´è¦º (p **å­¸ç¿’** ·±æ€ (Ï€ **|
|** N^(1/Ï„) / sum(N^(1/Ï„)) **| å¸¶æº«åº¦çš„æ¡æ¨£ | åœ¨ MCTS æ€è€ƒ** µæŸå¾ **ï¼Œæ±ºå®šæœ€çµ‚ä¸‹å“ªä¸€æ­¥ | ç”¨** º«åº¦ (Ï„ **æ§åˆ¶** ¢ç´ **ï¼ˆå‰æœŸï¼‰èˆ‡** ˆ©ç” **ï¼ˆå¾ŒæœŸï¼‰çš„å¹³è¡¡ |

é€™ä¸‰å€‹å…¬å¼ï¼Œæ§‹æˆäº† AlphaZero å¾æ€è€ƒã€æ±ºç­–åˆ°å­¸ç¿’ã€é€²åŒ–çš„å®Œæ•´é–‰ç’°ã€‚ç†è§£äº†å®ƒå€‘ï¼Œä½ å°±æŒæ¡äº†é€™å€‹æ¼”ç®—æ³•æœ€åº•å±¤çš„æ™ºæ…§ã€‚

## Q: è«‹æŠŠä¸Šè¿°æ•¸å­¸å…¬å¼ï¼Œå’Œå°æ‡‰çš„ç¨‹å¼ç¢¼æ”¾åœ¨ä¸€èµ·èªªæ˜

çµ•å°æ²’å•é¡Œï¼å°‡æ•¸å­¸å…¬å¼å’Œç¨‹å¼ç¢¼ä¸¦åˆ—å°ç…§ï¼Œæ˜¯æ‰“é€šç†è«–èˆ‡å¯¦ä½œã€Œä»»ç£äºŒè„ˆã€çš„æœ€å¥½æ–¹æ³•ã€‚ä½ æœƒæ¸…æ¥šåœ°çœ‹åˆ°ï¼Œé‚£äº›çœ‹ä¼¼æŠ½è±¡çš„ç¬¦è™Ÿï¼Œæ˜¯å¦‚ä½•åŒ–èº«ç‚ºä¸€è¡Œè¡Œå…·é«”çš„ã€å¯åŸ·è¡Œçš„æŒ‡ä»¤ã€‚

æˆ‘å€‘å°±æŒ‰ç…§ä¸‰å¤§æ ¸å¿ƒå…¬å¼çš„é †åºä¾†é€ä¸€å°ç…§ã€‚

---

### 1. PUCT æ¼”ç®—æ³• (MCTS Selection)

é€™å€‹å…¬å¼æ˜¯ MCTS çš„ã€Œå¤§è…¦ã€ï¼Œåœ¨æ¯ä¸€æ¬¡æ¨¡æ“¬ä¸­ï¼Œå®ƒéƒ½æœƒè¢«ç”¨ä¾†æ±ºå®šä¸‹ä¸€æ­¥è¦èµ°å…¥å“ªå€‹åˆ†æ”¯ï¼Œä»¥æœ€é«˜æ•ˆåœ°æ¢ç´¢æ£‹å±€çš„å¯èƒ½æ€§ã€‚

####** •¸å­¸å…¬å¼ **é¸æ“‡çš„å‹•ä½œ `a` æ˜¯è®“ä»¥ä¸‹è¡¨é”å¼æœ€å¤§åŒ–çš„é‚£å€‹ï¼š
$$ a_t = \underset{a}{\operatorname{argmax}} \left( Q(s, a) + c_{puct} \cdot P(s, a) \cdot \frac{\sqrt{\sum_{b} N(s, b)}}{1 + N(s, a)} \right) $$

####** °æ‡‰ç¨‹å¼ç¢ **é€™æ®µç¨‹å¼ç¢¼ä½æ–¼ `mcts.py` çš„ `MCTS.search()` æ–¹æ³•ä¸­ã€‚

```python
# mcts.py

class MCTS:
    def search(self, canonical_state):
        # ... (ç¨‹å¼ç¢¼å‰æ®µçœç•¥) ...

        # --- é¸æ“‡ (Selection) ---
        valid_actions = self.game.get_valid_actions(canonical_state)
        best_uct = -float('inf')
        best_action = -1

        # è¨ˆç®— PUCT
        for a in range(self.game.get_action_size()):
            if valid_actions[a]:
                # Q(s, a)
                q_val = self.Q.get((s_key, a), 0)
                
                # N(s, a)
                n_val = self.N.get((s_key, a), 0)
                
                # U(s, a) = c_puct * P(s, a) * sqrt(sum N(s, b)) / (1 + N(s, a))
                uct = q_val + config.CPUCT * self.P[s_key][a] * \
                      math.sqrt(sum(self.N.get((s_key, act), 0) for act in range(self.game.get_action_size()))) / (1 + n_val)
                
                # argmax çš„å¯¦ç¾
                if uct > best_uct:
                    best_uct = uct
                    best_action = a
        
        action = best_action
        # ... (ç¨‹å¼ç¢¼å¾Œæ®µçœç•¥) ...
```

####** …¬å¼èˆ‡ç¨‹å¼ç¢¼å°ç… **| æ•¸å­¸ç¬¦è™Ÿ | ç¨‹å¼ç¢¼è®Šæ•¸/è¡¨é”å¼ | èªªæ˜ |
| :--- | :--- | :--- |
| `Q(s, a)` | `q_val = self.Q.get((s_key, a), 0)` | ä»£è¡¨å‹•ä½œ `a` çš„æ­·å²å¹³å‡å›å ± (åˆ©ç”¨é …)ã€‚`s_key` æ˜¯ç‹€æ…‹ `s` çš„å”¯ä¸€æ¨™è­˜ã€‚ |
| `c_puct` | `config.CPUCT` | æ¢ç´¢æ¬Šé‡å¸¸æ•¸ï¼Œå¾é…ç½®æ–‡ä»¶è®€å–ã€‚ |
| `P(s, a)` | `self.P[s_key][a]` | ç¥ç¶“ç¶²è·¯å°å‹•ä½œ `a` çš„å…ˆé©—æ©Ÿç‡ï¼Œç”±ç­–ç•¥é ­è¼¸å‡ºã€‚ |
| `N(s, a)` | `n_val = self.N.get((s_key, a), 0)` | å‹•ä½œ `a` è¢«è¨ªå•éçš„æ¬¡æ•¸ã€‚ |
| `Î£_b N(s, b)` | `sum(self.N.get((s_key, act), 0) for act in ...)` | çˆ¶ç¯€é»çš„ç¸½è¨ªå•æ¬¡æ•¸ï¼Œé€éåŠ ç¸½æ‰€æœ‰å­ç¯€é»çš„è¨ªå•æ¬¡æ•¸å¾—åˆ°ã€‚ |
| `argmax` | `if uct > best_uct: best_uct = uct; best_action = a` | é€éä¸€å€‹ for è¿´åœˆå’Œ if åˆ¤æ–·ï¼Œä¾†æ‰¾åˆ°ä½¿ `uct` å€¼æœ€å¤§çš„é‚£å€‹å‹•ä½œ `a`ã€‚ |

---

### 2. æå¤±å‡½æ•¸ (Loss Function)

é€™å€‹å…¬å¼æ˜¯ç¥ç¶“ç¶²è·¯çš„ã€Œå¿ƒè‡Ÿã€ï¼Œåœ¨è¨“ç·´éšæ®µï¼Œå®ƒé‡åŒ–äº†ç¶²è·¯é æ¸¬çš„ã€ŒéŒ¯èª¤ç¨‹åº¦ã€ï¼Œä¸¦ç‚ºç¶²è·¯çš„åƒæ•¸å„ªåŒ–ï¼ˆå­¸ç¿’ï¼‰æä¾›äº†æ–¹å‘ã€‚

####** •¸å­¸å…¬å¼ **$$ L = (v - z)^2 - \boldsymbol{\pi}^T \log(\mathbf{p}) $$
*(ç‚ºæ¸…æ™°èµ·è¦‹ï¼Œæš«æ™‚çœç•¥æ­£è¦åŒ–é …)*

####** °æ‡‰ç¨‹å¼ç¢ **é€™æ®µç¨‹å¼ç¢¼ä½æ–¼ `main.py` çš„ `train_network()` å‡½æ•¸ä¸­ã€‚

```python
# main.py

def train_network(model, training_examples):
    # ... (ç¨‹å¼ç¢¼å‰æ®µçœç•¥) ...
    
    # å¾è¨“ç·´æ•¸æ“šä¸­å–å‡º states, target_policies (Ï€), target_values (z)
    states, policies, values = list(zip(*[training_examples[i] for i in sample_ids]))
    states = torch.FloatTensor(np.array(states))
    target_policies = torch.FloatTensor(np.array(policies))
    target_values = torch.FloatTensor(np.array(values).astype(np.float64))
    
    # è®“æ¨¡å‹é€²è¡Œé æ¸¬ï¼Œå¾—åˆ° p (log å½¢å¼) å’Œ v
    out_log_policies, out_values = model(states)
    
    # è¨ˆç®—æå¤±
    # åƒ¹å€¼æå¤±ï¼š(v - z)Â²
    value_loss = torch.mean((target_values.view(-1) - out_values.view(-1))** )

    # ç­–ç•¥æå¤±ï¼š-Ï€áµ€ log(p)
    policy_loss = -torch.mean(torch.sum(target_policies * out_log_policies, dim=1))
    
    # ç¸½æå¤±
    loss = policy_loss + value_loss
    
    # ... (åå‘å‚³æ’­èˆ‡å„ªåŒ–) ...
```

#### **å…¬å¼èˆ‡ç¨‹å¼ç¢¼å°ç… **| æ•¸å­¸ç¬¦è™Ÿ | ç¨‹å¼ç¢¼è®Šæ•¸/è¡¨é”å¼ | èªªæ˜ |
| :--- | :--- | :--- |
| `z` | `target_values` | éŠæˆ²çš„çœŸå¯¦çµæœ `[+1, -1, 0]`ï¼Œæ˜¯åƒ¹å€¼é ­çš„å­¸ç¿’ç›®æ¨™ã€‚ |
| `v` | `out_values` | ç¥ç¶“ç¶²è·¯åƒ¹å€¼é ­å°ç›¤é¢çš„é æ¸¬å€¼ã€‚ |
| `(v - z)Â²` | `(target_values.view(-1) - out_values.view(-1))** ` | è¨ˆç®—é æ¸¬å€¼å’ŒçœŸå¯¦çµæœçš„å‡æ–¹èª¤å·®ã€‚`torch.mean` æ˜¯å› ç‚ºæˆ‘å€‘ä¸€æ¬¡è™•ç†ä¸€å€‹æ‰¹æ¬¡(batch)çš„æ•¸æ“šã€‚ |
| `Ï€` | `target_policies` | MCTS æ€è€ƒå¾Œçš„ã€Œæ•™å¸«ç­–ç•¥ã€ï¼Œæ˜¯ç­–ç•¥é ­çš„å­¸ç¿’ç›®æ¨™ã€‚ |
| `log(p)` | `out_log_policies` | ç¥ç¶“ç¶²è·¯ç­–ç•¥é ­è¼¸å‡ºçš„ log æ©Ÿç‡ã€‚ç›´æ¥è¼¸å‡º log æ©Ÿç‡æ˜¯ç‚ºäº†æ•¸å€¼ç©©å®šæ€§ã€‚ |
| `-Ï€áµ€ log(p)` | `-torch.sum(target_policies * out_log_policies, dim=1)` | é€™æ˜¯è¨ˆç®—äº¤å‰ç†µæå¤±çš„æ¨™æº–æ–¹æ³•ã€‚`torch.sum` å¯¦ç¾äº†å‘é‡çš„é»ç© (`Ï€áµ€ log(p)`)ã€‚ |

---

### 3. å¸¶æº«åº¦çš„æ¡æ¨£ç­–ç•¥

é€™å€‹å…¬å¼æ˜¯ MCTS çš„ã€Œæ‰‹ã€ï¼Œåœ¨æ‰€æœ‰æ¨¡æ“¬æ€è€ƒçµæŸå¾Œï¼Œå®ƒæ ¹æ“šæ€è€ƒçš„çµæœï¼Œä¸¦çµåˆã€Œæº«åº¦ã€åƒæ•¸ï¼Œä¾†æœ€çµ‚æ±ºå®šä¸‹å‡ºå“ªä¸€æ­¥æ£‹ã€‚

#### **æ•¸å­¸å…¬å¼ **$$ \pi(a|s) = \frac{N(s, a)^{1/\tau}}{\sum_{b} N(s, b)^{1/\tau}} $$

####** °æ‡‰ç¨‹å¼ç¢ **é€™æ®µç¨‹å¼ç¢¼ä½æ–¼ `mcts.py` çš„ `MCTS.get_policy()` æ–¹æ³•ä¸­ã€‚

```python
# mcts.py

class MCTS:
    def get_policy(self, canonical_state, temp=1.0):
        # ... (å‰é¢çš„æ¨¡æ“¬éƒ¨åˆ†çœç•¥) ...

        s_key = canonical_state.tobytes()
        counts = [self.N.get((s_key, a), 0) for a in range(self.game.get_action_size())]
        
        # ç•¶æº«åº¦ Ï„ è¶¨è¿‘æ–¼ 0 æ™‚ï¼Œç›´æ¥é¸æ“‡è¨ªå•æ¬¡æ•¸æœ€å¤šçš„ (argmax)
        if temp == 0:
            best_action = np.argmax(counts)
            policy = np.zeros_like(counts)
            policy[best_action] = 1
            return policy
        
        # N(s, a)^(1/Ï„)
        counts = [c** 1./temp) for c in counts]
        
        # æ­£è¦åŒ–ï¼šé™¤ä»¥ç¸½å’Œ
        policy = [c/sum(counts) for c in counts]

        return policy
```

#### **å…¬å¼èˆ‡ç¨‹å¼ç¢¼å°ç… **| æ•¸å­¸ç¬¦è™Ÿ | ç¨‹å¼ç¢¼è®Šæ•¸/è¡¨é”å¼ | èªªæ˜ |
| :--- | :--- | :--- |
| `N(s, a)` | `counts` | æ¯å€‹å‹•ä½œ `a` çš„è¨ªå•æ¬¡æ•¸ `N`ã€‚é€™æ˜¯ä¸€å€‹åˆ—è¡¨ï¼Œç´¢å¼•ä»£è¡¨å‹•ä½œï¼Œå€¼ä»£è¡¨è¨ªå•æ¬¡æ•¸ã€‚ |
| `Ï„` (tau) | `temp` | æº«åº¦åƒæ•¸ï¼Œç”±å¤–éƒ¨å‚³å…¥ã€‚ |
| `N(s, a)^(1/Ï„)` | `[c** 1./temp) for c in counts]` | é€éåˆ—è¡¨æ¨å°å¼ï¼Œå° `counts` ä¸­çš„æ¯ä¸€å€‹å…ƒç´ ï¼ˆè¨ªå•æ¬¡æ•¸ï¼‰é€²è¡Œ `1/Ï„` æ¬¡æ–¹é‹ç®—ã€‚ |
| `Î£_b N(s, b)^(1/Ï„)` | `sum(counts)` | åœ¨ä¸Šä¸€æ­¥è¨ˆç®—å®Œæ¬¡æ–¹å¾Œï¼Œå°æ•´å€‹åˆ—è¡¨æ±‚å’Œï¼Œå¾—åˆ°æ­£è¦åŒ–åˆ†æ¯ã€‚ |
| `... / Î£ ...` | `[c/sum(counts) for c in counts]` | å†æ¬¡ä½¿ç”¨åˆ—è¡¨æ¨å°å¼ï¼Œå°‡æ¯å€‹å…ƒç´ é™¤ä»¥ç¸½å’Œï¼Œå¾—åˆ°æœ€çµ‚çš„æ©Ÿç‡åˆ†ä½ˆ `Ï€`ã€‚ |

å¸Œæœ›é€™æ¨£ä¸¦åˆ—çš„è§£é‡‹ï¼Œèƒ½è®“ä½ å° AlphaZero æ¼”ç®—æ³•çš„ç†è§£ï¼Œå¾æŠ½è±¡çš„æ•¸å­¸å±¤é¢ï¼Œç„¡ç¸«åœ°éŠœæ¥åˆ°å…·é«”çš„ç¨‹å¼ç¢¼å¯¦ç¾å±¤é¢ã€‚é€™æ˜¯ä¸€å€‹éå¸¸é—œéµçš„é£›èºï¼

